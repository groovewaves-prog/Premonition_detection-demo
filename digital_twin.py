# -*- coding: utf-8 -*-
"""
digital_twin.py (Universal Edition - Final Fix v2.1)
====================================================
AIOps Digital Twin Engine

[ä¿®æ­£å±¥æ­´]
 - Fix: å€™è£œé¸å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’æ‹¡å¼µã€‚
        ã“ã‚Œã¾ã§ã¯ Warning (prob >= 0.45) ã®æ©Ÿå™¨ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã¦ã„ãŸãŒã€
        INFOãƒ¬ãƒ™ãƒ«ã®äºˆå…†ã‚·ã‚°ãƒŠãƒ« (Weak Signal) ã‚‚æ¤œçŸ¥ã§ãã‚‹ã‚ˆã†ã€
        ã€Œãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹å…¨æ©Ÿå™¨ã€ã‚‚ã‚¹ã‚­ãƒ£ãƒ³å¯¾è±¡ã«è¿½åŠ ã€‚

è¨­è¨ˆæ–¹é‡:
 - UXå¤‰æ›´ãªã—: inference_engine ã‹ã‚‰å‘¼ã³å‡ºã—ã€äºˆå…†ãƒ‡ãƒ¼ã‚¿ã‚’æ³¨å…¥ã™ã‚‹ã€‚
 - Hybrid Matching: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´(é«˜é€Ÿ) + Embeddingé¡ä¼¼åº¦(æŸ”è»Ÿ)
"""

import logging
import numpy as np
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass

# --- ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    import networkx as nx
    HAS_NX = True
except ImportError:
    HAS_NX = False

try:
    from sentence_transformers import SentenceTransformer
    HAS_BERT = True
except ImportError:
    HAS_BERT = False

logger = logging.getLogger(__name__)

# ==========================================================
# Escalation Rules (ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ - ç§‘å­¦çš„æ ¹æ‹ ç‰ˆ)
# ==========================================================
@dataclass
class EscalationRule:
    """
    WARNING ãŒã©ã† CRITICAL ã«é€²è¡Œã™ã‚‹ã‹ã®ãƒ«ãƒ¼ãƒ«å®šç¾©
    
    Attributes:
        time_to_critical_min: æ€¥æ€§æœŸã®é€²è¡Œé€Ÿåº¦ï¼ˆç™ºç—‡ã—ã¦ã‹ã‚‰å…¨æ–­ã™ã‚‹ã¾ã§ã®æ™‚é–“ï¼‰
        early_warning_hours:  æ—©æœŸäºˆå…†æ¤œçŸ¥å¯èƒ½æ™‚é–“ï¼ˆTCNå¢—åŠ ã‚„CRCã‚¨ãƒ©ãƒ¼ãªã©ã€å‰å…†ãŒå‡ºå§‹ã‚ã‚‹æœ€æ—©æ™‚é–“ï¼‰
    """
    pattern: str                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (lowercase)
    semantic_phrases: List[str]     # Embedding ãƒãƒƒãƒç”¨ãƒ•ãƒ¬ãƒ¼ã‚ºç¾¤
    escalated_state: str            # é€²è¡Œå¾Œã®çŠ¶æ…‹
    time_to_critical_min: int       # æ€¥æ€§æœŸ (åˆ†)
    early_warning_hours: int        # æ—©æœŸäºˆå…† (æ™‚é–“)
    base_confidence: float          # åŸºç¤ä¿¡é ¼åº¦
    category: str = "Generic"       # åˆ†é¡

# ç§‘å­¦çš„çŸ¥è¦‹ãƒ»çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
ESCALATION_RULES: List[EscalationRule] = [
    # --- Network / L2 (Critical & Fast) ---
    EscalationRule(
        "stp_loop",
        ["stp loop", "spanning tree topology change", "tcn received", "bpdu guard", 
         "blocking port", "loop guard", "root bridge change", "excessive broadcasts"],
        "L2ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚¹ãƒˆãƒ¼ãƒ ï¼ˆå…¨æ–­ï¼‰", 
        time_to_critical_min=5, early_warning_hours=24, base_confidence=0.95,
        category="Network/L2"
    ),
    EscalationRule(
        "mac_flap",
        ["mac flapping", "host moving", "mac address move", "mac table overflow", 
         "learning disable", "duplicate mac"],
        "MACãƒ†ãƒ¼ãƒ–ãƒ«ä¸å®‰å®šåŒ–ã«ã‚ˆã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ æ¶ˆå¤±", 
        time_to_critical_min=10, early_warning_hours=24, base_confidence=0.90,
        category="Network/L2"
    ),
    EscalationRule(
        "arp_storm",
        ["arp storm", "duplicate ip", "gratuitous arp", "arp table overflow", 
         "arp rate limit", "neighbor table full"],
        "ARPãƒ†ãƒ¼ãƒ–ãƒ«æ±šæŸ“ã«ã‚ˆã‚‹é€šä¿¡æ–­", 
        time_to_critical_min=10, early_warning_hours=12, base_confidence=0.85,
        category="Network/L2"
    ),

    # --- Network / L3 & Routing ---
    EscalationRule(
        "bgp_flap",
        ["bgp flapping", "bgp neighbor down", "bgp session reset", "route oscillation", 
         "prefix withdrawal", "hold timer expired", "notification received"],
        "BGPã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸å®‰å®šåŒ–ã«ã‚ˆã‚‹å¤§è¦æ¨¡çµŒè·¯æ¶ˆå¤±", 
        time_to_critical_min=15, early_warning_hours=48, base_confidence=0.90,
        category="Network/Routing"
    ),
    EscalationRule(
        "ospf_adj",
        ["ospf adjacency down", "neighbor down", "dead timer expired", "lsa age", 
         "database description", "retransmission limit", "spf calculation"],
        "OSPFãƒã‚¤ãƒãƒ¼å–ªå¤±ã«ã‚ˆã‚‹ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—/ãƒ–ãƒ©ãƒƒã‚¯ãƒ›ãƒ¼ãƒ«", 
        time_to_critical_min=15, early_warning_hours=24, base_confidence=0.85,
        category="Network/Routing"
    ),
    
    # --- Network / HA ---
    EscalationRule(
        "ha_split",
        ["ha state degraded", "failover state changed", "standby not ready", 
         "heartbeat lost", "split brain", "cluster link down"],
        "HAåŒæœŸä¸å…¨ã«ã‚ˆã‚‹ã‚¹ãƒ—ãƒªãƒƒãƒˆãƒ–ãƒ¬ã‚¤ãƒ³ç™ºç”Ÿ", 
        time_to_critical_min=30, early_warning_hours=48, base_confidence=0.85,
        category="Network/HA"
    ),

    # --- Network / QoS & Performance ---
    EscalationRule(
        "bandwidth",
        ["bandwidth exceeded", "interface congestion", "shaping active", "policing drop", 
         "tail drop", "queue full", "output drops"],
        "å¸¯åŸŸé£½å’Œã«ã‚ˆã‚‹ã‚µãƒ¼ãƒ“ã‚¹å“è³ªåŠ£åŒ–ï¼ˆé…å»¶ãƒ»ãƒ‘ã‚±ãƒƒãƒˆãƒ­ã‚¹ï¼‰", 
        time_to_critical_min=20, early_warning_hours=72, base_confidence=0.80,
        category="Network/QoS"
    ),
    EscalationRule(
        "drop_error",
        ["input errors", "crc error", "symbol error", "runts", "giants", 
         "interface resets", "fcs error"],
        "ç‰©ç†å›ç·šå“è³ªåŠ£åŒ–ã«ã‚ˆã‚‹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä½ä¸‹", 
        time_to_critical_min=30, early_warning_hours=168, base_confidence=0.75,
        category="Network/Interface"
    ),

    # --- Network / Services ---
    EscalationRule(
        "ntp_drift",
        ["ntp unsynchronized", "stratum change", "peer unreachable", "time drift", 
         "clock offset", "leap second"],
        "æ™‚åˆ»ä¸æ•´åˆã«ã‚ˆã‚‹ãƒ­ã‚°ä¸å…¨ãƒ»èªè¨¼ã‚¨ãƒ©ãƒ¼ãƒ»è¨¼æ˜æ›¸ç„¡åŠ¹åŒ–", 
        time_to_critical_min=120, early_warning_hours=168, base_confidence=0.70,
        category="Network/Service"
    ),
    EscalationRule(
        "dhcp_dns",
        ["dhcp pool exhausted", "no ip address available", "dns timeout", "nxdomain", 
         "server not responding", "discovery failed"],
        "æ–°è¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šä¸å¯", 
        time_to_critical_min=30, early_warning_hours=48, base_confidence=0.80,
        category="Network/Service"
    ),

    # --- Hardware / Environmental ---
    EscalationRule(
        "optical",
        ["rx power low", "optical signal degradation", "light level warning", 
         "transceiver threshold", "dbm low", "link fluctuation"],
        "å…‰ä¿¡å·åŠ£åŒ–ã«ã‚ˆã‚‹çªç„¶ã®ãƒªãƒ³ã‚¯ãƒ€ã‚¦ãƒ³", 
        time_to_critical_min=60, early_warning_hours=336, base_confidence=0.90,
        category="Hardware/Optical"
    ),
    EscalationRule(
        "temperature",
        ["temperature high", "overheat", "thermal threshold", "intake air temp", 
         "exhaust temp", "sensor alarm"],
        "ç†±æš´èµ°ã«ã‚ˆã‚‹ç·Šæ€¥ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³", 
        time_to_critical_min=30, early_warning_hours=48, base_confidence=0.85,
        category="Hardware/Thermal"
    ),
    EscalationRule(
        "fan_fail",
        ["fan failure", "fan malfunction", "fan speed low", "fan tray removed"],
        "å†·å´èƒ½åŠ›å–ªå¤±ã«ã‚ˆã‚‹æ¸©åº¦ä¸Šæ˜‡", 
        time_to_critical_min=45, early_warning_hours=72, base_confidence=0.80,
        category="Hardware/Thermal"
    ),
    EscalationRule(
        "power_quality",
        ["ups on battery", "input voltage low", "pdu alarm", "redundancy lost", 
         "power supply failed", "psu failure"],
        "é›»æºä¾›çµ¦ä¸å®‰å®šã«ã‚ˆã‚‹äºˆæœŸã›ã¬å†èµ·å‹•", 
        time_to_critical_min=15, early_warning_hours=24, base_confidence=0.85,
        category="Hardware/Power"
    ),
    EscalationRule(
        "storage",
        ["flash error", "file system full", "nvram corruption", "disk fail", 
         "write protect", "read error", "smart error"],
        "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšœå®³ã«ã‚ˆã‚‹è¨­å®šå–ªå¤±ãƒ»èµ·å‹•ä¸èƒ½", 
        time_to_critical_min=180, early_warning_hours=720, base_confidence=0.75,
        category="Hardware/Storage"
    ),

    # --- Software / Resources & Process ---
    EscalationRule(
        "memory_leak",
        ["memory usage high", "malloc fail", "memory pool depletion", "heap exhaustion", 
         "fragmentation", "leak detected"],
        "ãƒ¡ãƒ¢ãƒªæ¯æ¸‡ã«ã‚ˆã‚‹OOM Killerç™ºå‹•ãƒ»ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ãƒƒã‚·ãƒ¥", 
        time_to_critical_min=180, early_warning_hours=336, base_confidence=0.85,
        category="Software/Resource"
    ),
    EscalationRule(
        "cpu_load",
        ["cpu usage high", "cpu spike", "load average high", "control plane overload", 
         "process stuck", "interrupt storm"],
        "CPUæ¯æ¸‡ã«ã‚ˆã‚‹ç®¡ç†ä¸èƒ½ãƒ»ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒ€ã‚¦ãƒ³", 
        time_to_critical_min=20, early_warning_hours=48, base_confidence=0.85,
        category="Software/Resource"
    ),
    EscalationRule(
        "process_crash",
        ["process terminated", "segmentation fault", "core dump", "watchdog timeout", 
         "daemon exit", "service restart"],
        "é‡è¦ãƒ—ãƒ­ã‚»ã‚¹åœæ­¢ã«ã‚ˆã‚‹åˆ¶å¾¡æ©Ÿèƒ½å–ªå¤±", 
        time_to_critical_min=10, early_warning_hours=24, base_confidence=0.90,
        category="Software/Process"
    ),

    # --- Security ---
    EscalationRule(
        "auth_failure",
        ["authentication failed", "radius timeout", "tacacs unreachable", "invalid user", 
         "login failed", "aaa server down"],
        "èªè¨¼åŸºç›¤éšœå®³ã«ã‚ˆã‚‹ç®¡ç†ã‚¢ã‚¯ã‚»ã‚¹ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¥ç¶šä¸èƒ½", 
        time_to_critical_min=15, early_warning_hours=12, base_confidence=0.80,
        category="Security/Auth"
    ),
    EscalationRule(
        "crypto_vpn",
        ["ike sa deleted", "ipsec phase1 failed", "certificate expired", "decryption error", 
         "vpn tunnel down", "proposal mismatch"],
        "VPN/æš—å·åŒ–ãƒˆãƒ³ãƒãƒ«ã®åˆ‡æ–­", 
        time_to_critical_min=60, early_warning_hours=720, base_confidence=0.80,
        category="Security/Crypto"
    ),

    # --- Fallback ---
    EscalationRule(
        "generic_error",
        ["error", "fail", "critical", "warning", "emergency", "alert"],
        "æœªåˆ†é¡ã®ã‚µãƒ¼ãƒ“ã‚¹åŠ£åŒ–é€²è¡Œ", 
        time_to_critical_min=30, early_warning_hours=24, base_confidence=0.50,
        category="Generic"
    ),
]

# ==========================================================
# Digital Twin Engine
# ==========================================================
class DigitalTwinEngine:
    """
    Digital Twin Engine for predictive fault analysis.
    """
    
    # --- Class-level cache (Singleton Pattern) ---
    _model: Optional[Any] = None
    _rule_embeddings: Optional[Dict[str, Any]] = None
    _model_loaded: bool = False

    # --- Configuration ---
    MIN_PREDICTION_CONFIDENCE = 0.40 
    MAX_PROPAGATION_HOPS = 3
    HOP_DECAY_RATE = 0.10
    REDUNDANCY_DISCOUNT = 0.15
    SPOF_BOOST = 1.10
    EMBEDDING_THRESHOLD = 0.40

    def __init__(self, topology: Dict[str, Any], children_map: Optional[Dict[str, List[str]]] = None):
        self.topology = topology
        self.children_map = children_map or {}

        # --- NetworkX ã‚°ãƒ©ãƒ•æ§‹ç¯‰ ---
        self.graph = None
        if HAS_NX:
            self.graph = nx.DiGraph()
            for node_id, attrs in topology.items():
                node_attrs = attrs if isinstance(attrs, dict) else vars(attrs)
                self.graph.add_node(node_id, **node_attrs)
                
                parent_id = node_attrs.get("parent_id")
                if parent_id and parent_id in topology:
                    self.graph.add_edge(parent_id, node_id, relation="downstream")
                    self.graph.add_edge(node_id, parent_id, relation="upstream")

        self._redundancy_groups = self._build_redundancy_map()
        self._ensure_model_loaded()

    def _build_redundancy_map(self) -> Dict[str, List[str]]:
        rg_map = {}
        for dev_id, info in self.topology.items():
            attrs = info if isinstance(info, dict) else vars(info)
            rg = attrs.get('redundancy_group')
            if rg:
                rg_map.setdefault(rg, []).append(dev_id)
        return rg_map

    @classmethod
    def _ensure_model_loaded(cls):
        if cls._model_loaded:
            return

        if not HAS_BERT:
            logger.warning("sentence-transformers not available. Semantic matching disabled.")
            cls._model_loaded = True
            return

        try:
            cls._model = SentenceTransformer('all-MiniLM-L6-v2')
            
            all_phrases = []
            phrase_to_rule_idx = []
            
            for idx, rule in enumerate(ESCALATION_RULES):
                for phrase in rule.semantic_phrases:
                    all_phrases.append(phrase)
                    phrase_to_rule_idx.append(idx)
            
            if all_phrases:
                embeddings = cls._model.encode(all_phrases, convert_to_numpy=True)
                cls._rule_embeddings = {
                    "vectors": embeddings,
                    "phrase_to_rule_idx": phrase_to_rule_idx,
                    "phrases": all_phrases
                }
            
            cls._model_loaded = True
            
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            cls._model = None
            cls._model_loaded = True

    # ----------------------------------------------------------
    # Hybrid Matching Logic
    # ----------------------------------------------------------
    def _match_rule(self, alarm_text: str) -> Tuple[Optional[EscalationRule], float]:
        text_lower = alarm_text.lower()
        for rule in ESCALATION_RULES:
            if rule.pattern in text_lower:
                return rule, 1.0
        
        if self._model and self._rule_embeddings:
            try:
                query_vec = self._model.encode([alarm_text], convert_to_numpy=True)
                rule_vecs = self._rule_embeddings["vectors"]
                
                similarities = np.dot(rule_vecs, query_vec.T).flatten()
                norms = np.linalg.norm(rule_vecs, axis=1) * np.linalg.norm(query_vec)
                norms = np.where(norms == 0, 1e-10, norms)
                cosine_sim = similarities / norms
                
                best_idx = np.argmax(cosine_sim)
                best_score = float(cosine_sim[best_idx])
                
                if best_score >= self.EMBEDDING_THRESHOLD:
                    rule_idx = self._rule_embeddings["phrase_to_rule_idx"][best_idx]
                    return ESCALATION_RULES[rule_idx], best_score
                    
            except Exception as e:
                logger.error(f"Embedding matching error: {e}")
        
        return None, 0.0

    # ----------------------------------------------------------
    # Graph & Reliability Logic
    # ----------------------------------------------------------
    def _get_downstream_impact(self, root_id: str) -> List[Tuple[str, int]]:
        impacts = []
        if not self.graph or root_id not in self.graph:
            return impacts
            
        try:
            def downstream_filter(u, v):
                return self.graph[u][v].get("relation") == "downstream"
            
            subgraph = nx.subgraph_view(self.graph, filter_edge=downstream_filter)
            tree = nx.bfs_tree(subgraph, root_id, depth_limit=self.MAX_PROPAGATION_HOPS)
            
            for node in tree:
                if node == root_id: continue
                dist = nx.shortest_path_length(subgraph, root_id, node)
                impacts.append((node, dist))
                
        except Exception as e:
            logger.error(f"Graph traversal error: {e}")
            
        return impacts

    def _calculate_confidence(self, rule: EscalationRule, device_id: str, match_quality: float) -> float:
        attrs = self.topology.get(device_id, {})
        if not isinstance(attrs, dict): attrs = vars(attrs)
        
        rg = attrs.get('redundancy_group')
        has_redundancy = False
        if rg and len(self._redundancy_groups.get(rg, [])) > 1:
            has_redundancy = True
            
        is_spof = False
        children = self.children_map.get(device_id, [])
        if children and not has_redundancy:
            is_spof = True
            
        confidence = rule.base_confidence
        confidence *= (0.8 + 0.2 * match_quality)
        
        if has_redundancy:
            confidence *= (1.0 - self.REDUNDANCY_DISCOUNT)
            
        if is_spof:
            confidence *= self.SPOF_BOOST
            
        return min(0.99, max(0.1, confidence))

    # ----------------------------------------------------------
    # Main Prediction Method
    # ----------------------------------------------------------
    def predict(self, 
                analysis_results: List[Dict[str, Any]], 
                msg_map: Dict[str, List[str]], 
                alarms: Optional[List] = None) -> List[Dict[str, Any]]:
        """
        äºˆå…†æ¤œçŸ¥ã®ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹
        """
        predictions = []
        
        # ã€ä¿®æ­£ç®‡æ‰€ã€‘å€™è£œé¸å®šãƒ­ã‚¸ãƒƒã‚¯ã®æ‹¡å¼µ
        # 1. æ—¢å­˜ã®åˆ†æã§ Warning ãŒå‡ºã¦ã„ã‚‹æ©Ÿå™¨ (prob 0.45-0.85)
        warning_ids = {
            r["id"] for r in analysis_results
            if 0.45 <= float(r.get("prob", 0)) <= 0.85
            and r.get("id", "") != "SYSTEM"
        }
        
        # 2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸(ãƒ­ã‚°)ã‚’æŒã£ã¦ã„ã‚‹å…¨æ©Ÿå™¨
        #    ç†ç”±: "INFO"ãƒ¬ãƒ™ãƒ«ã®äºˆå…†ã‚·ã‚°ãƒŠãƒ«(Weak Signal)ã¯ analysis_results ã§ã¯
        #          "Normal"(prob < 0.45) ã¨åˆ¤å®šã•ã‚Œã‚‹ãŸã‚ã€warning_ids ã«ã¯å«ã¾ã‚Œãªã„ã€‚
        #          Digital Twin ã¯ã“ã‚Œã‚‰ã‚‚å«ã‚ã¦ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
        active_ids = set(msg_map.keys())
        
        # å€™è£œã®çµ±åˆ
        candidates = warning_ids.union(active_ids)
        
        processed_devices = set()
        
        for dev_id in candidates:
            if dev_id in processed_devices: continue
            
            messages = msg_map.get(dev_id, [])
            if not messages: continue
            msg = messages[0]
            
            rule, quality = self._match_rule(msg)
            if not rule: continue
            
            downstream = self._get_downstream_impact(dev_id)
            impact_count = len(downstream)
            
            confidence = self._calculate_confidence(rule, dev_id, quality)
            
            if confidence < self.MIN_PREDICTION_CONFIDENCE:
                continue
                
            affected_names = [d[0] for d in downstream[:3]]
            if impact_count > 3: affected_names.append(f"ä»–{impact_count-3}å°")
            affected_str = ", ".join(affected_names) if affected_names else "é…ä¸‹ãªã—"
            
            # ãƒŠãƒ©ãƒ†ã‚£ãƒ–ã®å¼·åŒ–: ã€Œæ—©æœŸäºˆå…†ã€ã¨ã€Œæ€¥æ€§æœŸã€ã®2è»¸ã§è¡¨ç¾
            if rule.early_warning_hours >= 24:
                early_str = f"æœ€å¤§ {rule.early_warning_hours // 24}æ—¥å‰"
            else:
                early_str = f"æœ€å¤§ {rule.early_warning_hours}æ™‚é–“å‰"

            pred = {
                "id": dev_id,
                "label": f"ğŸ”® [äºˆå…†] {rule.escalated_state}",
                "severity": "CRITICAL", 
                "status": "CRITICAL",
                "prob": round(confidence, 2),
                "type": f"Predictive/{rule.category}",
                "tier": 1,
                "reason": (
                    f"ã€Digital Twinæœªæ¥äºˆæ¸¬ (Predictive Maintenance)ã€‘\n"
                    f"ãƒ»è¦³æ¸¬: {msg} (Match: {quality:.2f})\n"
                    f"ãƒ»æ—©æœŸäºˆå…†: {early_str} ã‹ã‚‰æ¤œçŸ¥å¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³\n"
                    f"ãƒ»æ€¥æ€§æœŸé€²è¡Œ: ç™ºç—‡å¾Œ {rule.time_to_critical_min}åˆ† ã§æ·±åˆ»åŒ–ã™ã‚‹æã‚Œ\n"
                    f"ãƒ»å½±éŸ¿: {affected_str} ãŒé€£é–çš„ã«é€šä¿¡æ–­ã«ãªã‚Šã¾ã™ã€‚\n"
                    f"ãƒ»æ¨å¥¨: ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã®äºˆé˜²äº¤æ›/å¯¾å¿œ\n"
                    f"(ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {confidence:.2f})"
                ),
                "is_prediction": True,
                # UIè¡¨ç¤ºç”¨ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚‚æ¸¡ã—ã¦ãŠã
                "prediction_early_warning_hours": rule.early_warning_hours,
                "prediction_time_to_critical_min": rule.time_to_critical_min
            }
            
            predictions.append(pred)
            processed_devices.add(dev_id)
            
        return predictions

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    topology = {"FW01": {"parent_id": None, "redundancy_group": "FW_HA"}, "SW01": {"parent_id": "FW01"}}
    msgs = {"FW01": ["Rx Power -24.8 dBm (Low)"]} 
    dummy_results = [{"id": "FW01", "prob": 0.10, "status": "NORMAL"}] # INFOç›¸å½“
    
    dt = DigitalTwinEngine(topology)
    preds = dt.predict(dummy_results, msgs, alarms=[])
    
    for p in preds:
        print(f"Label: {p['label']}")
        print(f"Reason: {p['reason']}")
