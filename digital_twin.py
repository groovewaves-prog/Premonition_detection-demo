# -*- coding: utf-8 -*-
"""
digital_twin.py (Universal Edition - Production Ready v3.0)
===========================================================
AIOps Digital Twin Engine

[ä¿®æ­£å±¥æ­´]
 - Fix 1: å½±éŸ¿ç¯„å›²(0å°)ã®ãƒã‚°ä¿®æ­£ã€‚Graphæ§‹ç¯‰æ™‚ã®ã‚¨ãƒƒã‚¸æ–¹å‘(Downstream)ã‚’æ­£ã—ãå®šç¾©ã€‚
 - Fix 2: è¤‡æ•°ã‚·ã‚°ãƒŠãƒ«å¯¾å¿œã€‚1ãƒ‡ãƒã‚¤ã‚¹ã«å¯¾ã—è¤‡æ•°ã®äºˆå…†ãƒ­ã‚°ãŒã‚ã‚‹å ´åˆã€ãã‚Œã‚‰ã‚’çµ±åˆã—ã¦ã‚¹ã‚³ã‚¢ã‚’ãƒ–ãƒ¼ã‚¹ãƒˆã€‚
 - Fix 3: éšœå®³æ¸ˆã¿é™¤å¤–ã€‚æ—¢ã«CRITICALåˆ¤å®šã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã¯äºˆå…†å¯¾è±¡ã‹ã‚‰å¤–ã™ã€‚
 - Fix 4: æœªå®šç¾©ãƒ¡ã‚½ãƒƒãƒ‰ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£ (_build_prediction ã‚’å®Ÿè£…)ã€‚

è¨­è¨ˆæ–¹é‡:
 - ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€Œæœ¬ç•ªé‹ç”¨ã€ã«è€ãˆã†ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’æŒã¤ã€‚
 - å…¥åŠ›å…ƒãŒãƒ‡ãƒ¢(app.py)ã§ã‚‚æœ¬ç•ª(Syslog)ã§ã‚‚å‹•ä½œã™ã‚‹ã€‚
"""

import logging
import numpy as np
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass

try:
    import networkx as nx
    HAS_NX = True
except ImportError:
    HAS_NX = False

try:
    from sentence_transformers import SentenceTransformer
    HAS_BERT = True
except ImportError:
    HAS_BERT = False

logger = logging.getLogger(__name__)

# ==========================================================
# Escalation Rules (ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ãè¨­å®š)
# ==========================================================
@dataclass
class EscalationRule:
    pattern: str
    semantic_phrases: List[str]
    escalated_state: str
    time_to_critical_min: int
    early_warning_hours: int
    base_confidence: float
    category: str = "Generic"

ESCALATION_RULES: List[EscalationRule] = [
    EscalationRule("stp_loop", ["stp loop", "tcn received", "blocking port"], "L2ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚¹ãƒˆãƒ¼ãƒ ", 5, 24, 0.95, "Network/L2"),
    EscalationRule("mac_flap", ["mac flapping", "host moving"], "MACãƒ†ãƒ¼ãƒ–ãƒ«ä¸å®‰å®šåŒ–ã«ã‚ˆã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ æ¶ˆå¤±", 10, 24, 0.90, "Network/L2"),
    EscalationRule("arp_storm", ["arp storm", "duplicate ip"], "ARPãƒ†ãƒ¼ãƒ–ãƒ«æ±šæŸ“ã«ã‚ˆã‚‹é€šä¿¡æ–­", 10, 12, 0.85, "Network/L2"),
    EscalationRule("bgp_flap", ["bgp flapping", "neighbor down", "route oscillation"], "BGPã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸å®‰å®šåŒ–ã«ã‚ˆã‚‹çµŒè·¯æ¶ˆå¤±", 15, 48, 0.90, "Network/Routing"),
    EscalationRule("ospf_adj", ["ospf adjacency down", "dead timer"], "OSPFãƒã‚¤ãƒãƒ¼å–ªå¤±ã«ã‚ˆã‚‹ãƒ–ãƒ©ãƒƒã‚¯ãƒ›ãƒ¼ãƒ«", 15, 24, 0.85, "Network/Routing"),
    EscalationRule("ha_split", ["ha state degraded", "heartbeat lost"], "HAåŒæœŸä¸å…¨ã«ã‚ˆã‚‹ã‚¹ãƒ—ãƒªãƒƒãƒˆãƒ–ãƒ¬ã‚¤ãƒ³", 30, 48, 0.85, "Network/HA"),
    EscalationRule("bandwidth", ["bandwidth exceeded", "output drops"], "å¸¯åŸŸé£½å’Œã«ã‚ˆã‚‹ã‚µãƒ¼ãƒ“ã‚¹å“è³ªåŠ£åŒ–", 20, 72, 0.80, "Network/QoS"),
    EscalationRule("drop_error", ["input errors", "crc error", "fcs error"], "ç‰©ç†å›ç·šå“è³ªåŠ£åŒ–ã«ã‚ˆã‚‹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä½ä¸‹", 30, 168, 0.75, "Network/Interface"),
    EscalationRule("ntp_drift", ["ntp unsynchronized", "time drift"], "æ™‚åˆ»ä¸æ•´åˆã«ã‚ˆã‚‹èªè¨¼ã‚¨ãƒ©ãƒ¼", 120, 168, 0.70, "Network/Service"),
    EscalationRule("dhcp_dns", ["dhcp pool exhausted", "dns timeout"], "æ–°è¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šä¸å¯", 30, 48, 0.80, "Network/Service"),
    EscalationRule("optical", ["rx power", "optical signal", "transceiver", "light level", "dbm"], "å…‰ä¿¡å·åŠ£åŒ–ã«ã‚ˆã‚‹çªç„¶ã®ãƒªãƒ³ã‚¯ãƒ€ã‚¦ãƒ³", 60, 336, 0.95, "Hardware/Optical"),
    EscalationRule("temperature", ["temperature high", "overheat"], "ç†±æš´èµ°ã«ã‚ˆã‚‹ç·Šæ€¥ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³", 30, 48, 0.85, "Hardware/Thermal"),
    EscalationRule("fan_fail", ["fan failure", "fan malfunction"], "å†·å´èƒ½åŠ›å–ªå¤±ã«ã‚ˆã‚‹æ¸©åº¦ä¸Šæ˜‡", 45, 72, 0.80, "Hardware/Thermal"),
    EscalationRule("power_quality", ["ups on battery", "power supply failed"], "é›»æºä¾›çµ¦ä¸å®‰å®šã«ã‚ˆã‚‹å†èµ·å‹•", 15, 24, 0.85, "Hardware/Power"),
    EscalationRule("storage", ["flash error", "nvram corruption"], "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšœå®³ã«ã‚ˆã‚‹èµ·å‹•ä¸èƒ½", 180, 720, 0.75, "Hardware/Storage"),
    EscalationRule("memory_leak", ["memory usage high", "malloc fail"], "ãƒ¡ãƒ¢ãƒªæ¯æ¸‡ã«ã‚ˆã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ãƒƒã‚·ãƒ¥", 180, 336, 0.85, "Software/Resource"),
    EscalationRule("cpu_load", ["cpu usage high", "load average high"], "CPUæ¯æ¸‡ã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒ€ã‚¦ãƒ³", 20, 48, 0.85, "Software/Resource"),
    EscalationRule("process_crash", ["process terminated", "core dump"], "é‡è¦ãƒ—ãƒ­ã‚»ã‚¹åœæ­¢", 10, 24, 0.90, "Software/Process"),
    EscalationRule("auth_failure", ["authentication failed", "radius timeout"], "èªè¨¼åŸºç›¤éšœå®³", 15, 12, 0.80, "Security/Auth"),
    EscalationRule("crypto_vpn", ["ike sa deleted", "vpn tunnel down"], "VPNãƒˆãƒ³ãƒãƒ«åˆ‡æ–­", 60, 720, 0.80, "Security/Crypto"),
    EscalationRule("generic_error", ["error", "fail", "critical", "warning"], "æœªåˆ†é¡ã®ã‚µãƒ¼ãƒ“ã‚¹åŠ£åŒ–", 30, 24, 0.50, "Generic"),
]

# ==========================================================
# Digital Twin Engine
# ==========================================================
class DigitalTwinEngine:
    """
    æœ¬ç•ªé‹ç”¨ã«è€ãˆã†ã‚‹AIOpsã‚¨ãƒ³ã‚¸ãƒ³ã€‚
    ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã¨ã€NetworkXã«ã‚ˆã‚‹ã‚°ãƒ©ãƒ•è§£æã‚’æä¾›ã™ã‚‹ã€‚
    """
    _model: Optional[Any] = None
    _rule_embeddings: Optional[Dict[str, Any]] = None
    _model_loaded: bool = False

    # è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    MIN_PREDICTION_CONFIDENCE = 0.40
    MAX_PROPAGATION_HOPS = 3
    REDUNDANCY_DISCOUNT = 0.15
    SPOF_BOOST = 1.10
    EMBEDDING_THRESHOLD = 0.40
    MULTI_SIGNAL_BOOST = 0.05 # è¿½åŠ ã‚·ã‚°ãƒŠãƒ«ã”ã¨ã®åŠ ç‚¹

    def __init__(self, topology: Dict[str, Any], children_map: Optional[Dict[str, List[str]]] = None):
        self.topology = topology
        self.children_map = children_map or {}
        self.graph = None
        
        # --- NetworkX ã‚°ãƒ©ãƒ•æ§‹ç¯‰ (ä¿®æ­£ç‰ˆ: æ–¹å‘æ€§ã®å³å¯†åŒ–) ---
        if HAS_NX:
            self.graph = nx.DiGraph()
            for node_id, attrs in topology.items():
                node_attrs = attrs if isinstance(attrs, dict) else vars(attrs)
                self.graph.add_node(node_id, **node_attrs)
                
                # ãƒˆãƒãƒ­ã‚¸ãƒ¼ã‹ã‚‰ã‚¨ãƒƒã‚¸ã‚’æ§‹ç¯‰
                # è¦ª(Upstream) -> å­(Downstream) ã®æ–¹å‘ã‚’ "downstream" relation ã¨ã—ã¦å®šç¾©
                parent_id = node_attrs.get("parent_id")
                if parent_id and parent_id in topology:
                    self.graph.add_edge(parent_id, node_id, relation="downstream")
                    self.graph.add_edge(node_id, parent_id, relation="upstream")
                
                # children_map ã‹ã‚‰ã®è£œå®Œ (è¦ªIDã‚’æŒãŸãªã„ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰ç”¨)
                if node_id in self.children_map:
                    for child in self.children_map[node_id]:
                        if child in topology:
                            # æ—¢å­˜ã‚¨ãƒƒã‚¸ãŒãªã‘ã‚Œã°è¿½åŠ 
                            if not self.graph.has_edge(node_id, child):
                                self.graph.add_edge(node_id, child, relation="downstream")
                            if not self.graph.has_edge(child, node_id):
                                self.graph.add_edge(child, node_id, relation="upstream")

        self._redundancy_groups = self._build_redundancy_map()
        self._ensure_model_loaded()

    def _build_redundancy_map(self) -> Dict[str, List[str]]:
        rg_map = {}
        for dev_id, info in self.topology.items():
            attrs = info if isinstance(info, dict) else vars(info)
            rg = attrs.get('redundancy_group')
            if rg:
                rg_map.setdefault(rg, []).append(dev_id)
        return rg_map

    @classmethod
    def _ensure_model_loaded(cls):
        """Embeddingãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ (åˆå›ã®ã¿)"""
        if cls._model_loaded: return
        if not HAS_BERT:
            cls._model_loaded = True
            return
        try:
            cls._model = SentenceTransformer('all-MiniLM-L6-v2')
            all_phrases = []
            phrase_to_rule_idx = []
            for idx, rule in enumerate(ESCALATION_RULES):
                for phrase in rule.semantic_phrases:
                    all_phrases.append(phrase)
                    phrase_to_rule_idx.append(idx)
            if all_phrases:
                embeddings = cls._model.encode(all_phrases, convert_to_numpy=True)
                cls._rule_embeddings = {"vectors": embeddings, "phrase_to_rule_idx": phrase_to_rule_idx, "phrases": all_phrases}
            cls._model_loaded = True
        except:
            cls._model = None
            cls._model_loaded = True

    def _match_rule(self, alarm_text: str) -> Tuple[Optional[EscalationRule], float]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒãƒƒãƒãƒ³ã‚° (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ + Embedding)"""
        text_lower = alarm_text.lower()
        # 1. Keyword Match
        for rule in ESCALATION_RULES:
            if rule.pattern in text_lower:
                return rule, 1.0
        # 2. Embedding Match
        if self._model and self._rule_embeddings:
            try:
                query_vec = self._model.encode([alarm_text], convert_to_numpy=True)
                rule_vecs = self._rule_embeddings["vectors"]
                similarities = np.dot(rule_vecs, query_vec.T).flatten()
                norms = np.linalg.norm(rule_vecs, axis=1) * np.linalg.norm(query_vec)
                cosine_sim = similarities / np.where(norms==0, 1e-10, norms)
                best_idx = np.argmax(cosine_sim)
                best_score = float(cosine_sim[best_idx])
                if best_score >= self.EMBEDDING_THRESHOLD:
                    return ESCALATION_RULES[self._rule_embeddings["phrase_to_rule_idx"][best_idx]], best_score
            except: pass
        return None, 0.0

    def _get_downstream_impact(self, root_id: str) -> List[Tuple[str, int]]:
        """é…ä¸‹ãƒ‡ãƒã‚¤ã‚¹ã®æ¢ç´¢ (Graph Traversal)"""
        impacts = []
        if not self.graph or root_id not in self.graph: return impacts
        try:
            # relation="downstream" ã®ã‚¨ãƒƒã‚¸ã ã‘ã‚’è¾¿ã‚‹ãƒ•ã‚£ãƒ«ã‚¿
            def downstream_filter(u, v):
                edge_data = self.graph[u][v]
                return edge_data.get("relation") == "downstream"
            
            subgraph = nx.subgraph_view(self.graph, filter_edge=downstream_filter)
            # BFSå®Ÿè¡Œ
            tree = nx.bfs_tree(subgraph, root_id, depth_limit=self.MAX_PROPAGATION_HOPS)
            for node in tree:
                if node == root_id: continue
                dist = nx.shortest_path_length(subgraph, root_id, node)
                impacts.append((node, dist))
        except Exception as e:
            logger.error(f"Traversal error: {e}")
        return impacts

    def _calculate_confidence(self, rule: EscalationRule, device_id: str, match_quality: float) -> float:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        attrs = self.topology.get(device_id, {})
        if not isinstance(attrs, dict): attrs = vars(attrs)
        
        rg = attrs.get('redundancy_group')
        has_redundancy = bool(rg and len(self._redundancy_groups.get(rg, [])) > 1)
        # é…ä¸‹ãŒã„ã‚‹ã®ã«å†—é•·åŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯SPOFã¨ã¿ãªã™
        downstream_count = len(self._get_downstream_impact(device_id))
        is_spof = bool(downstream_count > 0 and not has_redundancy)
        
        confidence = rule.base_confidence
        confidence *= (0.8 + 0.2 * match_quality)
        if has_redundancy: confidence *= (1.0 - self.REDUNDANCY_DISCOUNT)
        if is_spof: confidence *= self.SPOF_BOOST
        return min(0.99, max(0.1, confidence))

    def _build_prediction(self, dev_id, rule, quality, matched_signals, confidence, extra_signal_count, boost_factor):
        """äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰ (ãƒŠãƒ©ãƒ†ã‚£ãƒ–ç”Ÿæˆå«ã‚€)"""
        # å½±éŸ¿ç¯„å›²
        downstream = self._get_downstream_impact(dev_id)
        impact_count = len(downstream)
        
        # é…ä¸‹ãƒ‡ãƒã‚¤ã‚¹åã®ç”Ÿæˆ
        if impact_count == 0:
            affected_str = "é…ä¸‹ãªã—(End Node)"
        else:
            names = [d[0] for d in downstream[:3]]
            if impact_count > 3:
                names.append(f"ä»–{impact_count-3}å°")
            affected_str = ", ".join(names)

        # æ—©æœŸäºˆå…†ãƒ†ã‚­ã‚¹ãƒˆ
        if rule.early_warning_hours >= 24:
            early_str = f"æœ€å¤§ {rule.early_warning_hours // 24}æ—¥å‰"
        else:
            early_str = f"æœ€å¤§ {rule.early_warning_hours}æ™‚é–“å‰"
        
        # è¤‡æ•°ã‚·ã‚°ãƒŠãƒ«ã®æ³¨é‡ˆ
        multi_signal_note = ""
        if extra_signal_count > 0:
            boost_val = min(extra_signals * boost_factor, 0.20)
            multi_signal_note = f"\nãƒ»ç›¸é–¢åˆ†æ: ä»– {extra_signal_count} ä»¶ã®é–¢é€£ã‚·ã‚°ãƒŠãƒ«ã‚’æ¤œçŸ¥ (ç¢ºä¿¡åº¦ +{boost_val:.0%})"

        # ãƒ¡ã‚¤ãƒ³ã®æ¤œçŸ¥æ ¹æ‹ 
        primary_msg = matched_signals[0][2]

        return {
            "id": dev_id,
            "label": f"ğŸ”® [äºˆå…†] {rule.escalated_state}",
            "severity": "CRITICAL",
            "status": "CRITICAL",
            "prob": round(confidence, 2),
            "type": f"Predictive/{rule.category}",
            "tier": 1,
            "reason": (
                f"ã€Digital Twinæœªæ¥äºˆæ¸¬ã€‘\n"
                f"ãƒ»æ—©æœŸäºˆå…†: {early_str} ã‹ã‚‰æ¤œçŸ¥å¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³\n"
                f"ãƒ»æ€¥æ€§æœŸ: ç™ºç—‡å¾Œ {rule.time_to_critical_min}åˆ† ã§æ·±åˆ»åŒ–ã™ã‚‹æã‚Œ\n"
                f"ãƒ»å½±éŸ¿ç¯„å›²: {affected_str} ({impact_count}å°) ãŒé€šä¿¡æ–­ã«ãªã‚‹ãƒªã‚¹ã‚¯\n"
                f"ãƒ»æ¨å¥¨: æ¬¡å›ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã®äºˆé˜²äº¤æ›/å¯¾å¿œ\n"
                f"--------------------------------\n"
                f"ãƒ»æ¤œå‡ºæ ¹æ‹ : {primary_msg} (Match: {quality:.2f}){multi_signal_note}"
            ),
            "is_prediction": True,
            # UIè¡¨ç¤ºç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            "prediction_timeline": f"{rule.time_to_critical_min}åˆ†å¾Œ",
            "prediction_early_warning": early_str,
            "prediction_affected_count": impact_count,
            "prediction_escalated_state": rule.escalated_state
        }

    def predict(self, analysis_results: List[Dict[str, Any]], msg_map: Dict[str, List[str]], alarms: Optional[List] = None) -> List[Dict[str, Any]]:
        """
        äºˆå…†æ¤œçŸ¥ã®å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰
        """
        predictions = []
        
        # 1. æ—¢ã«éšœå®³(CRITICAL)åˆ¤å®šã•ã‚Œã¦ã„ã‚‹æ©Ÿå™¨ã¯é™¤å¤–
        critical_ids = {
            r["id"] for r in analysis_results 
            if str(r.get("status")) == "HealthStatus.CRITICAL" or r.get("status") == "CRITICAL" or r.get("severity") == "CRITICAL"
        }

        # 2. å€™è£œé¸å®š
        # Warningæ©Ÿå™¨(prob 0.45~0.85) + ãƒ­ã‚°ãŒã‚ã‚‹å…¨æ©Ÿå™¨(INFOäºˆå…†å«ã‚€)
        warning_ids = {
            r["id"] for r in analysis_results
            if 0.45 <= float(r.get("prob", 0)) <= 0.85
        }
        active_ids = set(msg_map.keys())
        # éšœå®³æ¸ˆã¿ã‚’é™¤å¤–ã—ãŸå€™è£œãƒªã‚¹ãƒˆ
        candidates = (warning_ids.union(active_ids)) - critical_ids
        
        processed_devices = set()

        for dev_id in candidates:
            if dev_id in processed_devices: continue
            
            # ãƒ­ã‚°å–å¾—
            messages = msg_map.get(dev_id, [])
            if not messages: continue

            # 3. è¤‡æ•°ã‚·ã‚°ãƒŠãƒ«ã®å…¨ã‚¹ã‚­ãƒ£ãƒ³
            matched_signals = []  # [(rule, quality, msg), ...]
            
            for msg in messages:
                rule, quality = self._match_rule(msg)
                # ãƒãƒƒãƒåº¦ãŒä½ã„ã€ã¾ãŸã¯Genericãªã‚‚ã®ã¯ãƒã‚¤ã‚ºã¨ã—ã¦é™¤å¤–
                if not rule or quality < 0.35: continue
                if rule.pattern == "generic_error": continue
                
                matched_signals.append((rule, quality, msg))
            
            if not matched_signals: continue

            # æœ€ã‚‚ç¢ºä¿¡åº¦ãŒé«˜ã„ãƒ«ãƒ¼ãƒ«ã‚’ãƒ¡ã‚¤ãƒ³ã¨ã—ã¦æ¡ç”¨
            matched_signals.sort(key=lambda x: x[1], reverse=True)
            primary_rule, primary_quality, _ = matched_signals[0]

            # ä¿¡é ¼åº¦è¨ˆç®—
            confidence = self._calculate_confidence(primary_rule, dev_id, primary_quality)
            
            # è¤‡æ•°ã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒˆ
            extra_signals = len(matched_signals) - 1
            if extra_signals > 0:
                boost = min(extra_signals * self.MULTI_SIGNAL_BOOST, 0.20)
                confidence = min(0.99, confidence + boost)

            # é–¾å€¤åˆ¤å®š
            if confidence < self.MIN_PREDICTION_CONFIDENCE: continue

            # äºˆæ¸¬ç”Ÿæˆ
            pred = self._build_prediction(
                dev_id, primary_rule, primary_quality, matched_signals, 
                confidence, extra_signals, self.MULTI_SIGNAL_BOOST
            )
            
            predictions.append(pred)
            processed_devices.add(dev_id)
            
        return predictions

if __name__ == "__main__":
    # ç°¡æ˜“å‹•ä½œãƒ†ã‚¹ãƒˆ
    logging.basicConfig(level=logging.INFO)
    print("Initializing Digital Twin Engine...")
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒˆãƒãƒ­ã‚¸ãƒ¼
    topo = {
        "WAN_ROUTER": {"parent_id": None},
        "FW": {"parent_id": "WAN_ROUTER"},
        "SW": {"parent_id": "FW"}
    }
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ­ã‚° (è¤‡æ•°ã‚·ã‚°ãƒŠãƒ«)
    msgs = {
        "WAN_ROUTER": [
            "Rx Power -25.5 dBm (Threshold -25.0)", # Optical
            "Interface CRC errors increasing",      # Drop Error
            "Link fluctuation detected"             # Optical (Semantic match)
        ]
    }
    dummy_results = [] # ä½•ã‚‚éšœå®³ãŒå‡ºã¦ã„ãªã„çŠ¶æ…‹
    
    dt = DigitalTwinEngine(topo)
    preds = dt.predict(dummy_results, msgs)
    
    print(f"Predictions Generated: {len(preds)}")
    for p in preds:
        print(f"[{p['id']}] {p['label']} (Prob: {p['prob']})")
        print(p['reason'])
