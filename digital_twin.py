# -*- coding: utf-8 -*-
"""
digital_twin.py (v2.0 - Predictive Maintenance Edition)
========================================================
AIOps Digital Twin Engine

[ä¿®æ­£å±¥æ­´]
 - Fix 1: prob ã‚’ str("72%") ã‹ã‚‰ float(0.72) ã«å¤‰æ›´ (app.pyäº’æ›æ€§ç¢ºä¿)
 - Fix 2: predict() ã« alarms å¼•æ•°ã‚’è¿½åŠ  (inference_engineå‘¼ã³å‡ºã—äº’æ›æ€§ç¢ºä¿)
 - Fix 3: å€™è£œé¸å®šã‚’å³æ ¼åŒ– (ç¢ºç‡0.45-0.85ã®Warningã®ã¿å¯¾è±¡ã€Criticalé™¤å¤–)
 - Fix 4: Enumæ¯”è¼ƒãƒã‚°ä¿®æ­£ (statusæ–‡å­—åˆ—åˆ¤å®šã§ã¯ãªãprobæ•°å€¤åˆ¤å®šã¸å¤‰æ›´)
 - v2.0: EscalationRule ã« early_warning_hours è¿½åŠ ã€11â†’21ãƒ«ãƒ¼ãƒ«æ‹¡å……
         ãƒŠãƒ©ãƒ†ã‚£ãƒ–2è»¸åŒ–ï¼ˆæ—©æœŸäºˆå…†+æ€¥æ€§æœŸï¼‰ã€æ–°è¦å‡ºåŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ 
         MIN_PREDICTION_CONFIDENCE 0.50â†’0.40

è¨­è¨ˆæ–¹é‡:
 - UXå¤‰æ›´ãªã—: inference_engine ã‹ã‚‰å‘¼ã³å‡ºã—ã€äºˆå…†ãƒ‡ãƒ¼ã‚¿ã‚’æ³¨å…¥ã™ã‚‹ã€‚
 - Hybrid Matching: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´(é«˜é€Ÿ) + Embeddingé¡ä¼¼åº¦(æŸ”è»Ÿ)
"""

import logging
import numpy as np
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass

try:
    import networkx as nx
    HAS_NX = True
except ImportError:
    HAS_NX = False

try:
    from sentence_transformers import SentenceTransformer
    HAS_BERT = True
except ImportError:
    HAS_BERT = False

logger = logging.getLogger(__name__)

# ==========================================================
# Escalation Rules (ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜) - v2.0: L1-L7å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚«ãƒãƒ¼
# ==========================================================
@dataclass
class EscalationRule:
    pattern: str
    semantic_phrases: List[str]
    escalated_state: str
    time_to_critical_min: int       # æ€¥æ€§æœŸï¼ˆåˆ†ï¼‰- æ—¢å­˜
    early_warning_hours: int        # æ—©æœŸäºˆå…†ï¼ˆæ™‚é–“ï¼‰- â˜…æ–°è¦
    base_confidence: float
    category: str = "Generic"

ESCALATION_RULES: List[EscalationRule] = [
    # --- Network / L2 (Critical & Fast) ---
    EscalationRule(
        "stp_loop",
        ["stp loop", "spanning tree topology change", "tcn received", "bpdu guard",
         "blocking port", "loop guard", "root bridge change", "excessive broadcasts"],
        "L2ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚¹ãƒˆãƒ¼ãƒ ï¼ˆå…¨æ–­ï¼‰",
        time_to_critical_min=5, early_warning_hours=24, base_confidence=0.95,
        category="Network/L2"
    ),
    EscalationRule(
        "mac_flap",
        ["mac flapping", "host moving", "mac address move", "mac table overflow",
         "learning disable", "duplicate mac"],
        "MACãƒ†ãƒ¼ãƒ–ãƒ«ä¸å®‰å®šåŒ–ã«ã‚ˆã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ æ¶ˆå¤±",
        time_to_critical_min=10, early_warning_hours=24, base_confidence=0.90,
        category="Network/L2"
    ),
    EscalationRule(
        "arp_storm",
        ["arp storm", "duplicate ip", "gratuitous arp", "arp table overflow",
         "arp rate limit", "neighbor table full"],
        "ARPãƒ†ãƒ¼ãƒ–ãƒ«æ±šæŸ“ã«ã‚ˆã‚‹é€šä¿¡æ–­",
        time_to_critical_min=10, early_warning_hours=12, base_confidence=0.85,
        category="Network/L2"
    ),

    # --- Network / L3 & Routing ---
    EscalationRule(
        "bgp_flap",
        ["bgp flapping", "bgp neighbor down", "bgp session reset", "route oscillation",
         "prefix withdrawal", "hold timer expired", "notification received",
         # â˜… app.pyæ³¨å…¥äº’æ›: æ—§route_flapãƒ•ãƒ¬ãƒ¼ã‚º
         "route updates", "adjchange", "stability warning", "prefix flapping", "neighbor flap"],
        "BGPã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸å®‰å®šåŒ–ã«ã‚ˆã‚‹å¤§è¦æ¨¡çµŒè·¯æ¶ˆå¤±",
        time_to_critical_min=15, early_warning_hours=48, base_confidence=0.90,
        category="Network/Routing"
    ),
    EscalationRule(
        "ospf_adj",
        ["ospf adjacency down", "neighbor down", "dead timer expired", "lsa age",
         "database description", "retransmission limit", "spf calculation",
         # â˜… app.pyæ³¨å…¥äº’æ›: OSPF ADJCHANGE ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å¯¾å¿œ
         "adjchange", "keepalive delayed", "keepalive timeout"],
        "OSPFãƒã‚¤ãƒãƒ¼å–ªå¤±ã«ã‚ˆã‚‹ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—/ãƒ–ãƒ©ãƒƒã‚¯ãƒ›ãƒ¼ãƒ«",
        time_to_critical_min=15, early_warning_hours=24, base_confidence=0.85,
        category="Network/Routing"
    ),

    # --- Network / HA ---
    EscalationRule(
        "ha_split",
        ["ha state degraded", "failover state changed", "standby not ready",
         "heartbeat lost", "split brain", "cluster link down",
         # â˜… æ—§heartbeatãƒ«ãƒ¼ãƒ«çµ±åˆ
         "heartbeat failure", "keepalive timeout", "peer unreachable",
         "cluster communication lost", "redundancy state change"],
        "HAåŒæœŸä¸å…¨ã«ã‚ˆã‚‹ã‚¹ãƒ—ãƒªãƒƒãƒˆãƒ–ãƒ¬ã‚¤ãƒ³ç™ºç”Ÿ",
        time_to_critical_min=30, early_warning_hours=48, base_confidence=0.85,
        category="Network/HA"
    ),

    # --- Network / QoS & Performance ---
    EscalationRule(
        "bandwidth",
        ["bandwidth exceeded", "interface congestion", "shaping active", "policing drop",
         "tail drop", "queue full", "output drops",
         # â˜… app.pyæ³¨å…¥äº’æ›: æ—§dropãƒ•ãƒ¬ãƒ¼ã‚º(QoSç³»)
         "microburst", "buffer overflow", "queue congestion", "burst traffic"],
        "å¸¯åŸŸé£½å’Œã«ã‚ˆã‚‹ã‚µãƒ¼ãƒ“ã‚¹å“è³ªåŠ£åŒ–ï¼ˆé…å»¶ãƒ»ãƒ‘ã‚±ãƒƒãƒˆãƒ­ã‚¹ï¼‰",
        time_to_critical_min=20, early_warning_hours=72, base_confidence=0.80,
        category="Network/QoS"
    ),
    EscalationRule(
        "drop_error",
        ["input errors", "crc error", "symbol error", "runts", "giants",
         "interface resets", "fcs error",
         # â˜… app.pyæ³¨å…¥äº’æ›: æ—§dropãƒ•ãƒ¬ãƒ¼ã‚º(ã‚¨ãƒ©ãƒ¼ç³»)
         "input queue drops", "packet drops detected", "asic error"],
        "ç‰©ç†å›ç·šå“è³ªåŠ£åŒ–ã«ã‚ˆã‚‹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä½ä¸‹",
        time_to_critical_min=30, early_warning_hours=168, base_confidence=0.75,
        category="Network/Interface"
    ),

    # --- Network / Services ---
    EscalationRule(
        "ntp_drift",
        ["ntp unsynchronized", "stratum change", "peer unreachable", "time drift",
         "clock offset", "leap second"],
        "æ™‚åˆ»ä¸æ•´åˆã«ã‚ˆã‚‹ãƒ­ã‚°ä¸å…¨ãƒ»èªè¨¼ã‚¨ãƒ©ãƒ¼ãƒ»è¨¼æ˜æ›¸ç„¡åŠ¹åŒ–",
        time_to_critical_min=120, early_warning_hours=168, base_confidence=0.70,
        category="Network/Service"
    ),
    EscalationRule(
        "dhcp_dns",
        ["dhcp pool exhausted", "no ip address available", "dns timeout", "nxdomain",
         "server not responding", "discovery failed"],
        "æ–°è¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šä¸å¯",
        time_to_critical_min=30, early_warning_hours=48, base_confidence=0.80,
        category="Network/Service"
    ),

    # --- Hardware / Environmental ---
    EscalationRule(
        "optical",
        ["rx power low", "optical signal degradation", "light level warning",
         "transceiver threshold", "dbm low", "link fluctuation",
         # â˜… app.pyæ³¨å…¥äº’æ›: æ—§opticalãƒ•ãƒ¬ãƒ¼ã‚º
         "rx power", "signal degrading", "signal degradation",
         "threshold violation", "sfp rx power"],
        "å…‰ä¿¡å·åŠ£åŒ–ã«ã‚ˆã‚‹çªç„¶ã®ãƒªãƒ³ã‚¯ãƒ€ã‚¦ãƒ³",
        time_to_critical_min=60, early_warning_hours=336, base_confidence=0.90,
        category="Hardware/Optical"
    ),
    EscalationRule(
        "temperature",
        ["temperature high", "overheat", "thermal threshold", "intake air temp",
         "exhaust temp", "sensor alarm"],
        "ç†±æš´èµ°ã«ã‚ˆã‚‹ç·Šæ€¥ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³",
        time_to_critical_min=30, early_warning_hours=48, base_confidence=0.85,
        category="Hardware/Thermal"
    ),
    EscalationRule(
        "fan_fail",
        ["fan failure", "fan malfunction", "fan speed low", "fan tray removed",
         # â˜… æ—§fanãƒ«ãƒ¼ãƒ«äº’æ›
         "cooling failure", "fan speed critical", "temperature high"],
        "å†·å´èƒ½åŠ›å–ªå¤±ã«ã‚ˆã‚‹æ¸©åº¦ä¸Šæ˜‡",
        time_to_critical_min=45, early_warning_hours=72, base_confidence=0.80,
        category="Hardware/Thermal"
    ),
    EscalationRule(
        "power_quality",
        ["ups on battery", "input voltage low", "pdu alarm", "redundancy lost",
         "power supply failed", "psu failure",
         # â˜… æ—§powerãƒ«ãƒ¼ãƒ«äº’æ›
         "power redundancy lost", "power feed interrupted", "input power absent"],
        "é›»æºä¾›çµ¦ä¸å®‰å®šã«ã‚ˆã‚‹äºˆæœŸã›ã¬å†èµ·å‹•",
        time_to_critical_min=15, early_warning_hours=24, base_confidence=0.85,
        category="Hardware/Power"
    ),
    EscalationRule(
        "storage",
        ["flash error", "file system full", "nvram corruption", "disk fail",
         "write protect", "read error", "smart error"],
        "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšœå®³ã«ã‚ˆã‚‹è¨­å®šå–ªå¤±ãƒ»èµ·å‹•ä¸èƒ½",
        time_to_critical_min=180, early_warning_hours=720, base_confidence=0.75,
        category="Hardware/Storage"
    ),

    # --- Software / Resources & Process ---
    EscalationRule(
        "memory_leak",
        ["memory usage high", "malloc fail", "memory pool depletion", "heap exhaustion",
         "fragmentation", "leak detected",
         # â˜… æ—§memoryãƒ«ãƒ¼ãƒ«äº’æ›
         "high memory utilization", "memory threshold exceeded",
         "mbuf cluster limit reached", "resource exhaustion"],
        "ãƒ¡ãƒ¢ãƒªæ¯æ¸‡ã«ã‚ˆã‚‹OOM Killerç™ºå‹•ãƒ»ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ãƒƒã‚·ãƒ¥",
        time_to_critical_min=180, early_warning_hours=336, base_confidence=0.85,
        category="Software/Resource"
    ),
    EscalationRule(
        "cpu_load",
        ["cpu usage high", "cpu spike", "load average high", "control plane overload",
         "process stuck", "interrupt storm",
         # â˜… æ—§cpuãƒ«ãƒ¼ãƒ«äº’æ›
         "high cpu utilization", "cpu threshold exceeded", "cpuhog"],
        "CPUæ¯æ¸‡ã«ã‚ˆã‚‹ç®¡ç†ä¸èƒ½ãƒ»ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒ€ã‚¦ãƒ³",
        time_to_critical_min=20, early_warning_hours=48, base_confidence=0.85,
        category="Software/Resource"
    ),
    EscalationRule(
        "process_crash",
        ["process terminated", "segmentation fault", "core dump", "watchdog timeout",
         "daemon exit", "service restart"],
        "é‡è¦ãƒ—ãƒ­ã‚»ã‚¹åœæ­¢ã«ã‚ˆã‚‹åˆ¶å¾¡æ©Ÿèƒ½å–ªå¤±",
        time_to_critical_min=10, early_warning_hours=24, base_confidence=0.90,
        category="Software/Process"
    ),

    # --- Security ---
    EscalationRule(
        "auth_failure",
        ["authentication failed", "radius timeout", "tacacs unreachable", "invalid user",
         "login failed", "aaa server down"],
        "èªè¨¼åŸºç›¤éšœå®³ã«ã‚ˆã‚‹ç®¡ç†ã‚¢ã‚¯ã‚»ã‚¹ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¥ç¶šä¸èƒ½",
        time_to_critical_min=15, early_warning_hours=12, base_confidence=0.80,
        category="Security/Auth"
    ),
    EscalationRule(
        "crypto_vpn",
        ["ike sa deleted", "ipsec phase1 failed", "certificate expired", "decryption error",
         "vpn tunnel down", "proposal mismatch"],
        "VPN/æš—å·åŒ–ãƒˆãƒ³ãƒãƒ«ã®åˆ‡æ–­",
        time_to_critical_min=60, early_warning_hours=720, base_confidence=0.80,
        category="Security/Crypto"
    ),

    # --- Fallback ---
    EscalationRule(
        "generic_error",
        ["error", "fail", "critical", "warning", "emergency", "alert"],
        "æœªåˆ†é¡ã®ã‚µãƒ¼ãƒ“ã‚¹åŠ£åŒ–é€²è¡Œ",
        time_to_critical_min=30, early_warning_hours=24, base_confidence=0.50,
        category="Generic"
    ),
]

# ==========================================================
# Digital Twin Engine
# ==========================================================
class DigitalTwinEngine:
    _model: Optional[Any] = None
    _rule_embeddings: Optional[Dict[str, Any]] = None
    _model_loaded: bool = False

    MIN_PREDICTION_CONFIDENCE = 0.40   # â˜… v2.0: 0.50â†’0.40 (æ—©æœŸäºˆå…†æ¤œçŸ¥æ„Ÿåº¦å‘ä¸Š)
    MAX_PROPAGATION_HOPS = 3
    HOP_DECAY_RATE = 0.10
    REDUNDANCY_DISCOUNT = 0.15
    SPOF_BOOST = 1.10
    EMBEDDING_THRESHOLD = 0.40

    def __init__(self, topology: Dict[str, Any], children_map: Optional[Dict[str, List[str]]] = None):
        self.topology = topology
        self.children_map = children_map or {}

        self.graph = None
        if HAS_NX:
            self.graph = nx.DiGraph()
            for node_id, attrs in topology.items():
                node_attrs = attrs if isinstance(attrs, dict) else vars(attrs)
                self.graph.add_node(node_id, **node_attrs)
                parent_id = node_attrs.get("parent_id")
                if parent_id and parent_id in topology:
                    self.graph.add_edge(parent_id, node_id, relation="downstream")
                    self.graph.add_edge(node_id, parent_id, relation="upstream")

        self._redundancy_groups = self._build_redundancy_map()
        self._ensure_model_loaded()

    def _build_redundancy_map(self) -> Dict[str, List[str]]:
        rg_map = {}
        for dev_id, info in self.topology.items():
            attrs = info if isinstance(info, dict) else vars(info)
            rg = attrs.get('redundancy_group')
            if rg:
                rg_map.setdefault(rg, []).append(dev_id)
        return rg_map

    @classmethod
    def _ensure_model_loaded(cls):
        if cls._model_loaded:
            return
        if not HAS_BERT:
            logger.warning("sentence-transformers not available. Semantic matching disabled.")
            cls._model_loaded = True
            return
        try:
            import os
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹è§£æ±ºï¼ˆã‚¨ã‚¢ã‚®ãƒ£ãƒƒãƒ—å¯¾å¿œï¼‰:
            #   1. ç’°å¢ƒå¤‰æ•° DIGITAL_TWIN_MODEL_PATH (deploy_airgap.sh ãŒè¨­å®š)
            #   2. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ./models/all-MiniLM-L6-v2
            #   3. HuggingFaceå (ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç’°å¢ƒã®ã¿)
            model_path = os.environ.get("DIGITAL_TWIN_MODEL_PATH")
            if not model_path or not os.path.isdir(model_path):
                local_candidate = os.path.join(os.path.dirname(__file__), "models", "all-MiniLM-L6-v2")
                if os.path.isdir(local_candidate):
                    model_path = local_candidate
                else:
                    model_path = "all-MiniLM-L6-v2"  # HuggingFace fallback
            logger.info(f"Loading embedding model from: {model_path}")
            cls._model = SentenceTransformer(model_path)
            all_phrases = []
            phrase_to_rule_idx = []
            for idx, rule in enumerate(ESCALATION_RULES):
                for phrase in rule.semantic_phrases:
                    all_phrases.append(phrase)
                    phrase_to_rule_idx.append(idx)
            if all_phrases:
                embeddings = cls._model.encode(all_phrases, convert_to_numpy=True)
                cls._rule_embeddings = {
                    "vectors": embeddings,
                    "phrase_to_rule_idx": phrase_to_rule_idx,
                    "phrases": all_phrases
                }
            cls._model_loaded = True
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            cls._model = None
            cls._model_loaded = True

    def _match_rule(self, alarm_text: str) -> Tuple[Optional[EscalationRule], float]:
        text_lower = alarm_text.lower()

        # Phase 1: semantic_phrases ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç…§åˆï¼ˆå…¨ãƒ«ãƒ¼ãƒ«ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‰
        # generic_error ã¯ catch-all ãªã®ã§ Phase 1 ã§ã¯é™¤å¤–
        best_rule = None
        best_score = 0
        for rule in ESCALATION_RULES:
            if rule.pattern == "generic_error":
                continue
            hits = sum(1 for phrase in rule.semantic_phrases if phrase.lower() in text_lower)
            if hits > 0:
                score = hits + (hits / max(len(rule.semantic_phrases), 1)) * 0.1
                if score > best_score:
                    best_score = score
                    best_rule = rule
        if best_rule and best_score >= 1.0:
            quality = min(1.0, 0.7 + 0.15 * best_score)
            return best_rule, quality

        # Phase 2: å˜èªå¢ƒç•Œã¤ã pattern ãƒãƒƒãƒï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        import re
        for rule in ESCALATION_RULES:
            if rule.pattern == "generic_error":
                continue
            if re.search(r'\b' + re.escape(rule.pattern) + r'\b', text_lower):
                return rule, 1.0
        if self._model and self._rule_embeddings:
            try:
                query_vec = self._model.encode([alarm_text], convert_to_numpy=True)
                rule_vecs = self._rule_embeddings["vectors"]
                similarities = np.dot(rule_vecs, query_vec.T).flatten()
                norms = np.linalg.norm(rule_vecs, axis=1) * np.linalg.norm(query_vec)
                norms = np.where(norms == 0, 1e-10, norms)
                cosine_sim = similarities / norms
                best_idx = np.argmax(cosine_sim)
                best_score = float(cosine_sim[best_idx])
                if best_score >= self.EMBEDDING_THRESHOLD:
                    rule_idx = self._rule_embeddings["phrase_to_rule_idx"][best_idx]
                    return ESCALATION_RULES[rule_idx], best_score
            except Exception as e:
                logger.error(f"Embedding matching error: {e}")
        return None, 0.0

    def _get_downstream_impact(self, root_id: str) -> List[Tuple[str, int]]:
        impacts = []
        if not self.graph or root_id not in self.graph:
            return impacts
        try:
            def downstream_filter(u, v):
                return self.graph[u][v].get("relation") == "downstream"
            subgraph = nx.subgraph_view(self.graph, filter_edge=downstream_filter)
            tree = nx.bfs_tree(subgraph, root_id, depth_limit=self.MAX_PROPAGATION_HOPS)
            for node in tree:
                if node == root_id: continue
                dist = nx.shortest_path_length(subgraph, root_id, node)
                impacts.append((node, dist))
        except Exception as e:
            logger.error(f"Graph traversal error: {e}")
        return impacts

    def _calculate_confidence(self, rule: EscalationRule, device_id: str, match_quality: float) -> float:
        attrs = self.topology.get(device_id, {})
        if not isinstance(attrs, dict): attrs = vars(attrs)
        rg = attrs.get('redundancy_group')
        has_redundancy = False
        if rg and len(self._redundancy_groups.get(rg, [])) > 1:
            has_redundancy = True
        is_spof = False
        children = self.children_map.get(device_id, [])
        if children and not has_redundancy:
            is_spof = True
        confidence = rule.base_confidence
        confidence *= (0.8 + 0.2 * match_quality)
        if has_redundancy:
            confidence *= (1.0 - self.REDUNDANCY_DISCOUNT)
        if is_spof:
            confidence *= self.SPOF_BOOST
        return min(0.99, max(0.1, confidence))

    @staticmethod
    def _format_early_warning(hours: int) -> str:
        """æ—©æœŸäºˆå…†ã®è¡¨ç¤ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if hours >= 24:
            return f"æœ€å¤§ {hours // 24}æ—¥å‰"
        return f"æœ€å¤§ {hours}æ™‚é–“å‰"

    def _build_narrative(self, primary_rule: EscalationRule, matched_signals: list,
                         affected_str: str, confidence: float, extra_signals: int,
                         multi_signal_boost: float) -> str:
        """â˜… v2.0: 2è»¸ãƒŠãƒ©ãƒ†ã‚£ãƒ–ï¼ˆæ—©æœŸäºˆå…† + æ€¥æ€§æœŸï¼‰ã‚’ç”Ÿæˆ"""
        signal_lines = []
        for i, (r, q, m) in enumerate(matched_signals, 1):
            signal_lines.append(f"  ã‚·ã‚°ãƒŠãƒ«{i}: {m[:80]} (Match: {q:.2f})")
        signals_text = "\n".join(signal_lines)

        correlation_note = ""
        if extra_signals > 0:
            correlation_note = (
                f"\n  â˜… {len(matched_signals)}ä»¶ã®ç›¸é–¢ã‚·ã‚°ãƒŠãƒ«ã‚’æ¤œå‡º â†’ "
                f"ä¿¡é ¼åº¦ +{extra_signals * multi_signal_boost:.0%} ãƒ–ãƒ¼ã‚¹ãƒˆ"
            )

        early_warning_str = self._format_early_warning(primary_rule.early_warning_hours)

        return (
            f"ã€Digital Twinæœªæ¥äºˆæ¸¬ (Predictive Maintenance)ã€‘\n"
            f"{signals_text}{correlation_note}\n"
            f"ãƒ»æ—©æœŸäºˆå…†: {early_warning_str} ã‹ã‚‰æ¤œçŸ¥å¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³\n"
            f"ãƒ»æ€¥æ€§æœŸé€²è¡Œ: ç™ºç—‡å¾Œ {primary_rule.time_to_critical_min}åˆ† ã§æ·±åˆ»åŒ–ã™ã‚‹æã‚Œ\n"
            f"ãƒ»æ¨å¥¨: ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã®äºˆé˜²äº¤æ›/å¯¾å¿œ\n"
            f"å½±éŸ¿: {affected_str} ãŒé€£é–çš„ã«é€šä¿¡æ–­ã«ãªã‚Šã¾ã™ã€‚\n"
            f"(ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {confidence:.2f})"
        )

    def _build_prediction(self, dev_id: str, primary_rule: EscalationRule,
                          primary_quality: float, matched_signals: list,
                          confidence: float, extra_signals: int,
                          multi_signal_boost: float) -> Dict[str, Any]:
        """äºˆå…†äºˆæ¸¬è¾æ›¸ã‚’æ§‹ç¯‰ï¼ˆPrimary / Secondary å…±é€šï¼‰"""
        downstream = self._get_downstream_impact(dev_id)
        impact_count = len(downstream)

        affected_names = [d[0] for d in downstream[:3]]
        if impact_count > 3:
            affected_names.append(f"ä»–{impact_count - 3}å°")
        affected_str = ", ".join(affected_names) if affected_names else "é…ä¸‹ãªã—"

        reason = self._build_narrative(
            primary_rule, matched_signals, affected_str,
            confidence, extra_signals, multi_signal_boost
        )

        return {
            # --- æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆç¶­æŒå¿…é ˆï¼‰ ---
            "id": dev_id,
            "label": f"ğŸ”® [äºˆå…†] {primary_rule.escalated_state}",
            "severity": "CRITICAL",
            "status": "CRITICAL",
            "prob": round(confidence, 2),
            "type": f"Predictive/{primary_rule.category}",
            "tier": 1,
            "reason": reason,
            "is_prediction": True,
            "prediction_timeline": f"{primary_rule.time_to_critical_min}åˆ†å¾Œ",       # â˜…ç¶­æŒå¿…é ˆ
            "prediction_affected_count": impact_count,                                # â˜…ç¶­æŒå¿…é ˆ
            "prediction_affected_devices": [d[0] for d in downstream],
            "prediction_signal_count": len(matched_signals),                          # â˜…ç¶­æŒå¿…é ˆ
            "prediction_confidence_factors": {                                         # â˜…ç¶­æŒå¿…é ˆ
                "base": primary_rule.base_confidence,
                "match_quality": primary_quality,
                "has_redundancy": bool(self.topology.get(dev_id, {}).get('redundancy_group')),
                "is_spof": bool(self.children_map.get(dev_id) and not self.topology.get(dev_id, {}).get('redundancy_group')),
                "downstream_count": impact_count,
                "correlated_signals": len(matched_signals),
                "correlation_boost": extra_signals * multi_signal_boost if extra_signals > 0 else 0,
            },
            # --- æ–°è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆv2.0ï¼‰ ---
            "prediction_early_warning_hours": primary_rule.early_warning_hours,        # â˜…æ–°è¦
            "prediction_time_to_critical_min": primary_rule.time_to_critical_min,       # â˜…æ–°è¦
        }

    def predict(self, analysis_results: List[Dict[str, Any]], msg_map: Dict[str, List[str]], alarms: Optional[List] = None) -> List[Dict[str, Any]]:
        predictions = []
        MULTI_SIGNAL_BOOST = 0.08  # ã‚·ã‚°ãƒŠãƒ«1ä»¶è¿½åŠ ã”ã¨ã®ãƒ–ãƒ¼ã‚¹ãƒˆé‡

        warning_seeds = [
            r for r in analysis_results
            if 0.45 <= float(r.get("prob", 0)) <= 0.85
            and r.get("id", "") != "SYSTEM"
        ]
        candidates = {r["id"] for r in warning_seeds}
        processed_devices = set()

        for dev_id in candidates:
            if dev_id in processed_devices: continue
            messages = msg_map.get(dev_id, [])
            if not messages: continue

            # â˜… å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€ãƒãƒƒãƒã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’åé›†
            matched_signals = []
            for msg in messages:
                rule, quality = self._match_rule(msg)
                if rule and quality >= 0.30 and rule.pattern != "generic_error":
                    matched_signals.append((rule, quality, msg))

            if not matched_signals:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ generic ã‚’å«ã‚ã¦è©¦è¡Œ
                rule, quality = self._match_rule(messages[0])
                if not rule: continue
                matched_signals = [(rule, quality, messages[0])]

            # æœ€ã‚‚é«˜å“è³ªãªãƒ«ãƒ¼ãƒ«ã‚’ä¸»ãƒ«ãƒ¼ãƒ«ã¨ã—ã¦æ¡ç”¨
            matched_signals.sort(key=lambda x: x[1], reverse=True)
            primary_rule, primary_quality, primary_msg = matched_signals[0]

            downstream = self._get_downstream_impact(dev_id)
            confidence = self._calculate_confidence(primary_rule, dev_id, primary_quality)

            # â˜… è¤‡æ•°ã‚·ã‚°ãƒŠãƒ«ç›¸é–¢ãƒ–ãƒ¼ã‚¹ãƒˆ
            extra_signals = len(matched_signals) - 1
            if extra_signals > 0:
                boost = min(extra_signals * MULTI_SIGNAL_BOOST, 0.20)
                confidence = min(0.99, confidence + boost)

            if confidence < self.MIN_PREDICTION_CONFIDENCE:
                continue

            pred = self._build_prediction(
                dev_id, primary_rule, primary_quality, matched_signals,
                confidence, extra_signals, MULTI_SIGNAL_BOOST
            )
            predictions.append(pred)
            processed_devices.add(dev_id)

        # â˜… Secondary scan: Weak Signal ç›´æ¥æ¤œå‡º + è¤‡æ•°ã‚·ã‚°ãƒŠãƒ«ç›¸é–¢ãƒ–ãƒ¼ã‚¹ãƒˆ
        # LogicalRCA ãŒä½ã‚¹ã‚³ã‚¢ (< 0.45) ã‚’ã¤ã‘ãŸ INFO ã‚¢ãƒ©ãƒ¼ãƒ ã§ã‚‚ã€
        # Digital Twin ã®ãƒ«ãƒ¼ãƒ«ã«åˆè‡´ã™ã‚Œã°äºˆå…†ã¨ã—ã¦æ¤œå‡ºã™ã‚‹ã€‚
        # è¤‡æ•°ã®å¾®å¼±ã‚·ã‚°ãƒŠãƒ«ãŒç›¸é–¢ã™ã‚‹å ´åˆã€ä¿¡é ¼åº¦ã‚’æ®µéšçš„ã«ãƒ–ãƒ¼ã‚¹ãƒˆã™ã‚‹ã€‚

        for dev_id, messages in msg_map.items():
            if dev_id in processed_devices:
                continue
            if dev_id == "SYSTEM":
                continue
            if dev_id not in self.topology:
                continue

            # å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€ãƒãƒƒãƒã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’åé›†
            matched_signals = []  # [(rule, quality, msg), ...]
            seen_patterns = set()
            for msg in messages:
                rule, quality = self._match_rule(msg)
                if not rule or quality < 0.30:
                    continue
                if rule.pattern == "generic_error":
                    continue
                matched_signals.append((rule, quality, msg))
                seen_patterns.add(rule.pattern)

            if not matched_signals:
                continue

            # æœ€ã‚‚é«˜å“è³ªãªãƒ«ãƒ¼ãƒ«ã‚’ä¸»ãƒ«ãƒ¼ãƒ«ã¨ã—ã¦æ¡ç”¨
            matched_signals.sort(key=lambda x: x[1], reverse=True)
            primary_rule, primary_quality, primary_msg = matched_signals[0]

            downstream = self._get_downstream_impact(dev_id)
            confidence = self._calculate_confidence(primary_rule, dev_id, primary_quality)

            # â˜… è¤‡æ•°ã‚·ã‚°ãƒŠãƒ«ç›¸é–¢ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆè«–æ–‡: correlated weak signalsï¼‰
            # ç•°ãªã‚‹ãƒ«ãƒ¼ãƒ«ã«ãƒãƒƒãƒã™ã‚‹ã‚·ã‚°ãƒŠãƒ«ãŒå¤šã„ã»ã©ç¢ºä¿¡åº¦ãŒä¸ŠãŒã‚‹
            extra_signals = len(matched_signals) - 1
            if extra_signals > 0:
                boost = min(extra_signals * MULTI_SIGNAL_BOOST, 0.20)  # æœ€å¤§+20%
                confidence = min(0.99, confidence + boost)

            if confidence < 0.40:
                continue

            pred = self._build_prediction(
                dev_id, primary_rule, primary_quality, matched_signals,
                confidence, extra_signals, MULTI_SIGNAL_BOOST
            )
            predictions.append(pred)
            processed_devices.add(dev_id)

        return predictions
