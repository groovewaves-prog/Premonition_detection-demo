# -*- coding: utf-8 -*-
"""
digital_twin.py (Universal Edition - Production Ready v3.1)
===========================================================
AIOps Digital Twin Engine

[ä¿®æ­£å±¥æ­´]
 - Fix: predictãƒ¡ã‚½ãƒƒãƒ‰ã§ã®ã€Œéšœå®³æ¸ˆã¿æ©Ÿå™¨ã€ã®é™¤å¤–æ¡ä»¶ã‚’å³æ ¼åŒ–ã€‚
        prob >= 0.85 ã¾ãŸã¯ status == 'RED'/'CRITICAL' ã®æ©Ÿå™¨ã¯äºˆå…†æ¤œçŸ¥ã®å¯¾è±¡å¤–ã¨ã™ã‚‹ã€‚
"""

import logging
import numpy as np
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass

try:
    import networkx as nx
    HAS_NX = True
except ImportError:
    HAS_NX = False

try:
    from sentence_transformers import SentenceTransformer
    HAS_BERT = True
except ImportError:
    HAS_BERT = False

logger = logging.getLogger(__name__)

# ... (EscalationRuleã‚¯ãƒ©ã‚¹ã¨å®šæ•°ã¯å¤‰æ›´ãªã—ã€ãã®ã¾ã¾ç¶­æŒ) ...
# â€»ã‚¹ãƒšãƒ¼ã‚¹ç¯€ç´„ã®ãŸã‚çœç•¥ã—ã¾ã™ãŒã€ä»¥å‰ã®v3.0ã¨åŒã˜å®šç¾©ã‚’å«ã‚ã¦ãã ã•ã„

@dataclass
class EscalationRule:
    pattern: str
    semantic_phrases: List[str]
    escalated_state: str
    time_to_critical_min: int
    early_warning_hours: int
    base_confidence: float
    category: str = "Generic"

ESCALATION_RULES: List[EscalationRule] = [
    EscalationRule("stp_loop", ["stp loop", "tcn received", "blocking port"], "L2ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚¹ãƒˆãƒ¼ãƒ ", 5, 24, 0.95, "Network/L2"),
    EscalationRule("mac_flap", ["mac flapping", "host moving"], "MACãƒ†ãƒ¼ãƒ–ãƒ«ä¸å®‰å®šåŒ–ã«ã‚ˆã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ æ¶ˆå¤±", 10, 24, 0.90, "Network/L2"),
    EscalationRule("arp_storm", ["arp storm", "duplicate ip"], "ARPãƒ†ãƒ¼ãƒ–ãƒ«æ±šæŸ“ã«ã‚ˆã‚‹é€šä¿¡æ–­", 10, 12, 0.85, "Network/L2"),
    EscalationRule("bgp_flap", ["bgp flapping", "neighbor down", "route oscillation"], "BGPã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸å®‰å®šåŒ–ã«ã‚ˆã‚‹çµŒè·¯æ¶ˆå¤±", 15, 48, 0.90, "Network/Routing"),
    EscalationRule("ospf_adj", ["ospf adjacency down", "dead timer"], "OSPFãƒã‚¤ãƒãƒ¼å–ªå¤±ã«ã‚ˆã‚‹ãƒ–ãƒ©ãƒƒã‚¯ãƒ›ãƒ¼ãƒ«", 15, 24, 0.85, "Network/Routing"),
    EscalationRule("ha_split", ["ha state degraded", "heartbeat lost"], "HAåŒæœŸä¸å…¨ã«ã‚ˆã‚‹ã‚¹ãƒ—ãƒªãƒƒãƒˆãƒ–ãƒ¬ã‚¤ãƒ³", 30, 48, 0.85, "Network/HA"),
    EscalationRule("bandwidth", ["bandwidth exceeded", "output drops"], "å¸¯åŸŸé£½å’Œã«ã‚ˆã‚‹ã‚µãƒ¼ãƒ“ã‚¹å“è³ªåŠ£åŒ–", 20, 72, 0.80, "Network/QoS"),
    EscalationRule("drop_error", ["input errors", "crc error", "fcs error"], "ç‰©ç†å›ç·šå“è³ªåŠ£åŒ–ã«ã‚ˆã‚‹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä½ä¸‹", 30, 168, 0.75, "Network/Interface"),
    EscalationRule("ntp_drift", ["ntp unsynchronized", "time drift"], "æ™‚åˆ»ä¸æ•´åˆã«ã‚ˆã‚‹èªè¨¼ã‚¨ãƒ©ãƒ¼", 120, 168, 0.70, "Network/Service"),
    EscalationRule("dhcp_dns", ["dhcp pool exhausted", "dns timeout"], "æ–°è¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šä¸å¯", 30, 48, 0.80, "Network/Service"),
    EscalationRule("optical", ["rx power", "optical signal", "transceiver", "light level", "dbm"], "å…‰ä¿¡å·åŠ£åŒ–ã«ã‚ˆã‚‹çªç„¶ã®ãƒªãƒ³ã‚¯ãƒ€ã‚¦ãƒ³", 60, 336, 0.95, "Hardware/Optical"),
    EscalationRule("temperature", ["temperature high", "overheat"], "ç†±æš´èµ°ã«ã‚ˆã‚‹ç·Šæ€¥ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³", 30, 48, 0.85, "Hardware/Thermal"),
    EscalationRule("fan_fail", ["fan failure", "fan malfunction"], "å†·å´èƒ½åŠ›å–ªå¤±ã«ã‚ˆã‚‹æ¸©åº¦ä¸Šæ˜‡", 45, 72, 0.80, "Hardware/Thermal"),
    EscalationRule("power_quality", ["ups on battery", "power supply failed"], "é›»æºä¾›çµ¦ä¸å®‰å®šã«ã‚ˆã‚‹å†èµ·å‹•", 15, 24, 0.85, "Hardware/Power"),
    EscalationRule("storage", ["flash error", "nvram corruption"], "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšœå®³ã«ã‚ˆã‚‹èµ·å‹•ä¸èƒ½", 180, 720, 0.75, "Hardware/Storage"),
    EscalationRule("memory_leak", ["memory usage high", "malloc fail"], "ãƒ¡ãƒ¢ãƒªæ¯æ¸‡ã«ã‚ˆã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ãƒƒã‚·ãƒ¥", 180, 336, 0.85, "Software/Resource"),
    EscalationRule("cpu_load", ["cpu usage high", "load average high"], "CPUæ¯æ¸‡ã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒ€ã‚¦ãƒ³", 20, 48, 0.85, "Software/Resource"),
    EscalationRule("process_crash", ["process terminated", "core dump"], "é‡è¦ãƒ—ãƒ­ã‚»ã‚¹åœæ­¢", 10, 24, 0.90, "Software/Process"),
    EscalationRule("auth_failure", ["authentication failed", "radius timeout"], "èªè¨¼åŸºç›¤éšœå®³", 15, 12, 0.80, "Security/Auth"),
    EscalationRule("crypto_vpn", ["ike sa deleted", "vpn tunnel down"], "VPNãƒˆãƒ³ãƒãƒ«åˆ‡æ–­", 60, 720, 0.80, "Security/Crypto"),
    EscalationRule("generic_error", ["error", "fail", "critical", "warning"], "æœªåˆ†é¡ã®ã‚µãƒ¼ãƒ“ã‚¹åŠ£åŒ–", 30, 24, 0.50, "Generic"),
]

# ... (DigitalTwinEngineã‚¯ãƒ©ã‚¹ã®initç­‰ã¯ v3.0 ã¨åŒã˜ãŸã‚çœç•¥ã€predictãƒ¡ã‚½ãƒƒãƒ‰ã®ã¿ä¿®æ­£) ...

class DigitalTwinEngine:
    # (ã‚¯ãƒ©ã‚¹å¤‰æ•°ã¨__init__ãªã©ã¯å‰å›ã®v3.0ã¨åŒã˜)
    _model: Optional[Any] = None
    _rule_embeddings: Optional[Dict[str, Any]] = None
    _model_loaded: bool = False
    MIN_PREDICTION_CONFIDENCE = 0.40
    MAX_PROPAGATION_HOPS = 3
    REDUNDANCY_DISCOUNT = 0.15
    SPOF_BOOST = 1.10
    EMBEDDING_THRESHOLD = 0.40
    MULTI_SIGNAL_BOOST = 0.05

    def __init__(self, topology: Dict[str, Any], children_map: Optional[Dict[str, List[str]]] = None):
        self.topology = topology
        self.children_map = children_map or {}
        self.graph = None
        if HAS_NX:
            self.graph = nx.DiGraph()
            for node_id, attrs in topology.items():
                node_attrs = attrs if isinstance(attrs, dict) else vars(attrs)
                self.graph.add_node(node_id, **node_attrs)
                parent_id = node_attrs.get("parent_id")
                if parent_id and parent_id in topology:
                    self.graph.add_edge(parent_id, node_id, relation="downstream")
                    self.graph.add_edge(node_id, parent_id, relation="upstream")
                if node_id in self.children_map:
                    for child in self.children_map[node_id]:
                        if child in topology:
                            if not self.graph.has_edge(node_id, child):
                                self.graph.add_edge(node_id, child, relation="downstream")
                            if not self.graph.has_edge(child, node_id):
                                self.graph.add_edge(child, node_id, relation="upstream")
        self._redundancy_groups = self._build_redundancy_map()
        self._ensure_model_loaded()

    def _build_redundancy_map(self) -> Dict[str, List[str]]:
        rg_map = {}
        for dev_id, info in self.topology.items():
            attrs = info if isinstance(info, dict) else vars(info)
            rg = attrs.get('redundancy_group')
            if rg:
                rg_map.setdefault(rg, []).append(dev_id)
        return rg_map

    @classmethod
    def _ensure_model_loaded(cls):
        if cls._model_loaded: return
        if not HAS_BERT:
            cls._model_loaded = True
            return
        try:
            cls._model = SentenceTransformer('all-MiniLM-L6-v2')
            all_phrases = []
            phrase_to_rule_idx = []
            for idx, rule in enumerate(ESCALATION_RULES):
                for phrase in rule.semantic_phrases:
                    all_phrases.append(phrase)
                    phrase_to_rule_idx.append(idx)
            if all_phrases:
                embeddings = cls._model.encode(all_phrases, convert_to_numpy=True)
                cls._rule_embeddings = {"vectors": embeddings, "phrase_to_rule_idx": phrase_to_rule_idx, "phrases": all_phrases}
            cls._model_loaded = True
        except:
            cls._model = None
            cls._model_loaded = True

    def _match_rule(self, alarm_text: str) -> Tuple[Optional[EscalationRule], float]:
        text_lower = alarm_text.lower()
        for rule in ESCALATION_RULES:
            if rule.pattern in text_lower:
                return rule, 1.0
        if self._model and self._rule_embeddings:
            try:
                query_vec = self._model.encode([alarm_text], convert_to_numpy=True)
                rule_vecs = self._rule_embeddings["vectors"]
                similarities = np.dot(rule_vecs, query_vec.T).flatten()
                norms = np.linalg.norm(rule_vecs, axis=1) * np.linalg.norm(query_vec)
                cosine_sim = similarities / np.where(norms==0, 1e-10, norms)
                best_idx = np.argmax(cosine_sim)
                best_score = float(cosine_sim[best_idx])
                if best_score >= self.EMBEDDING_THRESHOLD:
                    return ESCALATION_RULES[self._rule_embeddings["phrase_to_rule_idx"][best_idx]], best_score
            except: pass
        return None, 0.0

    def _get_downstream_impact(self, root_id: str) -> List[Tuple[str, int]]:
        impacts = []
        if not self.graph or root_id not in self.graph: return impacts
        try:
            def downstream_filter(u, v):
                return self.graph[u][v].get("relation") == "downstream"
            subgraph = nx.subgraph_view(self.graph, filter_edge=downstream_filter)
            tree = nx.bfs_tree(subgraph, root_id, depth_limit=self.MAX_PROPAGATION_HOPS)
            for node in tree:
                if node == root_id: continue
                dist = nx.shortest_path_length(subgraph, root_id, node)
                impacts.append((node, dist))
        except: pass
        return impacts

    def _calculate_confidence(self, rule: EscalationRule, device_id: str, match_quality: float) -> float:
        attrs = self.topology.get(device_id, {})
        if not isinstance(attrs, dict): attrs = vars(attrs)
        rg = attrs.get('redundancy_group')
        has_redundancy = bool(rg and len(self._redundancy_groups.get(rg, [])) > 1)
        is_spof = bool(self.children_map.get(device_id) and not has_redundancy)
        confidence = rule.base_confidence
        confidence *= (0.8 + 0.2 * match_quality)
        if has_redundancy: confidence *= (1.0 - self.REDUNDANCY_DISCOUNT)
        if is_spof: confidence *= self.SPOF_BOOST
        return min(0.99, max(0.1, confidence))

    def _build_prediction(self, dev_id, rule, quality, matched_signals, confidence, extra_signal_count, boost_factor):
        downstream = self._get_downstream_impact(dev_id)
        impact_count = len(downstream)
        affected_names = [d[0] for d in downstream[:3]]
        if impact_count > 3: affected_names.append(f"ä»–{impact_count-3}å°")
        affected_str = ", ".join(affected_names) if affected_names else "é…ä¸‹ãªã—"
        
        if rule.early_warning_hours >= 24:
            early_str = f"æœ€å¤§ {rule.early_warning_hours // 24}æ—¥å‰"
        else:
            early_str = f"æœ€å¤§ {rule.early_warning_hours}æ™‚é–“å‰"
        
        multi_signal_note = ""
        if extra_signal_count > 0:
            boost_val = min(extra_signal_count * boost_factor, 0.20)
            multi_signal_note = f"\nãƒ»ç›¸é–¢åˆ†æ: ä»– {extra_signal_count} ä»¶ã®é–¢é€£ã‚·ã‚°ãƒŠãƒ«ã‚’æ¤œçŸ¥ (ç¢ºä¿¡åº¦ +{boost_val:.0%})"

        return {
            "id": dev_id,
            "label": f"ğŸ”® [äºˆå…†] {rule.escalated_state}",
            "severity": "CRITICAL",
            "status": "CRITICAL",
            "prob": round(confidence, 2),
            "type": f"Predictive/{rule.category}",
            "tier": 1,
            "reason": (
                f"ã€Digital Twinæœªæ¥äºˆæ¸¬ã€‘\n"
                f"ãƒ»æ—©æœŸäºˆå…†: {early_str} ã‹ã‚‰æ¤œçŸ¥å¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³\n"
                f"ãƒ»æ€¥æ€§æœŸ: ç™ºç—‡å¾Œ {rule.time_to_critical_min}åˆ† ã§æ·±åˆ»åŒ–ã™ã‚‹æã‚Œ\n"
                f"ãƒ»å½±éŸ¿ç¯„å›²: {affected_str} ({impact_count}å°) ãŒé€šä¿¡æ–­ã«ãªã‚‹ãƒªã‚¹ã‚¯\n"
                f"ãƒ»æ¨å¥¨: æ¬¡å›ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã®äºˆé˜²äº¤æ›/å¯¾å¿œ\n"
                f"--------------------------------\n"
                f"ãƒ»æ¤œå‡ºæ ¹æ‹ : {matched_signals[0][2]} (Match: {quality:.2f}){multi_signal_note}"
            ),
            "is_prediction": True,
            "prediction_timeline": f"{rule.time_to_critical_min}åˆ†å¾Œ",
            "prediction_early_warning_hours": rule.early_warning_hours,
            "prediction_affected_count": impact_count,
            "prediction_escalated_state": rule.escalated_state,
            "prediction_signal_count": len(matched_signals),
            "prediction_confidence_factors": {"base": rule.base_confidence}
        }

    def predict(self, analysis_results: List[Dict[str, Any]], msg_map: Dict[str, List[str]], alarms: Optional[List] = None) -> List[Dict[str, Any]]:
        predictions = []
        MULTI_SIGNAL_BOOST = 0.08

        # â˜… ä¿®æ­£: æ—¢ã«éšœå®³(CRITICAL/RED)åˆ¤å®šã•ã‚Œã¦ã„ã‚‹æ©Ÿå™¨ã¯ã€äºˆå…†æ¤œçŸ¥ã®å¯¾è±¡ã‹ã‚‰ç¢ºå®Ÿã«é™¤å¤–ã™ã‚‹
        # prob >= 0.85 ã¯éšœå®³ã¨ã¿ãªã™
        critical_ids = {
            r["id"] for r in analysis_results 
            if r.get("status") in ["RED", "CRITICAL"] or r.get("severity") == "CRITICAL" or r.get("prob", 0) >= 0.85
        }

        # å€™è£œé¸å®š: Warningæ©Ÿå™¨ + ãƒ­ã‚°ãŒã‚ã‚‹å…¨æ©Ÿå™¨ - éšœå®³æ¸ˆã¿æ©Ÿå™¨
        warning_ids = {
            r["id"] for r in analysis_results
            if 0.45 <= float(r.get("prob", 0)) <= 0.85
        }
        active_ids = set(msg_map.keys())
        candidates = (warning_ids.union(active_ids)) - critical_ids
        
        processed_devices = set()

        for dev_id in candidates:
            if dev_id in processed_devices: continue
            messages = msg_map.get(dev_id, [])
            if not messages: continue

            matched_signals = []
            for msg in messages:
                rule, quality = self._match_rule(msg)
                if rule and quality >= 0.30 and rule.pattern != "generic_error":
                    matched_signals.append((rule, quality, msg))

            if not matched_signals:
                rule, quality = self._match_rule(messages[0])
                if not rule: continue
                matched_signals = [(rule, quality, messages[0])]

            matched_signals.sort(key=lambda x: x[1], reverse=True)
            primary_rule, primary_quality, primary_msg = matched_signals[0]

            confidence = self._calculate_confidence(primary_rule, dev_id, primary_quality)
            extra_signals = len(matched_signals) - 1
            if extra_signals > 0:
                boost = min(extra_signals * MULTI_SIGNAL_BOOST, 0.20)
                confidence = min(0.99, confidence + boost)

            if confidence < self.MIN_PREDICTION_CONFIDENCE: continue

            pred = self._build_prediction(
                dev_id, primary_rule, primary_quality, matched_signals, 
                confidence, extra_signals, MULTI_SIGNAL_BOOST
            )
            predictions.append(pred)
            processed_devices.add(dev_id)
            
        return predictions
