# digital_twin_pkg/engine.py  â€•  DigitalTwinEngine ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆPhase1 Predict API + RULäºˆæ¸¬ï¼‰
import logging
import time
import json
import uuid
import re
import os
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import asdict
import traceback

from .config import *
from .rules import EscalationRule, DEFAULT_RULES, MAINTENANCE_SIGNATURES
from .storage import StorageManager
from .audit import AuditBuilder
from .tuning import AutoTuner
from .bayesian import BayesianInferenceEngine
from .gnn import create_gnn_engine

try:
    from sentence_transformers import SentenceTransformer
    HAS_BERT = True
except ImportError:
    HAS_BERT = False

logger = logging.getLogger(__name__)



# ==============================================================
# Phase1: Predict API + Forecast Ledger  (digital_twin_pkg)
# ==============================================================

import traceback
from dataclasses import asdict as _asdict

# DTO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from dataclasses import dataclass as _dc, field as _field
from typing import Optional as _Opt

@_dc
class PredictRequest:
    tenant_id:  str
    device_id:  str
    msg:        str
    timestamp:  float
    attrs:      dict = _field(default_factory=dict)

    def to_dict(self):
        return {"tenant_id": self.tenant_id, "device_id": self.device_id,
                "msg": self.msg, "timestamp": self.timestamp, "attrs": self.attrs or {}}

@_dc
class PredictResult:
    predicted_state:      str
    confidence:           float
    rule_pattern:         str
    category:             str
    reasons:              list = _field(default_factory=list)
    recommended_actions:  list = _field(default_factory=list)
    runbook_url:          str  = ""
    criticality:          str  = "standard"
    time_to_critical_min: int  = 60
    early_warning_hours:  int  = 24
    time_to_failure_hours: int = 336  # â˜… RUL: ä»Šã‹ã‚‰å®Œå…¨æ•…éšœã¾ã§ï¼ˆæ™‚é–“ï¼‰
    predicted_failure_datetime: str = ""  # â˜… æ•…éšœç™ºç”Ÿäºˆæ¸¬æ—¥æ™‚ï¼ˆISOå½¢å¼ï¼‰

    def to_dict(self, affected_count: int = 0, source: str = "real"):
        return {
            "predicted_state":      self.predicted_state,
            "confidence":           float(self.confidence),
            "rule_pattern":         self.rule_pattern,
            "category":             self.category,
            "reasons":              self.reasons or [],
            "recommended_actions":  self.recommended_actions or [],
            "runbook_url":          self.runbook_url or "",
            "criticality":          self.criticality or "standard",
            "time_to_critical_min": int(self.time_to_critical_min),
            "early_warning_hours":  int(self.early_warning_hours),
            "time_to_failure_hours": int(self.time_to_failure_hours),
            "predicted_failure_datetime": self.predicted_failure_datetime,
            # â”€â”€ cockpit.py äº’æ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            "is_prediction":        True,
            "source":               source,
            "prob":                 float(self.confidence),
            "label":                f"ğŸ”® [äºˆå…†] {self.predicted_state}",
            "type":                 f"Predictive/{self.category}",
            "tier":                 1,
            "prediction_timeline":  f"{self.time_to_critical_min}åˆ†å¾Œ",
            "prediction_time_to_critical_min": int(self.time_to_critical_min),
            "prediction_early_warning_hours":  int(self.early_warning_hours),
            "prediction_affected_count":       int(affected_count),
            "prediction_time_to_failure_hours": int(self.time_to_failure_hours),
            "prediction_failure_datetime":      self.predicted_failure_datetime,
        }


class DigitalTwinEngine:
    def __init__(self, topology: Dict[str, Any], children_map: Optional[Dict[str, List[str]]] = None, tenant_id: str = "default"):
        if not tenant_id or len(tenant_id) > 64: raise ValueError("Invalid tenant_id")
        self.tenant_id = tenant_id.lower()
        self.topology = topology
        self.children_map = children_map or {}
        self.storage = StorageManager(self.tenant_id, BASE_DIR)
        self.tuner = AutoTuner(self)
        self.bayesian = BayesianInferenceEngine(self.storage)  # â˜… ãƒ™ã‚¤ã‚ºæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
        self.gnn = create_gnn_engine(topology, children_map)  # â˜… GNNäºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³
        self.rules: List[EscalationRule] = []
        self._metric_rules: List[EscalationRule] = []
        self.history: List[Dict] = []
        self.outcomes: List[Dict] = []
        self.incident_register: List[Dict] = []
        self.maintenance_windows: List[Dict] = []
        self.evaluation_state: Dict = {}
        self.shadow_eval_state: Dict = {}
        self._model = None
        self._rule_embeddings = None
        self._model_loaded = False
        self._rules_sot = (os.environ.get(ENV_RULES_SOT, "json") or "json").strip().lower()
        self.reload_all()
        self._ensure_model_loaded()

    def reload_all(self):
        self._load_rules()
        self.history = self.storage.load_json("history", [])
        self.outcomes = self.storage.load_json("outcomes", [])
        self.incident_register = self.storage.load_json("incident_register", [])
        self.maintenance_windows = self.storage.load_json("maintenance_windows", [])
        self.evaluation_state = self.storage.load_json("evaluation_state", {})
        self.shadow_eval_state = self.storage.load_json("shadow_eval_state", {})
        self._init_forecast_ledger()

    def _sanitize_rule_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        return {k: v for k, v in data.items() if not k.startswith('_')}

    def _load_rules(self):
        loaded_from_db = False
        if self._rules_sot == "db":
            db_rules_json = self.storage.rule_config_get_all_json_strs()
            if db_rules_json:
                try:
                    self.rules = [EscalationRule(**self._sanitize_rule_data(json.loads(s))) for s in db_rules_json]
                    loaded_from_db = True
                except: pass
        if not loaded_from_db:
            path = self.storage.paths["rules"]
            if not os.path.exists(path):
                self.rules = [EscalationRule(**self._sanitize_rule_data(asdict(r))) for r in DEFAULT_RULES]
                self.storage.save_json_atomic("rules", [self._sanitize_rule_data(asdict(r)) for r in self.rules])
            else:
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    self.rules = [EscalationRule(**self._sanitize_rule_data(item)) for item in data]
                except Exception as e:
                    self.rules = [EscalationRule(**self._sanitize_rule_data(asdict(r))) for r in DEFAULT_RULES]
            self.storage._seed_rule_config_from_rules_json([self._sanitize_rule_data(asdict(r)) for r in self.rules])
        self._metric_rules = [r for r in self.rules if (r.requires_trend or r.requires_volatility) and r.trend_metric_regex]

    def _ensure_model_loaded(self):
        if self._model_loaded: return
        if not HAS_BERT:
            self._model_loaded = True
            return
        try:
            self._model = SentenceTransformer('all-MiniLM-L6-v2')
            phrases = []
            indices = []
            for idx, r in enumerate(self.rules):
                for p in r.semantic_phrases:
                    phrases.append(p)
                    indices.append(idx)
            if phrases:
                embeddings = self._model.encode(phrases, convert_to_numpy=True)
                self._rule_embeddings = {"vectors": embeddings, "indices": indices}
            self._model_loaded = True
        except: self._model_loaded = True

    def _match_rule(self, alarm_text: str) -> Tuple[Optional[EscalationRule], float]:
        text_lower = alarm_text.lower()
        for rule in self.rules:
            if rule._compiled_regex and rule._compiled_regex.search(alarm_text):
                return rule, 1.0
            if rule.pattern in text_lower:
                return rule, 1.0
        if self._model and self._rule_embeddings:
            try:
                query_vec = self._model.encode([alarm_text], convert_to_numpy=True)
                rule_vecs = self._rule_embeddings["vectors"]
                similarities = np.dot(rule_vecs, query_vec.T).flatten()
                norms = np.linalg.norm(rule_vecs, axis=1) * np.linalg.norm(query_vec)
                cosine_sim = similarities / np.where(norms==0, 1e-10, norms)
                best_idx = np.argmax(cosine_sim)
                best_score = float(cosine_sim[best_idx])
                rule_idx = self._rule_embeddings["indices"][best_idx]
                rule = self.rules[rule_idx]
                if best_score >= (rule.embedding_threshold or 0.40):
                    return rule, best_score
            except Exception: pass
        return None, 0.0

    def _calculate_confidence(self, rule: EscalationRule, device_id: str, match_quality: float) -> float:
        attrs = self.topology.get(device_id, {})
        if not isinstance(attrs, dict):
            try: attrs = vars(attrs)
            except: attrs = {}
        rg = attrs.get('redundancy_group')
        has_redundancy = bool(rg)
        children = self.children_map.get(device_id, [])
        is_spof = bool(children and not has_redundancy)
        confidence = rule.base_confidence
        confidence *= (0.8 + 0.2 * match_quality)
        if has_redundancy: confidence *= (1.0 - ROI_CONSERVATIVE_FACTOR * 0.2)
        if is_spof: confidence *= 1.1
        return min(0.99, max(0.1, confidence))

    def _sanitize_for_llm(self, text: str) -> str:
        """
        LLMé€ä¿¡å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        
        - IPã‚¢ãƒ‰ãƒ¬ã‚¹ã®ãƒã‚¹ã‚­ãƒ³ã‚°
        - ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆæƒ…å ±ã®é™¤å»
        - æ©Ÿå¯†æƒ…å ±ã®åŒ¿ååŒ–
        """
        import re
        
        sanitized = text
        
        # IPv4ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ãƒã‚¹ã‚­ãƒ³ã‚°
        sanitized = re.sub(
            r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b',
            'IP_MASKED',
            sanitized
        )
        
        # IPv6ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ãƒã‚¹ã‚­ãƒ³ã‚°
        sanitized = re.sub(
            r'\b(?:[0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}\b',
            'IPV6_MASKED',
            sanitized
        )
        
        # MACã‚¢ãƒ‰ãƒ¬ã‚¹ã®ãƒã‚¹ã‚­ãƒ³ã‚°
        sanitized = re.sub(
            r'\b(?:[0-9a-fA-F]{2}[:-]){5}[0-9a-fA-F]{2}\b',
            'MAC_MASKED',
            sanitized
        )
        
        # ãƒ›ã‚¹ãƒˆåã®ä¸€èˆ¬åŒ–ï¼ˆprod-, dev-, test-ãªã©ã‚’é™¤å»ï¼‰
        sanitized = re.sub(
            r'\b(prod|dev|test|stage|staging)-[\w-]+',
            'HOSTNAME_MASKED',
            sanitized,
            flags=re.IGNORECASE
        )
        
        # ASN (ASç•ªå·)ã®ãƒã‚¹ã‚­ãƒ³ã‚°
        sanitized = re.sub(
            r'\bAS\d+\b',
            'AS_MASKED',
            sanitized
        )
        
        # VLAN IDã®ãƒã‚¹ã‚­ãƒ³ã‚°
        sanitized = re.sub(
            r'\bVLAN\s*\d+\b',
            'VLAN_MASKED',
            sanitized,
            flags=re.IGNORECASE
        )
        
        return sanitized

    def _batch_generate_llm_recommendations(
        self,
        candidates: set,
        msg_map: Dict[str, List[str]]
    ) -> Dict[str, List[Dict[str, str]]]:
        """
        è¤‡æ•°ãƒ‡ãƒã‚¤ã‚¹ã®æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒãƒƒãƒç”Ÿæˆï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ãƒ»æ€§èƒ½å‘ä¸Šï¼‰
        
        åŒã˜ãƒ«ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã®äºˆå…†ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€1å›ã®LLMå‘¼ã³å‡ºã—ã§å‡¦ç†
        
        Args:
            candidates: äºˆå…†å€™è£œãƒ‡ãƒã‚¤ã‚¹IDã®ã‚»ãƒƒãƒˆ
            msg_map: ãƒ‡ãƒã‚¤ã‚¹ID â†’ ã‚¢ãƒ©ãƒ¼ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒãƒƒãƒ—
        
        Returns:
            ãƒ«ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ â†’ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒãƒƒãƒ—
        """
        from collections import defaultdict
        
        # ãƒ«ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        pattern_groups = defaultdict(list)
        
        for dev_id in candidates:
            messages = msg_map.get(dev_id, [])
            if not messages:
                continue
            
            # ä¸»è¦ãƒ«ãƒ¼ãƒ«ã‚’ç‰¹å®š
            matched_signals = []
            for msg in messages:
                rule, quality = self._match_rule(msg)
                if rule and quality >= 0.30 and rule.pattern != "generic_error":
                    matched_signals.append((rule, quality, msg))
            
            if not matched_signals:
                rule, quality = self._match_rule(messages[0])
                if not rule:
                    continue
                matched_signals = [(rule, quality, messages[0])]
            
            matched_signals.sort(key=lambda x: x[1], reverse=True)
            primary_rule, primary_quality, primary_msg = matched_signals[0]
            
            pattern_groups[primary_rule.pattern].append({
                'device_id': dev_id,
                'messages': [s[2] for s in matched_signals[:3]],
                'affected_count': len(matched_signals),
                'confidence': self._calculate_confidence(primary_rule, dev_id, primary_quality)
            })
        
        # ãƒãƒƒãƒLLMå‘¼ã³å‡ºã—
        llm_cache = {}
        WIDE_RANGE_THRESHOLD = 5
        
        for pattern, devices in pattern_groups.items():
            total_affected = sum(d['affected_count'] for d in devices)
            
            # åºƒç¯„å›²éšœå®³ã®å ´åˆã®ã¿LLMå‘¼ã³å‡ºã—
            if total_affected >= WIDE_RANGE_THRESHOLD:
                # ä»£è¡¨çš„ãªãƒ‡ãƒã‚¤ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                representative = max(devices, key=lambda d: d['affected_count'])
                
                llm_actions = self._generate_actions_with_gemini(
                    rule_pattern=pattern,
                    affected_count=total_affected,
                    confidence=sum(d['confidence'] for d in devices) / len(devices),
                    messages=representative['messages'],
                    device_id=representative['device_id']
                )
                
                if llm_actions:
                    llm_cache[pattern] = llm_actions
                    logger.info(f"Batch LLM: Generated actions for {pattern} "
                              f"({len(devices)} devices, {total_affected} total affected)")
        
        return llm_cache

    def _generate_actions_with_gemini(
        self,
        rule_pattern: str,
        affected_count: int,
        confidence: float,
        messages: List[str],
        device_id: str
    ) -> Optional[List[Dict[str, str]]]:
        """
        Gemini API ã‚’ä½¿ã£ã¦çŠ¶æ³ã«å¿œã˜ãŸæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‹•çš„ç”Ÿæˆ
        
        âš ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ãƒ‡ãƒ¼ã‚¿ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã—ã¦ã‹ã‚‰é€ä¿¡
        
        Args:
            rule_pattern: æ¤œå‡ºã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
            affected_count: å½±éŸ¿ã‚’å—ã‘ãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°
            confidence: äºˆæ¸¬ä¿¡é ¼åº¦
            messages: ã‚¢ãƒ©ãƒ¼ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
            device_id: ãƒ‡ãƒã‚¤ã‚¹ID
        
        Returns:
            æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ãƒˆã€ã¾ãŸã¯ Noneï¼ˆç”Ÿæˆå¤±æ•—æ™‚ï¼‰
        """
        try:
            import google.generativeai as genai
            import os
            import json
            
            # â˜… LLMé€ä¿¡ã®è¨­å®šç¢ºèªï¼ˆã‚ªãƒ—ãƒˆã‚¢ã‚¦ãƒˆå¯èƒ½ï¼‰
            enable_llm = os.environ.get("ENABLE_LLM_RECOMMENDATIONS", "true").lower()
            if enable_llm not in ["true", "1", "yes"]:
                logger.info("LLM recommendations disabled by configuration")
                return None
            
            # API ã‚­ãƒ¼ã®å–å¾—
            api_key = os.environ.get("GEMINI_API_KEY")
            if not api_key:
                logger.warning("GEMINI_API_KEY not found. Using static recommendations.")
                return None
            
            # â˜… ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆæ©Ÿå¯†æƒ…å ±ã®é™¤å»ï¼‰
            sanitized_device_id = self._sanitize_for_llm(device_id)
            # â˜… å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆæœ€å¤§50ä»¶ã¾ã§ï¼‰
            sanitized_messages = [self._sanitize_for_llm(msg) for msg in messages[:50]]
            
            # Gemini API ã®è¨­å®š
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            # ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ã®æ¨å®šï¼ˆä¸€èˆ¬åŒ–ï¼‰
            device_type = "Network Device"
            if "ROUTER" in device_id.upper():
                device_type = "Router"
            elif "SWITCH" in device_id.upper():
                device_type = "Switch"
            elif "FIREWALL" in device_id.upper():
                device_type = "Firewall"
            
            # â˜… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆï¼ˆå…¨ã‚¢ãƒ©ãƒ¼ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†æã•ã›ã‚‹ï¼‰
            prompt = f"""ã‚ãªãŸã¯20å¹´ä»¥ä¸Šã®çµŒé¨“ã‚’æŒã¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ©Ÿå™¨ã®éšœå®³å¯¾å¿œã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚

ã€ğŸš¨ ç·Šæ€¥: åºƒç¯„å›²éšœå®³ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‘

ã€æ¤œå‡ºã•ã‚ŒãŸäºˆå…†ã®è©³ç´°ã€‘
- ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—: {device_type}
- æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³: {rule_pattern}
- **å½±éŸ¿ç¯„å›²: {affected_count}å€‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ/ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹**
- äºˆæ¸¬ä¿¡é ¼åº¦: {confidence * 100:.0f}%
- **ã‚¢ãƒ©ãƒ¼ãƒ ä»¶æ•°: {len(sanitized_messages)}ä»¶**

ã€ğŸ“‹ å®Ÿéš›ã«æ¤œå‡ºã•ã‚ŒãŸã‚¢ãƒ©ãƒ¼ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆå…¨{len(sanitized_messages)}ä»¶ã€åŒ¿ååŒ–æ¸ˆã¿ï¼‰ã€‘
```
{chr(10).join(f"{i+1:3d}. {msg[:200]}" for i, msg in enumerate(sanitized_messages))}
```

ã€ğŸ” ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ - ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¢ãƒ©ãƒ¼ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æã€‘
ä¸Šè¨˜ã®{len(sanitized_messages)}ä»¶ã®ã‚¢ãƒ©ãƒ¼ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è©³ã—ãåˆ†æã—ã¦ãã ã•ã„ï¼š

**è³ªå•:**
1. ã©ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ/ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒå½±éŸ¿ã‚’å—ã‘ã¦ã„ã¾ã™ã‹ï¼Ÿ
   - åŒã˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ç•ªå·ãŒç¹°ã‚Šè¿”ã—ï¼Ÿ
   - è¤‡æ•°ã®ç•°ãªã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼Ÿ
   - ç¯„å›²ã¯ï¼Ÿï¼ˆä¾‹: Gi0/0/1ã‹ã‚‰Gi0/0/28ã¾ã§ï¼‰

2. ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã¯ï¼Ÿ
   - å…‰ä¿¡å·ãƒ¬ãƒ™ãƒ«ã®ä½ä¸‹ï¼ˆRx Power, Tx Powerï¼‰ï¼Ÿ
   - ãƒ‘ã‚±ãƒƒãƒˆãƒ‰ãƒ­ãƒƒãƒ—ï¼Ÿ
   - ãƒªãƒ³ã‚¯ãƒ•ãƒ©ãƒƒãƒ”ãƒ³ã‚°ï¼Ÿ
   
3. ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å…±é€šç‚¹ã¯ï¼Ÿ
   - å…¨ã¦åŒã˜ã‚¿ã‚¤ãƒ—ã®ã‚¨ãƒ©ãƒ¼ï¼Ÿ
   - ç‰¹å®šã®ç¯„å›²ã®ãƒãƒ¼ãƒˆç•ªå·ã«é›†ä¸­ï¼Ÿ
   - æ™‚ç³»åˆ—çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼Ÿ

ã€ğŸ”´ ã‚¹ãƒ†ãƒƒãƒ—2: çœŸå› ã®æ¨è«–ã€‘
**{affected_count}å€‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§åŒæ™‚ã«{rule_pattern}ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã¾ã™ã€‚**

ä¸Šè¨˜ã®ã‚¢ãƒ©ãƒ¼ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã«åŸºã¥ã„ã¦ã€**æœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„çœŸå› **ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š

**A. ç­ä½“ãƒ¬ãƒ™ãƒ«ã®å•é¡Œï¼ˆå…¨ãƒãƒ¼ãƒˆã«å½±éŸ¿ã™ã‚‹å ´åˆï¼‰**
   - é›»æºãƒ¦ãƒ‹ãƒƒãƒˆï¼ˆPSUï¼‰ã®æ•…éšœã¾ãŸã¯é›»åœ§ä¸å®‰å®š
   - ãƒã‚¶ãƒ¼ãƒœãƒ¼ãƒ‰/åˆ¶å¾¡åŸºæ¿ã®å•é¡Œ
   - ç­ä½“å†…ã®éç†±ï¼ˆå†·å´ãƒ•ã‚¡ãƒ³æ•…éšœï¼‰

**B. ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®å•é¡Œ**
   - ãƒ•ã‚¡ãƒ¼ãƒ ã‚¦ã‚§ã‚¢/IOS/NOS ã®ãƒã‚°
   - è¨­å®šãƒŸã‚¹ã«ã‚ˆã‚‹å…¨ãƒãƒ¼ãƒˆå½±éŸ¿

**C. ç’°å¢ƒãƒ¬ãƒ™ãƒ«ã®å•é¡Œ**
   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ç©ºèª¿ã®å•é¡Œ
   - é›»æºä¾›çµ¦ã®å•é¡Œï¼ˆUPSã€é…é›»ç›¤ï¼‰

**åˆ¤æ–­åŸºæº–:**
- å…¨{affected_count}å€‹ãŒåŒæ™‚ã«å½±éŸ¿ â†’ é›»æºã¾ãŸã¯ãƒ•ã‚¡ãƒ¼ãƒ ã‚¦ã‚§ã‚¢ã®å¯èƒ½æ€§ãŒé«˜ã„
- ç‰¹å®šç¯„å›²ã®ãƒãƒ¼ãƒˆã®ã¿ â†’ ãƒ©ã‚¤ãƒ³ã‚«ãƒ¼ãƒ‰ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®å•é¡Œ
- å…‰ä¿¡å·ãƒ¬ãƒ™ãƒ«ä½ä¸‹ â†’ é›»æºã€æ¸©åº¦ã€ã¾ãŸã¯å€‹åˆ¥SFPæ•…éšœ

ã€ğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—3: æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆã€‘
ä¸Šè¨˜ã®åˆ†æçµæœã«åŸºã¥ã„ã¦ã€å„ªå…ˆé †ä½é †ã«**4-5å€‹**ã®å…·ä½“çš„ãªæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

**å„ªå…ˆåº¦ã®æ±ºå®š:**
- **highï¼ˆæœ€å„ªå…ˆï¼‰**: ã‚¢ãƒ©ãƒ¼ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹çœŸå› ã«å¯¾ã™ã‚‹å¯¾å¿œ
  - ä¾‹: å…¨ãƒãƒ¼ãƒˆå½±éŸ¿ â†’ é›»æºèª¿æŸ»ã‚’high
  - ä¾‹: å…‰ä¿¡å·ãƒ¬ãƒ™ãƒ«ä½ä¸‹ â†’ æ¸©åº¦ãƒ»é›»æºèª¿æŸ»ã‚’high
- **mediumï¼ˆæ¨å¥¨ï¼‰**: è£œåŠ©çš„ãªç¢ºèª
- **lowï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰**: å€‹åˆ¥éƒ¨å“äº¤æ›ï¼ˆ{affected_count}å€‹å…¨äº¤æ›ã¯éç¾å®Ÿçš„ï¼‰

ã€å‡ºåŠ›å½¢å¼ - JSONé…åˆ—ã®ã¿ã€‘
**ä»¥ä¸‹ã®å½¢å¼ã§4-5å€‹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆJSONä»¥å¤–ã®æ–‡å­—ã¯ä¸€åˆ‡å«ã‚ãªã„ï¼‰:**

[
  {{
    "title": "å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³å",
    "effect": "æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœï¼ˆ{affected_count}å€‹ã¸ã®å½±éŸ¿ã‚’æ˜è¨˜ï¼‰",
    "priority": "high/medium/low",
    "rationale": "ã‚¢ãƒ©ãƒ¼ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ¨æ¸¬ã—ãŸæ ¹æ‹ ï¼ˆå…·ä½“çš„ã«ï¼‰",
    "steps": "1. å®Ÿè¡Œæ‰‹é †\\n2. CLIã‚³ãƒãƒ³ãƒ‰ä¾‹\\n3. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—"
  }}
]

**ğŸš¨ å‡ºåŠ›ãƒ«ãƒ¼ãƒ«:**
1. JSONé…åˆ—ã®ã¿å‡ºåŠ›ï¼ˆèª¬æ˜æ–‡ãƒ»ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆä¸è¦ï¼‰
2. å¿…ãš4-5å€‹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
3. highå„ªå…ˆåº¦ã‚’2å€‹ä»¥ä¸Š
4. rationaleã«ã¯ã€Œã‚¢ãƒ©ãƒ¼ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰â—‹â—‹ãŒç¢ºèªã§ãã‚‹ãŸã‚ã€ãªã©å…·ä½“çš„æ ¹æ‹ 
5. å€‹åˆ¥éƒ¨å“äº¤æ›ã¯lowå„ªå…ˆåº¦
6. stepsã«ã¯\\nã§æ”¹è¡Œ"""

            # Gemini API å‘¼ã³å‡ºã—
            logger.info(f"Calling Gemini API for {affected_count} affected components")
            response = model.generate_content(prompt)
            response_text = response.text.strip()
            
            # JSON ãƒ‘ãƒ¼ã‚¹
            # Markdown ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            actions = json.loads(response_text)
            
            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if not isinstance(actions, list):
                logger.warning("Gemini API returned invalid format (not a list)")
                return None
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
            validated_actions = []
            for action in actions:
                if all(k in action for k in ["title", "effect", "priority", "rationale"]):
                    validated_actions.append(action)
            
            if validated_actions:
                logger.info(f"Generated {len(validated_actions)} actions using Gemini API")
                return validated_actions
            else:
                logger.warning("No valid actions in Gemini API response")
                return None
        
        except Exception as e:
            logger.warning(f"Failed to generate actions with Gemini API: {e}")
            return None

    def _generate_smart_recommendations(
        self,
        rule_pattern: str,
        affected_count: int,
        confidence: float,
        messages: List[str],
        device_id: str,
        base_actions: List[Dict[str, str]],
        llm_cache: Optional[Dict[str, List[Dict[str, str]]]] = None
    ) -> List[Dict[str, str]]:
        """
        LLMã‚’ä½¿ã£ã¦çŠ¶æ³ã«å¿œã˜ãŸæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‹•çš„ç”Ÿæˆ
        
        åºƒç¯„å›²éšœå®³ã®å ´åˆã¯çœŸå› ï¼ˆé›»æº/ãƒ•ã‚¡ãƒ¼ãƒ ã‚¦ã‚§ã‚¢/ç’°å¢ƒï¼‰ã‚’æ¨è«–
        
        Args:
            llm_cache: ãƒãƒƒãƒç”Ÿæˆã•ã‚ŒãŸæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        """
        # åºƒç¯„å›²éšœå®³ã®é–¾å€¤
        WIDE_RANGE_THRESHOLD = 5
        
        # åºƒç¯„å›²éšœå®³ã§ãªã„å ´åˆã¯å›ºå®šã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿”ã™
        if affected_count < WIDE_RANGE_THRESHOLD:
            return base_actions
        
        # â˜… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ï¼ˆãƒãƒƒãƒç”Ÿæˆæ¸ˆã¿ï¼‰
        if llm_cache and rule_pattern in llm_cache:
            logger.debug(f"Using cached LLM recommendations for {rule_pattern}")
            return llm_cache[rule_pattern]
        
        # â˜… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã‘ã‚Œã°å€‹åˆ¥ã«Gemini APIå‘¼ã³å‡ºã—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        llm_actions = self._generate_actions_with_gemini(
            rule_pattern=rule_pattern,
            affected_count=affected_count,
            confidence=confidence,
            messages=messages,
            device_id=device_id
        )
        
        if llm_actions:
            # LLMç”ŸæˆãŒæˆåŠŸã—ãŸå ´åˆã¯ãã‚Œã‚’è¿”ã™
            return llm_actions
        
        # LLMç”ŸæˆãŒå¤±æ•—ã—ãŸå ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé™çš„ãªé«˜åº¦ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
        enhanced_actions = []
        
        if "optical" in rule_pattern:
            enhanced_actions = [
                {
                    "title": "âš ï¸ ç­ä½“é›»æºç³»çµ±ã®èª¿æŸ»ï¼ˆæœ€å„ªå…ˆï¼‰",
                    "effect": f"é›»æºãƒ¦ãƒ‹ãƒƒãƒˆæ•…éšœã«ã‚ˆã‚‹{affected_count}å€‹ã®å…‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒæ™‚åŠ£åŒ–ã‚’è§£æ¶ˆ",
                    "priority": "high",
                    "rationale": f"{affected_count}å€‹ã®å…‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒæ™‚åŠ£åŒ–ã¯å˜ç™ºæ•…éšœã§ã¯èª¬æ˜å›°é›£ã€‚é›»æºç³»çµ±ã®å•é¡Œã‚’ç–‘ã†ã€‚"
                },
                {
                    "title": "âš ï¸ IOS/ãƒ•ã‚¡ãƒ¼ãƒ ã‚¦ã‚§ã‚¢ã®ãƒã‚°èª¿æŸ»",
                    "effect": "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢èµ·å› ã®èª¤æ¤œçŸ¥/åˆ¶å¾¡ç•°å¸¸ã‚’è§£æ¶ˆ",
                    "priority": "high",
                    "rationale": "åºƒç¯„å›²éšœå®³ã¯ãƒ•ã‚¡ãƒ¼ãƒ ã‚¦ã‚§ã‚¢ãƒã‚°ã®å¯èƒ½æ€§ã‚ã‚Š"
                },
                {
                    "title": "åˆ¶å¾¡åŸºæ¿ã®æ¸©åº¦/ç’°å¢ƒèª¿æŸ»",
                    "effect": "ç­ä½“å†…éç†±ã«ã‚ˆã‚‹åŠ£åŒ–ã‚’è§£æ¶ˆ",
                    "priority": "medium",
                    "rationale": "ç’°å¢ƒè¦å› ã«ã‚ˆã‚‹å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å½±éŸ¿ã‚’ç¢ºèª"
                },
                {
                    "title": "SFPãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å€‹åˆ¥äº¤æ›ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰",
                    "effect": "å€‹åˆ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ•…éšœã®è§£æ¶ˆ",
                    "priority": "low",
                    "rationale": f"{affected_count}å€‹å…¨äº¤æ›ã¯éç¾å®Ÿçš„ã€ä¸Šè¨˜ã‚’å„ªå…ˆ"
                }
            ]
        
        elif "microburst" in rule_pattern:
            enhanced_actions = [
                {
                    "title": "âš ï¸ ASIC/ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®èª¿æŸ»",
                    "effect": f"{affected_count}å€‹ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã®ãƒãƒƒãƒ•ã‚¡å•é¡Œã‚’è§£æ¶ˆ",
                    "priority": "high",
                    "rationale": "åºƒç¯„å›²ã®queue dropsã¯ASIC/ãƒãƒƒãƒ—ã‚»ãƒƒãƒˆå•é¡Œã®å¯èƒ½æ€§"
                },
                {
                    "title": "IOS/ãƒ•ã‚¡ãƒ¼ãƒ ã‚¦ã‚§ã‚¢ã®ãƒã‚°ç¢ºèª",
                    "effect": "QoSå‡¦ç†ã®ç•°å¸¸ã‚’è§£æ¶ˆ",
                    "priority": "high",
                    "rationale": "è¤‡æ•°ãƒãƒ¼ãƒˆã§ã®åŒæ™‚ç™ºç”Ÿã¯ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒã‚°ã®å¯èƒ½æ€§"
                },
                {
                    "title": "ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ",
                    "effect": "ç•°å¸¸ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã®æ¤œå‡ºãƒ»å¯¾å‡¦",
                    "priority": "medium",
                    "rationale": "DDoSæ”»æ’ƒã‚„ç•°å¸¸ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã®å¯èƒ½æ€§ã‚’ç¢ºèª"
                },
                {
                    "title": "QoSãƒãƒªã‚·ãƒ¼ã®èª¿æ•´",
                    "effect": "ãƒãƒƒãƒ•ã‚¡å‰²ã‚Šå½“ã¦ã®æœ€é©åŒ–",
                    "priority": "low",
                    "rationale": "æ ¹æœ¬åŸå› è§£æ±ºå¾Œã®æœ€é©åŒ–"
                }
            ]
        
        elif "route_instability" in rule_pattern or "bgp" in rule_pattern:
            enhanced_actions = [
                {
                    "title": "âš ï¸ BGPè¨­å®šã®åŒ…æ‹¬çš„ãƒ¬ãƒ“ãƒ¥ãƒ¼",
                    "effect": f"{affected_count}å€‹ã®ãƒ”ã‚¢ã§ã®ä¸å®‰å®šã•ã‚’è§£æ¶ˆ",
                    "priority": "high",
                    "rationale": "è¤‡æ•°ãƒ”ã‚¢ã§ã®åŒæ™‚ç™ºç”Ÿã¯è¨­å®šãƒŸã‚¹ã®å¯èƒ½æ€§"
                },
                {
                    "title": "ä¸ŠæµISPã¨ã®é€£æº",
                    "effect": "ISPå´ã®å•é¡Œã‚’ç‰¹å®šãƒ»å¯¾å‡¦",
                    "priority": "high",
                    "rationale": "åºƒç¯„å›²ãƒ«ãƒ¼ãƒˆä¸å®‰å®šã¯ISPå´å•é¡Œã®å¯èƒ½æ€§"
                },
                {
                    "title": "IOS/ãƒ•ã‚¡ãƒ¼ãƒ ã‚¦ã‚§ã‚¢ã®ç¢ºèª",
                    "effect": "BGPå®Ÿè£…ã®ãƒã‚°ã‚’å›é¿",
                    "priority": "medium",
                    "rationale": "BGPå‡¦ç†ã®ç•°å¸¸ã«ã‚ˆã‚‹ä¸å®‰å®šã•ã‚’ç¢ºèª"
                },
                {
                    "title": "BGPãƒ•ãƒ©ãƒƒãƒ—ãƒ€ãƒ³ãƒ”ãƒ³ã‚°ã®èª¿æ•´",
                    "effect": "ä¸å®‰å®šãªçµŒè·¯ã®æŠ‘åˆ¶",
                    "priority": "low",
                    "rationale": "ç—‡çŠ¶ã®ç·©å’Œï¼ˆæ ¹æœ¬è§£æ±ºã§ã¯ãªã„ï¼‰"
                }
            ]
        
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: åŸºæœ¬ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ + åºƒç¯„å›²èª¿æŸ»ã‚’è¿½åŠ 
            enhanced_actions = base_actions + [
                {
                    "title": "âš ï¸ ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®å¥å…¨æ€§ç¢ºèª",
                    "effect": f"{affected_count}å€‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆéšœå®³ã®æ ¹æœ¬åŸå› ã‚’ç‰¹å®š",
                    "priority": "high",
                    "rationale": "åºƒç¯„å›²éšœå®³ã¯é›»æº/ãƒ•ã‚¡ãƒ¼ãƒ ã‚¦ã‚§ã‚¢/ç’°å¢ƒã®å•é¡Œã‚’ç–‘ã†"
                }
            ]
        
        return enhanced_actions if enhanced_actions else base_actions

    def predict(self, analysis_results: List[Dict], msg_map: Dict[str, List[str]], alarms: Optional[List] = None) -> List[Dict]:
        self.reload_all()
        predictions = []
        critical_ids = {r["id"] for r in analysis_results if r.get("status") in ["RED", "CRITICAL"] or r.get("severity") == "CRITICAL" or float(r.get("prob", 0)) >= 0.85}
        warning_ids = {r["id"] for r in analysis_results if 0.45 <= float(r.get("prob", 0)) <= 0.85}
        active_ids = set(msg_map.keys())
        candidates = (warning_ids.union(active_ids)) - critical_ids
        processed_devices = set()
        multi_signal_boost = 0.05
        
        # â˜… ãƒãƒƒãƒLLMæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ãƒ»æ€§èƒ½å‘ä¸Šï¼‰
        llm_recommendations_cache = self._batch_generate_llm_recommendations(
            candidates=candidates,
            msg_map=msg_map
        )

        for dev_id in candidates:
            if dev_id in processed_devices: continue
            messages = msg_map.get(dev_id, [])
            if not messages: continue
            matched_signals = []
            for msg in messages:
                rule, quality = self._match_rule(msg)
                if rule and quality >= 0.30 and rule.pattern != "generic_error":
                    matched_signals.append((rule, quality, msg))
            if not matched_signals:
                rule, quality = self._match_rule(messages[0])
                if not rule: continue
                matched_signals = [(rule, quality, messages[0])]

            matched_signals.sort(key=lambda x: x[1], reverse=True)
            primary_rule, primary_quality, primary_msg = matched_signals[0]
            confidence = self._calculate_confidence(primary_rule, dev_id, primary_quality)
            extra_signals = len(matched_signals) - 1
            if extra_signals > 0:
                boost = min(extra_signals * multi_signal_boost, 0.20)
                confidence = min(0.99, confidence + boost)
            
            # â˜… ãƒ™ã‚¤ã‚ºæ¨è«–ã«ã‚ˆã‚‹ä¿¡é ¼åº¦ã®æ›´æ–°
            confidence, bayesian_debug = self.bayesian.calculate_posterior_confidence(
                device_id=dev_id,
                rule_pattern=primary_rule.pattern,
                current_confidence=confidence,
                time_window_hours=168  # éå»7æ—¥é–“
            )
            
            # â˜… GNNäºˆæ¸¬ã«ã‚ˆã‚‹ä¿¡é ¼åº¦ã®è£œæ­£ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if self.gnn and self._model:
                try:
                    # ç¾åœ¨ã®ã‚¢ãƒ©ãƒ¼ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’BERTåŸ‹ã‚è¾¼ã¿ã«å¤‰æ›
                    alarm_embeddings = {}
                    for msg_dev_id, msg_list in msg_map.items():
                        if msg_list:
                            # è¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å¹³å‡åŸ‹ã‚è¾¼ã¿
                            embeddings = self._model.encode(msg_list, convert_to_numpy=True)
                            alarm_embeddings[msg_dev_id] = embeddings.mean(axis=0)
                    
                    # GNNã§äºˆæ¸¬
                    gnn_confidence, gnn_ttf = self.gnn.predict_with_gnn(
                        alarm_embeddings, dev_id
                    )
                    
                    # ãƒ™ã‚¤ã‚ºæ¨è«–ã¨GNNäºˆæ¸¬ã®åŠ é‡å¹³å‡ï¼ˆGNNã®é‡ã¿ã¯æ§ãˆã‚ï¼‰
                    confidence = 0.7 * confidence + 0.3 * gnn_confidence
                    confidence = min(0.99, max(0.1, confidence))
                    
                except Exception as e:
                    logger.warning(f"GNN prediction failed: {e}")

            threshold = MIN_PREDICTION_CONFIDENCE
            if primary_rule.paging_threshold is not None:
                threshold = primary_rule.paging_threshold
            if confidence < threshold: continue

            impact_count = 0
            if dev_id in self.children_map:
                impact_count = len(self.children_map[dev_id])
            
            # --- äºˆæ¸¬çµæœã«ã€Œé‹ç”¨è€…å‘ã‘ã®å…·ä½“çš„ãªçŸ¥è­˜ã€ã‚’æ³¨å…¥ ---
            
            # â˜… LLMãƒ™ãƒ¼ã‚¹ã®å‹•çš„æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆåºƒç¯„å›²éšœå®³ã«å¯¾å¿œï¼‰
            # affected_count: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã•ã‚Œã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°ã‚’è¨ˆç®—
            unique_components = set()
            for _, _, msg in matched_signals:
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåã‚’æŠ½å‡ºï¼ˆä¾‹: Gi0/0/1, Te1/0/1ï¼‰
                import re
                components = re.findall(r'\b(?:Gi|Te|Fa|Et)\d+/\d+/\d+|\b(?:Gi|Te|Fa|Et)\d+/\d+', msg)
                unique_components.update(components)
            
            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°ãŒã‚«ã‚¦ãƒ³ãƒˆã§ããªã„å ´åˆã¯ã‚·ã‚°ãƒŠãƒ«æ•°ã‚’ä½¿ç”¨
            component_count = len(unique_components) if unique_components else len(matched_signals)
            
            smart_actions = self._generate_smart_recommendations(
                rule_pattern=primary_rule.pattern,
                affected_count=component_count,  # â˜… ä¿®æ­£: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°ã‚’ä½¿ç”¨
                confidence=confidence,
                messages=[s[2] for s in matched_signals],  # â˜… å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ï¼ˆ[:3]ã‚’å‰Šé™¤ï¼‰
                device_id=dev_id,
                base_actions=primary_rule.recommended_actions,
                llm_cache=llm_recommendations_cache  # â˜… ãƒãƒƒãƒç”Ÿæˆã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨
            )
            
            pred = {
                "id": dev_id,
                "label": f"ğŸ”® [äºˆå…†] {primary_rule.escalated_state}",
                "severity": "CRITICAL",
                "status": "CRITICAL",
                "prob": round(confidence, 2),
                "type": f"Predictive/{primary_rule.category}",
                "tier": 1,
                "reason": f"Digital Twin Prediction: {primary_rule.time_to_critical_min}min to critical. Root: {primary_msg}",
                "is_prediction": True,
                "prediction_timeline": f"{primary_rule.time_to_critical_min}åˆ†å¾Œ",
                "prediction_time_to_critical_min": primary_rule.time_to_critical_min,
                "prediction_early_warning_hours": primary_rule.early_warning_hours,
                "prediction_affected_count": impact_count,
                "prediction_signal_count": len(matched_signals),
                "prediction_confidence_factors": {"base": primary_rule.base_confidence, "match_quality": primary_quality},
                "recommended_actions": smart_actions,  # LLMãƒ™ãƒ¼ã‚¹ã®å‹•çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
                "base_recommended_actions": primary_rule.recommended_actions,  # å…ƒã®å›ºå®šã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå‚è€ƒç”¨ï¼‰
                "runbook_url": primary_rule.runbook_url
            }
            pid = str(uuid.uuid4())
            self.history.append({"prediction_id": pid, "device_id": dev_id, "rule_pattern": primary_rule.pattern, "timestamp": time.time(), "prob": confidence, "anchor_event_time": time.time(), "raw_msg": primary_msg})
            self.storage.save_json_atomic("history", self.history)
            predictions.append(pred)
            processed_devices.add(dev_id)
        return predictions

    def generate_tuning_report(self, days: int = 30) -> Dict[str, Any]:
        return self.tuner.generate_report(days)

    def apply_tuning_proposals_if_auto(self, proposals: List[Dict]) -> Dict:
        applied = []
        skipped = []
        with self.storage.global_lock(timeout_sec=30.0):
            for p in proposals:
                rp = p.get("rule_pattern")
                rec = p.get("apply_recommendation", {})
                if rec.get("apply_mode") != "auto":
                    skipped.append({"rule": rp, "reason": "not_auto"})
                    continue
                prop = p.get("proposal", {})
                pt = float(prop.get("paging_threshold", 0.0))
                lt = float(prop.get("logging_threshold", 0.0))
                old_json_str = self.storage.rule_config_get_json_str(rp)
                rj_str = old_json_str
                if rj_str:
                    d = json.loads(rj_str)
                    d["paging_threshold"] = pt
                    d["logging_threshold"] = lt
                    rj_str = json.dumps(d, ensure_ascii=False)
                success = self.storage.rule_config_upsert(rp, pt, lt, rj_str)
                if success:
                    applied.append({"rule": rp, "paging": pt})
                else:
                    skipped.append({"rule": rp, "reason": "db_write_fail"})
        return {"applied": applied, "skipped": skipped}

    def repair_db_from_rules_json(self) -> bool:
        try:
            path = self.storage.paths["rules"]
            if not os.path.exists(path): return False
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            sanitized = [self._sanitize_rule_data(item) for item in data]
            self.storage._seed_rule_config_from_rules_json(sanitized)
            return True
        except Exception: return False
    # ==============================================================
    # Phase1: Predict API helpers
    # ==============================================================

    def _parse_timestamp(self, ts) -> float:
        if ts is None:
            return time.time()
        if isinstance(ts, (int, float)):
            return float(ts)
        s = str(ts).strip()
        try:
            return float(s)
        except Exception:
            pass
        try:
            from datetime import datetime as _dt
            return _dt.fromisoformat(s.replace("Z", "+00:00")).timestamp()
        except Exception:
            return time.time()

    def _should_ignore(self, msg: str) -> bool:
        m = (msg or "").lower()
        ignore = ["dry-run", "test message", "synthetic-monitor", "healthcheck"]
        return any(ph in m for ph in ignore)

    def _rule_match_simple(self, rule, msg: str):
        """regex + semantic phrase ãƒãƒƒãƒã€‚(hit, reasons) ã‚’è¿”ã™"""
        reasons = []
        hit = False
        try:
            if rule._compiled_regex and rule._compiled_regex.search(msg or ""):
                hit = True
                reasons.append(f"pattern matched: {rule.pattern}")
        except Exception:
            pass
        if not hit:
            low = (msg or "").lower()
            for sp in (rule.semantic_phrases or []):
                if sp and sp.lower() in low:
                    hit = True
                    reasons.append(f"semantic hit: {sp}")
                    break
        return hit, reasons

    def predict(self, device_id: str, msg: str, timestamp: float,
                attrs: Optional[Dict[str, Any]] = None,
                degradation_level: int = 1,
                source: str = "real") -> List[Dict[str, Any]]:
        """
        EscalationRule ãƒ™ãƒ¼ã‚¹ã®äºˆå…†äºˆæ¸¬ã€‚
        degradation_level (1-5): Level ã«å¿œã˜ã¦ confidence ã‚’ãƒ–ãƒ¼ã‚¹ãƒˆã€
                                  time_to_critical ã‚’çŸ­ç¸®ã€early_warning ã‚’å»¶é•·ã€‚
        source: "simulation" | "real"
        æˆ»ã‚Šå€¤ã¯ PredictResult.to_dict() ã®ãƒªã‚¹ãƒˆï¼ˆconfidence é™é †ï¼‰ã€‚
        """
        try:
            msg_n = self._normalize_msg(msg or "")
        except AttributeError:
            msg_n = (msg or "").strip()
        except Exception:
            msg_n = (msg or "").strip()

        if self._should_ignore(msg_n):
            return []

        _min_conf = float(MIN_PREDICTION_CONFIDENCE)

        # Level ãƒ–ãƒ¼ã‚¹ãƒˆä¿‚æ•°ï¼ˆLevel1=0.0 â†’ Level5=0.20ï¼‰
        _level = max(1, min(5, int(degradation_level or 1)))
        _conf_boost    = (_level - 1) * 0.05          # +0.00ã€œ+0.20
        _ttc_factor    = 1.0 - (_level - 1) * 0.12   # Ã—1.0ã€œÃ—0.52ï¼ˆçŸ­ç¸®ï¼‰
        _early_factor  = 1.0 + (_level - 1) * 0.20   # Ã—1.0ã€œÃ—1.80ï¼ˆå»¶é•·ï¼‰
        
        # â˜… RUL (Remaining Useful Life) äºˆæ¸¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Temporal GNNè«–æ–‡: time_to_failure = f(degradation_level)
        # Levelâ†‘ â†’ æ•…éšœãŒè¿‘ã„ â†’ RULâ†“
        _ttf_scale = (6 - _level) / 5  # L1=1.0(åˆæœŸ), L5=0.2(æœ«æœŸ)
        # L1=100%, L2=80%, L3=60%, L4=40%, L5=20%

        # å½±éŸ¿ç¯„å›²: children_map ã‹ã‚‰å†å¸°çš„ã«é…ä¸‹ãƒ‡ãƒã‚¤ã‚¹æ•°ã‚’ç®—å‡º
        def _count_children(dev_id: str, visited=None) -> int:
            if visited is None: visited = set()
            if dev_id in visited: return 0
            visited.add(dev_id)
            children = (self.children_map or {}).get(dev_id, [])
            return len(children) + sum(_count_children(c, visited) for c in children)

        _affected_count = _count_children(device_id)

        results = []
        for rule in (self.rules or []):
            try:
                hit, reasons = self._rule_match_simple(rule, msg_n)
                if not hit:
                    continue
                base_conf = float(getattr(rule, "base_confidence", 0.5) or 0.5)
                conf = min(0.99, base_conf + _conf_boost)
                if conf < _min_conf:
                    continue
                _base_ttc   = int(getattr(rule, "time_to_critical_min", 60) or 60)
                _base_early = int(getattr(rule, "early_warning_hours", 24) or 24)
                _ttc   = max(5,  int(_base_ttc   * _ttc_factor))
                _early = max(1,  int(_base_early * _early_factor))
                
                # â˜… RULè¨ˆç®—: early_warning_hours ã‚’ãƒ™ãƒ¼ã‚¹ã«æ•…éšœã¾ã§ã®æ™‚é–“ã‚’ç®—å‡º
                _base_ttf_hours = int(getattr(rule, "early_warning_hours", 336) or 336)
                _ttf_hours = max(1, int(_base_ttf_hours * _ttf_scale))
                # L1=336h(14æ—¥), L2=269h(11æ—¥), L3=202h(8æ—¥), L4=134h(6æ—¥), L5=67h(3æ—¥)
                
                # æ•…éšœäºˆæ¸¬æ—¥æ™‚ã‚’ç®—å‡º
                from datetime import datetime, timedelta
                _failure_dt = datetime.now() + timedelta(hours=_ttf_hours)
                _failure_dt_str = _failure_dt.strftime("%Y-%m-%d %H:%M")
                
                pr = PredictResult(
                    predicted_state      = str(getattr(rule, "escalated_state", "unknown")),
                    confidence           = conf,
                    rule_pattern         = str(getattr(rule, "pattern", "unknown")),
                    category             = str(getattr(rule, "category", "Generic")),
                    reasons              = reasons,
                    recommended_actions  = list(getattr(rule, "recommended_actions", []) or []),
                    runbook_url          = str(getattr(rule, "runbook_url", "") or ""),
                    criticality          = str(getattr(rule, "criticality", "standard") or "standard"),
                    time_to_critical_min = _ttc,
                    early_warning_hours  = _early,
                    time_to_failure_hours = _ttf_hours,
                    predicted_failure_datetime = _failure_dt_str,
                )
                results.append(pr)
            except Exception:
                continue
        results.sort(key=lambda x: x.confidence, reverse=True)
        return [r.to_dict(affected_count=_affected_count, source=source) for r in results]

    def predict_api(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        Cockpit / Simulator å…±é€šã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã€‚
        record_forecast=True (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ) ã®ã¨ã forecast_ledger ã«ç™»éŒ²ã™ã‚‹ã€‚
        """
        try:
            tenant_id = (request.get("tenant_id") or self.tenant_id or "default").strip().lower()
            device_id = str(request.get("device_id") or "").strip()
            msg       = str(request.get("msg") or "").strip()
            ts        = self._parse_timestamp(request.get("timestamp"))
            if not device_id:
                raise ValueError("device_id is required")
            if not msg:
                raise ValueError("msg is required")
            attrs = request.get("attrs") or {}
            if not isinstance(attrs, dict):
                attrs = {"raw_attrs": str(attrs)}

            req   = PredictRequest(tenant_id=tenant_id, device_id=device_id,
                                   msg=msg, timestamp=ts, attrs=attrs)
            _level  = int((attrs or {}).get("degradation_level", 1))
            _source = str((attrs or {}).get("source", "real"))
            preds = self.predict(device_id=device_id, msg=msg, timestamp=ts,
                                 attrs=attrs, degradation_level=_level, source=_source)

            record_forecast = bool(request.get("record_forecast", True))
            forecast_ids: List[str] = []
            if record_forecast and preds:
                fid = self._forecast_record(req=req.to_dict(), top_prediction=preds[0])
                if fid:
                    forecast_ids.append(fid)

            return {"ok": True, "input": req.to_dict(),
                    "predictions": preds, "forecast_ids": forecast_ids}
        except Exception as e:
            return {"ok": False, "error": f"{type(e).__name__}: {e}",
                    "trace": traceback.format_exc()}

    # ==============================================================
    # Phase1: Forecast Ledger DDLï¼ˆ_init_sqlite ã‹ã‚‰å‘¼ã°ã‚Œã‚‹ï¼‰
    # ==============================================================

    def _init_forecast_ledger(self):
        """forecast_ledger ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ migration ã‚’å®Ÿæ–½"""
        if not self.storage._conn:
            return
        try:
            with self.storage._db_lock:
                self.storage._conn.execute("""
                    CREATE TABLE IF NOT EXISTS forecast_ledger (
                        forecast_id      TEXT PRIMARY KEY,
                        created_at       REAL,
                        tenant_id        TEXT,
                        device_id        TEXT,
                        rule_pattern     TEXT,
                        predicted_state  TEXT,
                        confidence       REAL,
                        horizon_sec      INTEGER,
                        eval_deadline_ts REAL,
                        source           TEXT,
                        status           TEXT,
                        outcome_type     TEXT,
                        outcome_ts       REAL,
                        outcome_note     TEXT,
                        input_json       TEXT,
                        prediction_json  TEXT
                    )
                """)
                self.storage._conn.execute(
                    "CREATE INDEX IF NOT EXISTS idx_fl_open "
                    "ON forecast_ledger (status, eval_deadline_ts)")
                self.storage._conn.execute(
                    "CREATE INDEX IF NOT EXISTS idx_fl_device "
                    "ON forecast_ledger (device_id, created_at)")
                # migration: add source column if missing
                cur = self.storage._conn.cursor()
                cur.execute("PRAGMA table_info(forecast_ledger)")
                cols = [r[1] for r in cur.fetchall()]
                if "source" not in cols:
                    self.storage._conn.execute(
                        "ALTER TABLE forecast_ledger ADD COLUMN source TEXT")
                self.storage._conn.commit()
        except Exception as e:
            logger.warning(f"_init_forecast_ledger: {e}")

    def _forecast_horizon_sec(self, rule_pattern: str) -> int:
        for r in (self.rules or []):
            if (getattr(r, "pattern", "") or "").lower() == (rule_pattern or "").lower():
                ttc = getattr(r, "time_to_critical_min", None)
                if isinstance(ttc, int) and ttc > 0:
                    return max(1800, ttc * 60)
        return 3600

    def _forecast_record(self, req: Dict[str, Any], top_prediction: Dict[str, Any],
                         source: str = "real") -> Optional[str]:
        """forecast_ledger ã«1è¡Œ INSERTï¼ˆåŸå­çš„ï¼‰ã€‚forecast_id ã‚’è¿”ã™ã€‚"""
        if not self.storage._conn:
            return None
        try:
            forecast_id     = "f_" + uuid.uuid4().hex[:12]
            created_at      = time.time()
            tenant_id       = str(req.get("tenant_id") or self.tenant_id)
            device_id       = str(req.get("device_id") or "")
            rule_pattern    = str(top_prediction.get("rule_pattern") or "unknown")
            predicted_state = str(top_prediction.get("predicted_state") or "unknown")
            confidence      = float(top_prediction.get("confidence") or 0.0)
            horizon_sec     = self._forecast_horizon_sec(rule_pattern)
            event_ts        = float(req.get("timestamp") or created_at)
            eval_deadline_ts = event_ts + horizon_sec
            input_json      = json.dumps(req, ensure_ascii=False)
            prediction_json = json.dumps(top_prediction, ensure_ascii=False)

            with self.storage._db_lock:
                self.storage._conn.execute("""
                    INSERT INTO forecast_ledger
                    (forecast_id, created_at, tenant_id, device_id, rule_pattern, predicted_state,
                     confidence, horizon_sec, eval_deadline_ts, source, status,
                     outcome_type, outcome_ts, outcome_note, input_json, prediction_json)
                    VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)
                """, (forecast_id, created_at, tenant_id, device_id, rule_pattern, predicted_state,
                      confidence, horizon_sec, eval_deadline_ts, source, "open",
                      None, None, None, input_json, prediction_json))
                self.storage._conn.commit()
            return forecast_id
        except Exception as e:
            logger.warning(f"_forecast_record: {e}")
            return None

    def forecast_get(self, forecast_id: str) -> Optional[Dict[str, Any]]:
        if not self.storage._conn:
            return None
        try:
            with self.storage._db_lock:
                cur = self.storage._conn.cursor()
                cur.execute("""
                    SELECT forecast_id, created_at, tenant_id, device_id, rule_pattern,
                           predicted_state, confidence, horizon_sec, eval_deadline_ts,
                           source, status, outcome_type, outcome_ts, outcome_note
                    FROM forecast_ledger WHERE forecast_id=?""", (forecast_id,))
                row = cur.fetchone()
            if not row:
                return None
            keys = ["forecast_id","created_at","tenant_id","device_id","rule_pattern",
                    "predicted_state","confidence","horizon_sec","eval_deadline_ts",
                    "source","status","outcome_type","outcome_ts","outcome_note"]
            return dict(zip(keys, row))
        except Exception:
            return None

    def forecast_register_outcome(self, forecast_id: str, outcome_type: str,
                                  outcome_ts=None, note: str = "",
                                  auto: bool = False) -> Dict[str, Any]:
        """
        äºˆè¦‹æˆåŠŸåˆ¤å®š:
          deadline ä»¥å†…ã« OUTCOME_CONFIRMED â†’ status=confirmed, success=True
          deadline è¶…éå¾Œ   â†’ status=confirmed_late, success=False
          è‡ªå‹•ç™»éŒ² (auto=True) ã¯ audit_log ã« actor="auto" ã§è¨˜éŒ²
        """
        if not self.storage._conn:
            return {"ok": False, "reason": "sqlite_disabled"}
        fid = str(forecast_id or "").strip()
        if not fid:
            return {"ok": False, "reason": "missing_forecast_id"}

        ts  = time.time() if outcome_ts is None else self._parse_timestamp(outcome_ts)
        rec = self.forecast_get(fid)
        if not rec:
            return {"ok": False, "reason": "not_found"}
        if rec.get("status") not in ["open"]:
            return {"ok": False, "reason": "not_open", "status": rec.get("status")}

        deadline = float(rec.get("eval_deadline_ts") or 0.0)
        success  = bool(ts <= deadline) if deadline > 0 else False

        if outcome_type == "confirmed_incident":
            new_status = "confirmed" if success else "confirmed_late"
        elif outcome_type == "mitigated":
            new_status = "mitigated"
            success = True
        elif outcome_type == "false_alarm":
            new_status = "false_alarm"
            success = False
        else:
            new_status = "closed"
            success = False

        actor     = "auto" if auto else "operator"
        note_s    = (note or "")[:512]
        rule_pat  = str(rec.get("rule_pattern") or "")

        try:
            with self.storage._db_lock:
                self.storage._conn.execute("""
                    UPDATE forecast_ledger
                    SET status=?, outcome_type=?, outcome_ts=?, outcome_note=?
                    WHERE forecast_id=?""",
                    (new_status, outcome_type, ts, note_s, fid))
                # audit_log ã«è¨˜éŒ²
                self.storage.audit_log_generic({
                    "event_id":    str(uuid.uuid4()),
                    "timestamp":   ts,
                    "event_type":  "forecast_outcome",
                    "actor":       actor,
                    "rule_pattern": rule_pat,
                    "details": {"forecast_id": fid, "outcome_type": outcome_type,
                                "success": success, "status": new_status, "auto": auto}
                })
                self.storage._conn.commit()
        except Exception as e:
            return {"ok": False, "reason": str(e)}

        return {"ok": True, "forecast_id": fid, "success": success, "status": new_status}

    def forecast_expire_open(self, now_ts: Optional[float] = None,
                             limit: int = 200) -> Dict[str, Any]:
        """æœŸé™åˆ‡ã‚Œã® open äºˆå…†ã‚’ expired ã«æ›´æ–°"""
        if not self.storage._conn:
            return {"ok": False}
        now = float(now_ts or time.time())
        expired = 0
        try:
            with self.storage._db_lock:
                cur = self.storage._conn.cursor()
                cur.execute("""
                    SELECT forecast_id, rule_pattern FROM forecast_ledger
                    WHERE status='open' AND eval_deadline_ts < ?
                    ORDER BY eval_deadline_ts ASC LIMIT ?""", (now, limit))
                rows = cur.fetchall() or []
                for fid, rp in rows:
                    self.storage._conn.execute(
                        "UPDATE forecast_ledger SET status='expired', outcome_type='false_alarm', outcome_ts=? "
                        "WHERE forecast_id=?", (now, fid))
                    expired += 1
                if expired:
                    self.storage._conn.commit()
        except Exception as e:
            logger.warning(f"forecast_expire_open: {e}")
        return {"ok": True, "expired": expired}

    def forecast_auto_resolve(self, device_id: str, outcome_type: str,
                              note: str = "") -> int:
        """
        device_id ã® open äºˆå…†ã‚’è‡ªå‹• outcome ç™»éŒ²ã€‚
        cockpit ã® Execute æˆåŠŸæ™‚ãƒ»ã‚¢ãƒ©ãƒ¼ãƒ ç¢ºå®šæ™‚ã‹ã‚‰å‘¼ã°ã‚Œã‚‹ã€‚
        è§£æ±ºã—ãŸä»¶æ•°ã‚’è¿”ã™ã€‚
        """
        if not self.storage._conn:
            return 0
        resolved = 0
        try:
            with self.storage._db_lock:
                cur = self.storage._conn.cursor()
                cur.execute("""
                    SELECT forecast_id FROM forecast_ledger
                    WHERE device_id=? AND status='open'
                    ORDER BY created_at DESC""", (device_id,))
                rows = cur.fetchall() or []
            for (fid,) in rows:
                r = self.forecast_register_outcome(
                    fid, outcome_type, note=note, auto=True)
                if r.get("ok"):
                    resolved += 1
        except Exception as e:
            logger.warning(f"forecast_auto_resolve: {e}")
        return resolved

    def forecast_auto_confirm_on_incident(self, device_id: str, scenario: str = "",
                                          note: str = "") -> int:
        """
        éšœå®³ç™ºç”Ÿæ™‚ã«è©²å½“ãƒ‡ãƒã‚¤ã‚¹ã® open äºˆå…†ã‚’è‡ªå‹•çš„ã« confirmed_incident ã«æ›´æ–°
        
        é‹ç”¨å®Ÿæ…‹ã«å³ã—ãŸè¨­è¨ˆ:
        - é‹ç”¨è€…ãŒã€Œéšœå®³ç¢ºèªæ¸ˆã¿ã€ã‚’æ‰‹å‹•ç™»éŒ²ã™ã‚‹ã®ã¯éç¾å®Ÿçš„
        - éšœå®³ã‚·ãƒŠãƒªã‚ªç™ºç”Ÿæ™‚ã«è‡ªå‹•åˆ¤å®šã™ã‚‹æ–¹ãŒæ­£ç¢º
        
        Args:
            device_id: éšœå®³ãŒç™ºç”Ÿã—ãŸãƒ‡ãƒã‚¤ã‚¹ID
            scenario: ç™ºç”Ÿã—ãŸéšœå®³ã‚·ãƒŠãƒªã‚ªåï¼ˆãƒ­ã‚°ç”¨ï¼‰
            note: è¿½åŠ ãƒ¡ãƒ¢
        
        Returns:
            confirmed ã«æ›´æ–°ã—ãŸäºˆå…†ã®ä»¶æ•°
        """
        if not self.storage._conn:
            return 0
        confirmed = 0
        auto_note = f"Auto-confirmed on incident: {scenario}" if scenario else "Auto-confirmed on incident"
        if note:
            auto_note += f" | {note}"
        
        try:
            with self.storage._db_lock:
                cur = self.storage._conn.cursor()
                cur.execute("""
                    SELECT forecast_id FROM forecast_ledger
                    WHERE device_id=? AND status='open'
                    ORDER BY created_at DESC""", (device_id,))
                rows = cur.fetchall() or []
            
            for (fid,) in rows:
                r = self.forecast_register_outcome(
                    fid, "confirmed_incident", note=auto_note, auto=True)
                if r.get("ok"):
                    confirmed += 1
                    logger.info(f"Auto-confirmed forecast {fid[:12]} on incident: {scenario}")
        except Exception as e:
            logger.warning(f"forecast_auto_confirm_on_incident: {e}")
        
        return confirmed

    def forecast_list_open(self, device_id: Optional[str] = None,
                           limit: int = 50) -> List[Dict[str, Any]]:
        """open ä¸­ã®äºˆå…†ãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆUIè¡¨ç¤ºç”¨ï¼‰"""
        if not self.storage._conn:
            return []
        try:
            with self.storage._db_lock:
                cur = self.storage._conn.cursor()
                if device_id:
                    cur.execute("""
                        SELECT forecast_id, created_at, device_id, rule_pattern,
                               predicted_state, confidence, eval_deadline_ts, source, input_json
                        FROM forecast_ledger
                        WHERE status='open' AND device_id=?
                        ORDER BY created_at DESC LIMIT ?""", (device_id, limit))
                else:
                    cur.execute("""
                        SELECT forecast_id, created_at, device_id, rule_pattern,
                               predicted_state, confidence, eval_deadline_ts, source, input_json
                        FROM forecast_ledger
                        WHERE status='open'
                        ORDER BY confidence DESC, created_at DESC LIMIT ?""", (limit,))
                rows = cur.fetchall() or []
            keys = ["forecast_id","created_at","device_id","rule_pattern",
                    "predicted_state","confidence","eval_deadline_ts","source","input_json"]
            result = []
            for r in rows:
                d = dict(zip(keys, r))
                # input_jsonã‹ã‚‰ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ½å‡º
                try:
                    if d.get("input_json"):
                        input_data = json.loads(d["input_json"])
                        d["message"] = input_data.get("msg", "")
                except Exception:
                    d["message"] = ""
                result.append(d)
            return result
        except Exception:
            return []
