#!/bin/bash
# =============================================================
# E1: エアギャップ環境向け all-MiniLM-L6-v2 デプロイスクリプト
# =============================================================
# 
# 使い方:
#   【インターネット接続環境で実行】
#   1. ./deploy_airgap.sh download   ← モデル＋wheel をダウンロード
#   2. USB等で airgap_package/ を持ち込み
#
#   【エアギャップ環境で実行】
#   3. ./deploy_airgap.sh install    ← オフラインインストール
#   4. ./deploy_airgap.sh verify     ← 動作確認
#
# =============================================================

set -e

PACKAGE_DIR="./airgap_package"
MODEL_DIR="${PACKAGE_DIR}/models/all-MiniLM-L6-v2"
WHEEL_DIR="${PACKAGE_DIR}/wheels"
CHECKSUM_FILE="${PACKAGE_DIR}/checksums.sha256"

# --- Color output ---
RED='\033[0;31m'; GREEN='\033[0;32m'; YELLOW='\033[1;33m'; NC='\033[0m'
info()  { echo -e "${GREEN}[INFO]${NC} $1"; }
warn()  { echo -e "${YELLOW}[WARN]${NC} $1"; }
error() { echo -e "${RED}[ERROR]${NC} $1"; exit 1; }

# =============================================================
# Phase 1: ダウンロード（インターネット接続環境）
# =============================================================
cmd_download() {
    info "=== Phase 1: Download for Air-Gap Deployment ==="
    
    mkdir -p "${MODEL_DIR}" "${WHEEL_DIR}"
    
    # --- 1. Python Wheels (CPU版) ---
    info "1/3: Downloading Python wheels (CPU-only)..."
    
    # PyTorch CPU版のインデックスを明示指定
    pip download torch --only-binary=:all: \
        --platform manylinux2014_x86_64 \
        --python-version 3.11 \
        --extra-index-url https://download.pytorch.org/whl/cpu \
        -d "${WHEEL_DIR}" 2>&1 | tail -5
    
    # sentence-transformers と依存パッケージ
    pip download \
        "sentence-transformers>=2.2.2" \
        "networkx>=3.0" \
        "numpy>=1.24.0" \
        --only-binary=:all: \
        --platform manylinux2014_x86_64 \
        --python-version 3.11 \
        -d "${WHEEL_DIR}" 2>&1 | tail -5
    
    info "  Wheels downloaded to ${WHEEL_DIR}"
    info "  Total size: $(du -sh ${WHEEL_DIR} | cut -f1)"
    
    # --- 2. all-MiniLM-L6-v2 モデルファイル ---
    info "2/3: Downloading all-MiniLM-L6-v2 model (safetensors)..."
    
    python3 << 'PYEOF'
import os
MODEL_DIR = os.environ.get("MODEL_DIR", "./airgap_package/models/all-MiniLM-L6-v2")

# HuggingFaceからモデルをダウンロード
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="sentence-transformers/all-MiniLM-L6-v2",
    local_dir=MODEL_DIR,
    # safetensors 形式を優先（pickle攻撃対策）
    ignore_patterns=["*.bin", "*.h5", "*.ot", "flax_model*"],
)
print(f"Model saved to: {MODEL_DIR}")
PYEOF
    
    info "  Model size: $(du -sh ${MODEL_DIR} | cut -f1)"
    
    # --- 3. チェックサム生成 ---
    info "3/3: Generating checksums..."
    cd "${PACKAGE_DIR}"
    find . -type f \( -name "*.safetensors" -o -name "*.whl" \) \
        -exec sha256sum {} \; > checksums.sha256
    cd - > /dev/null
    
    info "  Checksums saved to ${CHECKSUM_FILE}"
    info "  $(wc -l < ${CHECKSUM_FILE}) files hashed"
    
    # --- サマリ ---
    echo ""
    info "=== Download Complete ==="
    info "Package directory: ${PACKAGE_DIR}"
    info "Total size: $(du -sh ${PACKAGE_DIR} | cut -f1)"
    echo ""
    info "次のステップ:"
    info "  1. ${PACKAGE_DIR} フォルダを USB 等でエアギャップ環境に転送"
    info "  2. エアギャップ環境で: ./deploy_airgap.sh install"
    echo ""
}

# =============================================================
# Phase 2: インストール（エアギャップ環境）
# =============================================================
cmd_install() {
    info "=== Phase 2: Air-Gap Installation ==="
    
    # --- 0. 前提チェック ---
    [ -d "${WHEEL_DIR}" ] || error "Wheel directory not found: ${WHEEL_DIR}"
    [ -d "${MODEL_DIR}" ] || error "Model directory not found: ${MODEL_DIR}"
    [ -f "${CHECKSUM_FILE}" ] || warn "Checksum file not found. Skipping verification."
    
    # --- 1. チェックサム検証 ---
    if [ -f "${CHECKSUM_FILE}" ]; then
        info "1/3: Verifying file integrity..."
        cd "${PACKAGE_DIR}"
        if sha256sum -c checksums.sha256 --quiet 2>/dev/null; then
            info "  ✅ All checksums verified"
        else
            error "Checksum verification FAILED! Files may have been tampered with."
        fi
        cd - > /dev/null
    fi
    
    # --- 2. pip install (オフライン) ---
    info "2/3: Installing Python packages (offline)..."
    pip install --no-index --find-links="${WHEEL_DIR}" \
        torch \
        sentence-transformers \
        networkx \
        numpy \
        2>&1 | tail -10
    
    info "  ✅ Packages installed"
    
    # --- 3. モデルパス設定 ---
    info "3/3: Configuring model path..."
    
    # 絶対パスを取得
    MODEL_ABS_PATH=$(cd "${MODEL_DIR}" && pwd)
    
    # 設定ファイルに書き出し
    cat > ./model_config.env << EOF
# Digital Twin Engine - Model Configuration
# Generated by deploy_airgap.sh on $(date -Iseconds)
DIGITAL_TWIN_MODEL_PATH=${MODEL_ABS_PATH}
EOF
    
    info "  Model path: ${MODEL_ABS_PATH}"
    info "  Config: ./model_config.env"
    
    echo ""
    info "=== Installation Complete ==="
    info "次のステップ: ./deploy_airgap.sh verify"
    echo ""
}

# =============================================================
# Phase 3: 動作確認
# =============================================================
cmd_verify() {
    info "=== Phase 3: Verification ==="
    
    # モデルパスの取得
    if [ -f "./model_config.env" ]; then
        source ./model_config.env
    fi
    
    # デフォルトパス
    MODEL_PATH="${DIGITAL_TWIN_MODEL_PATH:-${MODEL_DIR}}"
    
    python3 << PYEOF
import sys, time

print("=" * 50)
print("  Digital Twin Engine - Verification")
print("=" * 50)

# 1. Import check
print("\n[1/4] Import check...")
try:
    import torch
    print(f"  ✅ PyTorch {torch.__version__} (CPU: {not torch.cuda.is_available() or True})")
except ImportError as e:
    print(f"  ❌ PyTorch: {e}"); sys.exit(1)

try:
    import networkx as nx
    print(f"  ✅ NetworkX {nx.__version__}")
except ImportError as e:
    print(f"  ❌ NetworkX: {e}"); sys.exit(1)

try:
    from sentence_transformers import SentenceTransformer
    print(f"  ✅ sentence-transformers imported")
except ImportError as e:
    print(f"  ❌ sentence-transformers: {e}"); sys.exit(1)

# 2. Model load (from local path)
print("\n[2/4] Model load (${MODEL_PATH})...")
t0 = time.time()
try:
    model = SentenceTransformer("${MODEL_PATH}")
    load_time = time.time() - t0
    print(f"  ✅ Model loaded in {load_time:.2f}s")
except Exception as e:
    print(f"  ❌ Model load failed: {e}"); sys.exit(1)

# 3. Encoding test
print("\n[3/4] Encoding test...")
test_texts = [
    "Memory High",
    "kernel: mbuf cluster limit reached",         # Juniper
    "%SYS-3-CPUHOG: Task ran for 2120 msec",     # Cisco
    "Fan tray removed from slot 1",               # Generic
    "BGP neighbor 10.0.0.1 Down",                 # Routing
]

t0 = time.time()
vectors = model.encode(test_texts, convert_to_numpy=True)
enc_time = time.time() - t0

print(f"  ✅ Encoded {len(test_texts)} texts in {enc_time*1000:.1f}ms")
print(f"  ✅ Vector shape: {vectors.shape}")  # (5, 384)

# 4. Similarity test (zero-shot matching quality)
print("\n[4/4] Semantic similarity test (zero-shot quality)...")
import numpy as np

reference = model.encode(["memory usage high"], convert_to_numpy=True)

for i, text in enumerate(test_texts):
    vec = vectors[i:i+1]
    sim = np.dot(reference, vec.T).flatten()[0] / (
        np.linalg.norm(reference) * np.linalg.norm(vec)
    )
    marker = "✅" if sim > 0.4 else "⚠️"
    print(f"  {marker} '{text[:45]:45s}' → sim={sim:.3f}")

print("\n" + "=" * 50)
print("  ✅ All verification checks passed")
print("=" * 50)
PYEOF
}

# =============================================================
# Main
# =============================================================
case "${1:-help}" in
    download) cmd_download ;;
    install)  cmd_install  ;;
    verify)   cmd_verify   ;;
    *)
        echo "Usage: $0 {download|install|verify}"
        echo ""
        echo "  download  - インターネット環境でモデル＋wheelをダウンロード"
        echo "  install   - エアギャップ環境でオフラインインストール"
        echo "  verify    - 動作確認テスト"
        ;;
esac
