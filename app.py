import streamlit as st
import graphviz
import os
import google.generativeai as genai

from data import TOPOLOGY
from logic import CausalInferenceEngine, Alarm
# ä¿®æ­£ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from network_ops import run_diagnostic_simulation

st.set_page_config(page_title="Antigravity Live", page_icon="âš¡", layout="wide")

# --- ãƒˆãƒãƒ­ã‚¸ãƒ¼æç”» (å¤‰æ›´ãªã—) ---
def render_topology(alarms, root_cause_node):
    graph = graphviz.Digraph()
    graph.attr(rankdir='TB')
    graph.attr('node', shape='box', style='rounded,filled', fontname='Helvetica')
    alarmed_ids = {a.device_id for a in alarms}
    for node_id, node in TOPOLOGY.items():
        color = "#e8f5e9"
        penwidth = "1"
        if root_cause_node and node_id == root_cause_node.id:
            color = "#ffcdd2"
            penwidth = "3"
        elif node_id in alarmed_ids:
            color = "#fff9c4"
        graph.node(node_id, label=f"{node_id}\n({node.type})", fillcolor=color, color='black', penwidth=penwidth)
    for node_id, node in TOPOLOGY.items():
        if node.parent_id:
            graph.edge(node.parent_id, node_id)
            parent = TOPOLOGY.get(node.parent_id)
            if parent and parent.redundancy_group:
                partners = [n.id for n in TOPOLOGY.values() if n.redundancy_group == parent.redundancy_group and n.id != parent.id]
                for p in partners: graph.edge(p, node_id)
    return graph

# --- Configèª­ã¿è¾¼ã¿ (å¤‰æ›´ãªã—) ---
def load_config_by_id(device_id):
    path = f"configs/{device_id}.txt"
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f: return f.read()
    return None

# --- UIæ§‹ç¯‰ ---
st.title("âš¡ Antigravity AI Agent (Live Demo)")

api_key = None
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = os.environ.get("GOOGLE_API_KEY")

with st.sidebar:
    st.header("âš¡ é‹ç”¨ãƒ¢ãƒ¼ãƒ‰é¸æŠ")
    # é¸æŠè‚¢
    selected_scenario = st.radio(
        "ã‚·ãƒŠãƒªã‚ª:", 
        ("æ­£å¸¸ç¨¼åƒ", "1. WANå…¨å›ç·šæ–­", "2. FWç‰‡ç³»éšœå®³", "3. L2SWã‚µã‚¤ãƒ¬ãƒ³ãƒˆéšœå®³", "4. [Live] Ciscoå®Ÿæ©Ÿè¨ºæ–­")
    )
    if not api_key:
        st.warning("API Key Missing")
        user_key = st.text_input("Google API Key", type="password")
        if user_key: api_key = user_key

if "current_scenario" not in st.session_state:
    st.session_state.current_scenario = "æ­£å¸¸ç¨¼åƒ"
    st.session_state.messages = []
    st.session_state.chat_session = None 
    st.session_state.live_result = None

if st.session_state.current_scenario != selected_scenario:
    st.session_state.current_scenario = selected_scenario
    st.session_state.messages = []
    st.session_state.chat_session = None
    st.session_state.live_result = None
    st.rerun()

# --- Liveãƒ¢ãƒ¼ãƒ‰åˆ¤å®š ---
# 4ç•ªã ã‘ã§ãªãã€å…¨ã‚·ãƒŠãƒªã‚ªã§ã€Œè‡ªå¾‹èª¿æŸ»ã€ãƒœã‚¿ãƒ³ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’çµ±åˆã—ã¾ã™
is_live_mode = selected_scenario == "4. [Live] Ciscoå®Ÿæ©Ÿè¨ºæ–­"

# --- ã‚¢ãƒ©ãƒ¼ãƒ ç”Ÿæˆ (ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨) ---
alarms = []
if selected_scenario == "1. WANå…¨å›ç·šæ–­":
    alarms = [Alarm("WAN_ROUTER_01", "Down", "CRITICAL"), Alarm("AP_01", "Unreach", "CRITICAL")]
elif selected_scenario == "2. FWç‰‡ç³»éšœå®³":
    alarms = [Alarm("FW_01_PRIMARY", "HB Loss", "WARNING")]
elif selected_scenario == "3. L2SWã‚µã‚¤ãƒ¬ãƒ³ãƒˆéšœå®³":
    alarms = [Alarm("AP_01", "Lost", "CRITICAL"), Alarm("AP_02", "Lost", "CRITICAL")]

root_cause = None
reason = ""
if alarms:
    engine = CausalInferenceEngine(TOPOLOGY)
    res = engine.analyze_alarms(alarms)
    root_cause = res.root_cause_node
    reason = res.root_cause_reason

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ ---
col1, col2 = st.columns([1, 1])

# å·¦ã‚«ãƒ©ãƒ ï¼šãƒˆãƒãƒ­ã‚¸ãƒ¼ ï¼† å®Ÿæ©Ÿèª¿æŸ»ãƒœã‚¿ãƒ³
with col1:
    st.subheader("Network Status")
    st.graphviz_chart(render_topology(alarms, root_cause), use_container_width=True)
    
    if root_cause or is_live_mode:
        if root_cause:
            st.markdown(f'<div style="color:#d32f2f;background:#fdecea;padding:10px;border-radius:5px;">ğŸš¨ ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆï¼š{root_cause.id} ãƒ€ã‚¦ãƒ³</div>', unsafe_allow_html=True)
            st.caption(f"ç†ç”±: {reason}")
        
        st.markdown("---")
        st.info("ğŸ›  **è‡ªå¾‹èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**")
        st.markdown("SSHæ¥ç¶šã«ã‚ˆã‚‹è©³ç´°è¨ºæ–­ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        
        # è‡ªå¾‹èª¿æŸ»ãƒœã‚¿ãƒ³
        if st.session_state.live_result:
            res = st.session_state.live_result
            if res["status"] == "SUCCESS":
                # â˜…è¿½åŠ ï¼šã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒŠãƒ¼
                st.success("ğŸ›¡ï¸ **Data Sanitized**: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒ»IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ãƒã‚¹ã‚¯å‡¦ç†ã—ã¾ã—ãŸã€‚")
                
                with st.expander("å–å¾—ãƒ­ã‚°ç¢ºèª", expanded=True):
                    st.code(res["sanitized_log"], language="text")
            else:
                st.error(f"è¨ºæ–­çµæœ: {res['error']}")

        # è¨ºæ–­çµæœï¼ˆãƒ­ã‚°ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ï¼‰ã®è¡¨ç¤º
        if st.session_state.live_result:
            res = st.session_state.live_result
            if res["status"] == "SUCCESS":
                
                # â˜…ã“ã“ã‚’è¿½åŠ ï¼šã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒ”ãƒ¼ãƒ«ç”¨ã®ãƒãƒŠãƒ¼
                st.success("ğŸ›¡ï¸ **Security Filter Active**: æ©Ÿå¯†æƒ…å ±ï¼ˆIP, Passwordï¼‰ã¯ãƒã‚¹ã‚¯å‡¦ç†å¾Œã«AIã¸é€ä¿¡ã•ã‚Œã¾ã™ã€‚")
                
                with st.expander("ğŸ“„ å–å¾—ãƒ­ã‚° (Sanitized View)", expanded=True):
                    # ãƒ­ã‚°ã®ä¸­èº«ã‚’è¡¨ç¤ºï¼ˆ<HIDDEN>ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ãƒãƒ©è¦‹ã›ã™ã‚‹ï¼‰
                    st.code(res["sanitized_log"], language="text")
            else:
                st.error(f"è¨ºæ–­çµæœ: {res['error']}")

# å³ã‚«ãƒ©ãƒ ï¼šAIãƒãƒ£ãƒƒãƒˆ
with col2:
    st.subheader("AI Analyst Report")
    
    # LiveçµæœãŒã‚ã‚‹å ´åˆã€ãã‚Œã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
    if st.session_state.live_result:
        live_data = st.session_state.live_result
        
        # ãƒãƒ£ãƒƒãƒˆåˆæœŸåŒ–
        if st.session_state.chat_session is None:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel("gemini-2.0-flash", generation_config={"temperature": 0.0})
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã€å‡ºåŠ›è¦ä»¶ã€‘ã‚’æ›¸ãæ›ãˆ
            system_prompt = f"""
            ã‚ãªãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚ä»¥ä¸‹ã®è¨ºæ–­çµæœã«åŸºã¥ãã€ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®çµŒç·¯ã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚

            ã€è¨ºæ–­å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‘
            ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {live_data['status']}
            è©³ç´°/ãƒ­ã‚°: {live_data.get('sanitized_log') or live_data.get('error')}
            æ¨è«–ã•ã‚ŒãŸåŸå› : {reason if reason else "å®Ÿæ©Ÿèª¿æŸ»ãƒ¢ãƒ¼ãƒ‰"}

            ã€å‡ºåŠ›è¦ä»¶ã€‘
            ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚
            
            ### ğŸ›  ãƒã‚¯ã‚¹ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ
            
            **1. ãƒ‡ãƒ¼ã‚¿ä¿å…¨ã¨æ¥ç¶šç¢ºèª:**
            æ¥ç¶šè©¦è¡ŒãŠã‚ˆã³ãƒ­ã‚°å–å¾—ã‚’å®Ÿæ–½ã€‚
            â†’ **çµæœ: {live_data['status']}** (ğŸ›¡ï¸ æ©Ÿå¯†æƒ…å ±ã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿)
            
            **2. è©³ç´°åˆ†æ:**
            [æ¥ç¶šã§ããŸå ´åˆã¯ãƒ­ã‚°å†…å®¹ï¼ˆConfig/Interfaceï¼‰ã®åˆ†æã€ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è¦å› æ¨æ¸¬]
            â†’ [åˆ†æçµæœ]
            
            **3. ç‰©ç†/ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ç¢ºèª:**
            [çŠ¶æ³ã«å¿œã˜ãŸæ¨è«–]
            â†’ [åˆ†æçµæœ]
            
            ---
            **æœ€çµ‚åˆ¤å®š:** [çµè«–]
            """
            
            history = [{"role": "user", "parts": [system_prompt]}]
            chat = model.start_chat(history=history)
            
            with st.spinner("Gemini is analyzing diagnostic data..."):
                try:
                    response = chat.send_message("ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
                    st.session_state.chat_session = chat
                    st.session_state.messages.append({"role": "assistant", "content": response.text})
                except Exception as e:
                    st.error(str(e))

    # ãƒãƒ£ãƒƒãƒˆUI
    chat_container = st.container(height=500)
    with chat_container:
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
    
    if prompt := st.chat_input("è³ªå•..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with chat_container:
            with st.chat_message("user"): st.markdown(prompt)
        if st.session_state.chat_session:
            with chat_container:
                with st.chat_message("assistant"):
                    with st.spinner("Thinking..."):
                        res = st.session_state.chat_session.send_message(prompt)
                        st.markdown(res.text)
                        st.session_state.messages.append({"role": "assistant", "content": res.text})
