import streamlit as st
import pandas as pd
import json
import time
import re
import hashlib
from typing import Optional, List, Dict, Any

# Google Generative AI
try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False

from registry import get_paths, load_topology, get_display_name
from alarm_generator import generate_alarms_for_scenario, Alarm, get_alarm_summary
from inference_engine import LogicalRCA
from network_ops import (
    generate_analyst_report_streaming, 
    generate_remediation_commands_streaming, 
    run_remediation_parallel_v2, 
    RemediationEnvironment,
    sanitize_output
)
from utils.helpers import get_status_from_alarms, get_status_icon, load_config_by_id
from utils.llm_helper import get_rate_limiter, generate_content_with_retry
from verifier import verify_log_content
from .graph import render_topology_graph

# =====================================================
# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =====================================================
def _hash_text(text: str) -> str:
    return hashlib.sha256((text or "").encode("utf-8")).hexdigest()[:16]

def _pick_first(mapping: dict, keys: list, default: str = "") -> str:
    for k in keys:
        try:
            v = mapping.get(k, None)
        except Exception:
            v = None
        if v is None: continue
        if isinstance(v, (int, float, bool)):
            s = str(v)
            if s: return s
        elif isinstance(v, str):
            if v.strip(): return v.strip()
    return default

def _build_ci_context_for_chat(topology: dict, target_node_id: str) -> dict:
    node = topology.get(target_node_id) if target_node_id else None
    if node:
        if hasattr(node, 'metadata'):
            md = node.metadata or {}
        else:
            md = node.get('metadata', {}) if isinstance(node, dict) else {}
    else:
        md = {}

    ci = {
        "device_id": target_node_id or "",
        "hostname": _pick_first(md, ["hostname", "host", "name"], default=(target_node_id or "")),
        "vendor": _pick_first(md, ["vendor", "manufacturer"], default=""),
        "os": _pick_first(md, ["os", "platform"], default=""),
        "model": _pick_first(md, ["model", "hw_model"], default=""),
        "role": _pick_first(md, ["role", "type"], default=""),
        "layer": _pick_first(md, ["layer", "level"], default=""),
        "site": _pick_first(md, ["site", "location"], default=""),
    }
    try:
        conf = load_config_by_id(target_node_id) if target_node_id else ""
        if conf: ci["config_excerpt"] = conf[:1500]
    except Exception: pass
    return ci

def run_diagnostic_simulation_no_llm(selected_scenario: str, target_node_obj) -> dict:
    device_id = getattr(target_node_obj, "id", "UNKNOWN") if target_node_obj else "UNKNOWN"
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    lines = [f"[PROBE] ts={ts}", f"[PROBE] scenario={selected_scenario}", f"[PROBE] target_device={device_id}", ""]

    recovered_devices = st.session_state.get("recovered_devices") or {}
    recovered_map = st.session_state.get("recovered_scenario_map") or {}

    if recovered_devices.get(device_id) and recovered_map.get(device_id) == selected_scenario:
        if "FW" in selected_scenario:
            lines += ["show chassis cluster status", "Redundancy group 0: healthy", "control link: up"]
        elif "WAN" in selected_scenario:
            lines += ["show ip interface brief", "GigabitEthernet0/0 up up", "show ip bgp summary", "Neighbor 203.0.113.2 Established"]
        elif "L2SW" in selected_scenario:
            lines += ["show environment", "Fan: OK", "Temperature: OK", "show interface status", "Uplink: up"]
        else:
            lines += ["show system alarms", "No active alarms", "ping 8.8.8.8 repeat 5", "Success rate is 100 percent"]
        return {"status": "SUCCESS", "sanitized_log": "\n".join(lines), "device_id": device_id}

    if "WANå…¨å›ç·šæ–­" in selected_scenario or "[WAN]" in selected_scenario:
        lines += ["show ip interface brief", "GigabitEthernet0/0 down down", "Neighbor 203.0.113.2 Idle"]
    elif "FWç‰‡ç³»éšœå®³" in selected_scenario or "[FW]" in selected_scenario:
        lines += ["show chassis cluster status", "Redundancy group 0: degraded", "control link: down"]
    elif "L2SW" in selected_scenario:
        lines += ["show environment", "Fan: FAIL", "Temperature: HIGH", "show interface status", "Uplink: flapping"]
    else:
        lines += ["show system alarms", "No active alarms"]

    return {"status": "SUCCESS", "sanitized_log": "\n".join(lines), "device_id": device_id}

# =====================================================
# ãƒ¡ã‚¤ãƒ³æç”»é–¢æ•°
# =====================================================
def render_incident_cockpit(site_id: str, api_key: Optional[str]):
    display_name = get_display_name(site_id)
    scenario = st.session_state.site_scenarios.get(site_id, "æ­£å¸¸ç¨¼åƒ")
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ & æˆ»ã‚‹ãƒœã‚¿ãƒ³
    col_header = st.columns([4, 1])
    with col_header[0]:
        st.markdown(f"### ğŸ›¡ï¸ AIOps ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ»ã‚³ãƒƒã‚¯ãƒ”ãƒƒãƒˆ")
    with col_header[1]:
        st.markdown('<span id="back-btn-marker"></span>', unsafe_allow_html=True)
        st.markdown("""
        <style>
        #back-btn-marker + div button,
        #back-btn-marker ~ div[data-testid="stButton"] button {
            background-color: #d32f2f !important;
            color: white !important;
            border: 2px solid #b71c1c !important;
            font-weight: bold !important;
            border-radius: 8px !important;
        }
        #back-btn-marker + div button:hover,
        #back-btn-marker ~ div[data-testid="stButton"] button:hover {
            background-color: #b71c1c !important;
        }
        </style>
        """, unsafe_allow_html=True)
        
        if st.button("ğŸ”™ ä¸€è¦§ã«æˆ»ã‚‹", key="back_to_list"):
            st.session_state.active_site = None
            st.rerun()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    paths = get_paths(site_id)
    topology = load_topology(paths.topology_path)
    if not topology:
        st.error("ãƒˆãƒãƒ­ã‚¸ãƒ¼ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    alarms = generate_alarms_for_scenario(topology, scenario)
    status = get_status_from_alarms(scenario, alarms)
    
    injected = st.session_state.get("injected_weak_signal")
    if injected and injected["device_id"] in topology:
        for m in injected.get("messages", []):
            alarms.append(Alarm(injected["device_id"], m, "INFO", False))

    # åˆ†æã‚¨ãƒ³ã‚¸ãƒ³
    engine_key = f"engine_{site_id}"
    if engine_key not in st.session_state.logic_engines:
        st.session_state.logic_engines[engine_key] = LogicalRCA(topology)
    engine = st.session_state.logic_engines[engine_key]
    
    analysis_results = engine.analyze(alarms) if alarms else [{
        "id": "SYSTEM", "label": "æ­£å¸¸ç¨¼åƒ", "prob": 0.0, "type": "Normal", "tier": 3, "reason": "ã‚¢ãƒ©ãƒ¼ãƒ ãªã—"
    }]
    
    # KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹
    root_cause_alarms = [a for a in alarms if a.is_root_cause]
    total_alarms = len(alarms)
    noise_reduction = ((total_alarms - len(root_cause_alarms)) / total_alarms * 100) if total_alarms > 0 else 0.0
    action_required = len(set(a.device_id for a in root_cause_alarms))
    prediction_count = len([r for r in analysis_results if r.get('is_prediction')])
    
    st.markdown("---")
    cols = st.columns(3)
    cols[0].metric("ğŸš¨ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", f"{get_status_icon(status)} {status}")
    cols[1].metric("ğŸ“Š ã‚¢ãƒ©ãƒ¼ãƒ æ•°", f"{len(alarms)}ä»¶")
    suspect_count = len([r for r in analysis_results if r.get('prob', 0) > 0.5])
    cols[2].metric("ğŸ¯ è¢«ç–‘ç®‡æ‰€", f"{suspect_count}ä»¶", delta=f"ã†ã¡ğŸ”®äºˆå…† {prediction_count}ä»¶" if prediction_count else None, delta_color="off")
    
    kpi_cols = st.columns(3)
    with kpi_cols[0]:
        st.metric("ğŸ“‰ ãƒã‚¤ã‚ºå‰Šæ¸›ç‡", f"{noise_reduction:.1f}%", delta="â†‘ é«˜åŠ¹ç‡" if noise_reduction > 90 else "é€šå¸¸")
    with kpi_cols[1]:
        st.metric("ğŸ”® äºˆå…†æ¤œçŸ¥", f"{prediction_count}ä»¶", delta="âš¡ è¦æ³¨æ„" if prediction_count > 0 else "å•é¡Œãªã—", delta_color="inverse")
    with kpi_cols[2]:
        st.metric("ğŸš¨ è¦å¯¾å¿œã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ", f"{action_required}ä»¶", delta="â†‘ å¯¾å‡¦å¿…è¦" if action_required > 0 else "å•é¡Œãªã—", delta_color="inverse")
    
    st.markdown("---")
    
    # Future Radar
    preds = [c for c in analysis_results if c.get('is_prediction')]
    if preds:
        st.markdown("### ğŸ”® AIOps Future Radar")
        with st.container(border=True):
            injected_info = st.session_state.get("injected_weak_signal")
            scenario_lbl = f"ï¼ˆåŠ£åŒ–ã‚·ãƒŠãƒªã‚ª: {injected_info.get('scenario')}ï¼‰" if injected_info else ""
            st.info(f"âš ï¸ **äºˆå…†æ¤œçŸ¥**: å°†æ¥ã®éšœå®³ãƒªã‚¹ã‚¯ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚{scenario_lbl}")
            
            radar_cols = st.columns(min(len(preds), 3))
            for idx, item in enumerate(preds[:3]):
                with radar_cols[idx]:
                    prob_pct = f"{item.get('prob',0)*100:.0f}%"
                    st.error(f"**ğŸ“ {item['id']}**")
                    st.markdown(f"<div style='text-align:center;'><span style='font-size:36px;font-weight:bold;color:#d32f2f;'>{prob_pct}</span><br>ç™ºç”Ÿç¢ºç‡</div>", unsafe_allow_html=True)
                    st.divider()
                    st.markdown(f"**äºˆæ¸¬éšœå®³:** {item.get('label','').replace('ğŸ”® [äºˆå…†] ', '')}")
                    st.markdown(f"**æ€¥æ€§æœŸ:** {item.get('prediction_timeline','ä¸æ˜')}")
                    with st.expander("ğŸ” æ¤œçŸ¥è©³ç´°"):
                        st.text(item.get('reason', ''))
        st.markdown("---")

    # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå€™è£œ & å½±éŸ¿ç¯„å›²ãƒªã‚¹ãƒˆ
    root_cause_ids = {a.device_id for a in alarms if a.is_root_cause}
    downstream_ids = {a.device_id for a in alarms if not a.is_root_cause}
    
    rc_candidates = []
    ds_devices = []
    
    for c in analysis_results:
        did = c.get('id')
        if c.get('is_prediction') or did in root_cause_ids or c.get('prob', 0) > 0.5:
            rc_candidates.append(c)
        elif did in downstream_ids:
            ds_devices.append(c)
            
    if not rc_candidates and not alarms:
        rc_candidates = [{"id": "SYSTEM", "label": "æ­£å¸¸ç¨¼åƒ", "prob": 0.0, "type": "Normal"}]

    selected_cand = None
    target_dev_id = None

    if rc_candidates:
        df_data = []
        for i, c in enumerate(rc_candidates, 1):
            prob = c.get('prob', 0)
            act = "âš¡ äºˆå…†å¯¾å¿œ" if c.get('is_prediction') else "ğŸš€ è‡ªå‹•ä¿®å¾©" if prob > 0.8 else "ğŸ” èª¿æŸ»"
            status_txt = "ğŸ”® äºˆå…†" if c.get('is_prediction') else "ğŸ”´ å±é™º" if prob > 0.9 else "ğŸŸ¡ è­¦å‘Š"
            
            df_data.append({
                "é †ä½": i, "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": status_txt, "ãƒ‡ãƒã‚¤ã‚¹": c['id'], 
                "åŸå› ": c.get('label'), "ç¢ºä¿¡åº¦": f"{prob*100:.0f}%", "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³": act,
                "_id": c['id']
            })
        
        st.markdown("#### ğŸ¯ æ ¹æœ¬åŸå› å€™è£œ")
        df = pd.DataFrame(df_data)
        event = st.dataframe(df.drop(columns=["_id"]), use_container_width=True, hide_index=True, selection_mode="single-row", on_select="rerun")
        
        if event.selection and event.selection.rows:
            sel_row = df.iloc[event.selection.rows[0]]
            for c in rc_candidates:
                if c['id'] == sel_row['_id']:
                    selected_cand = c
                    target_dev_id = c['id']
                    break
        elif rc_candidates:
            selected_cand = rc_candidates[0]
            target_dev_id = rc_candidates[0]['id']

        # â˜… ã“ã“ãŒå¾©æ´»ç®‡æ‰€: å½±éŸ¿ã‚’å—ã‘ã¦ã„ã‚‹æ©Ÿå™¨ï¼ˆä¸Šæµå¾©æ—§å¾…ã¡ï¼‰ãƒªã‚¹ãƒˆ
        if ds_devices:
            with st.expander(f"â–¼ å½±éŸ¿ã‚’å—ã‘ã¦ã„ã‚‹æ©Ÿå™¨ ({len(ds_devices)}å°) - ä¸Šæµå¾©æ—§å¾…ã¡", expanded=False):
                dd_df = pd.DataFrame([
                    {"No": i+1, "ãƒ‡ãƒã‚¤ã‚¹": d['id'], "çŠ¶æ…‹": "âš« å¿œç­”ãªã—", "å‚™è€ƒ": "ä¸Šæµå¾©æ—§å¾…ã¡"}
                    for i, d in enumerate(ds_devices)
                ])
                if len(ds_devices) >= 10:
                    with st.container(height=300):
                        st.dataframe(dd_df, use_container_width=True, hide_index=True)
                else:
                    st.dataframe(dd_df, use_container_width=True, hide_index=True)

    # 2ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    col_map, col_chat = st.columns([1.2, 1])
    
    with col_map:
        st.subheader("ğŸŒ Network Topology")
        st.graphviz_chart(render_topology_graph(topology, alarms, analysis_results), use_container_width=True)
        
        st.markdown("---")
        st.subheader("ğŸ› ï¸ Auto-Diagnostics")
        if st.button("ğŸš€ è¨ºæ–­å®Ÿè¡Œ (Run Diagnostics)", type="primary"):
            if not api_key:
                st.error("API Key Required")
            else:
                with st.status("Agent Operating...", expanded=True) as status:
                    target_node = topology.get(target_dev_id)
                    res = run_diagnostic_simulation_no_llm(scenario, target_node)
                    st.session_state.live_result = res
                    if res["status"] == "SUCCESS":
                        status.update(label="Diagnostics Complete!", state="complete", expanded=False)
                        st.session_state.verification_result = verify_log_content(res.get('sanitized_log', ""))
                    else:
                        st.write("âŒ Failed.")
                        status.update(label="Failed", state="error")
                st.rerun()
        
        if st.session_state.live_result:
            res = st.session_state.live_result
            st.markdown("#### ğŸ“„ Diagnostic Results")
            with st.container(border=True):
                if st.session_state.verification_result:
                    v = st.session_state.verification_result
                    c1, c2, c3 = st.columns(3)
                    c1.metric("Ping", v.get('ping_status'))
                    c2.metric("Interface", v.get('interface_status'))
                    c3.metric("HW", v.get('hardware_status'))
                st.divider()
                st.code(res.get("sanitized_log"), language="text")

    with col_chat:
        st.subheader("ğŸ“ AI Analyst Report")
        if selected_cand:
            if st.session_state.generated_report is None:
                st.info(f"Target: **{selected_cand['id']}**")
                if api_key and (scenario != "æ­£å¸¸ç¨¼åƒ" or selected_cand.get('is_prediction')):
                    btn_label = "ğŸ”® äºˆå…†åˆ†æãƒ¬ãƒãƒ¼ãƒˆ" if selected_cand.get('is_prediction') else "ğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ"
                    if st.button(btn_label):
                        cont = st.empty()
                        t_node = topology.get(selected_cand['id'])
                        topology_context = {"id": selected_cand['id']}
                        target_conf = load_config_by_id(selected_cand['id'])
                        
                        cache_key = _hash_text(f"{site_id}|{scenario}|{selected_cand['id']}")
                        if cache_key in st.session_state.report_cache:
                            full_text = st.session_state.report_cache[cache_key]
                            cont.markdown(full_text)
                        else:
                            cont.write("ğŸ¤– AI åˆ†æä¸­...")
                            full_text = ""
                            try:
                                for chunk in generate_analyst_report_streaming(
                                    scenario, t_node, topology_context, target_conf, "", api_key
                                ):
                                    full_text += chunk
                                    cont.markdown(full_text)
                                st.session_state.report_cache[cache_key] = full_text
                            except Exception as e:
                                cont.error(f"Error: {e}")
                        st.session_state.generated_report = full_text
            else:
                with st.container(height=400, border=True):
                    st.markdown(st.session_state.generated_report)
                if st.button("ğŸ”„ å†ä½œæˆ"):
                    st.session_state.generated_report = None
                    st.rerun()
        
        st.markdown("---")
        st.subheader("ğŸ¤– Remediation & Chat")
        
        if selected_cand and selected_cand['prob'] > 0.6:
            if st.session_state.remediation_plan is None:
                if st.button("âœ¨ ä¿®å¾©ãƒ—ãƒ©ãƒ³ä½œæˆ"):
                    if not st.session_state.generated_report:
                        st.warning("å…ˆã«ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„")
                    else:
                        cont = st.empty()
                        t_node = topology.get(selected_cand['id'])
                        rem_text = ""
                        for chunk in generate_remediation_commands_streaming(
                            scenario, st.session_state.generated_report, t_node, api_key
                        ):
                            rem_text += chunk
                            cont.markdown(rem_text)
                        st.session_state.remediation_plan = rem_text
                        st.rerun()
            
            if st.session_state.remediation_plan:
                with st.container(height=300, border=True):
                    st.info("AI Remediation Plan")
                    st.markdown(st.session_state.remediation_plan)
                
                c1, c2 = st.columns(2)
                with c1:
                    if st.button("ğŸš€ ä¿®å¾©å®Ÿè¡Œ", type="primary"):
                        with st.status("Executing Fix...", expanded=True):
                            t_node = topology.get(selected_cand['id'])
                            dev_info = t_node.metadata if t_node and hasattr(t_node, 'metadata') else {}
                            res = run_remediation_parallel_v2(selected_cand['id'], dev_info, scenario)
                            st.write("âœ… Done.")
                            st.session_state.recovered_devices[selected_cand['id']] = True
                            st.balloons()
                with c2:
                    if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«"):
                        st.session_state.remediation_plan = None
                        st.rerun()

        with st.expander("ğŸ’¬ Chat with AI Agent", expanded=False):
            if not st.session_state.chat_session and api_key and GENAI_AVAILABLE:
                genai.configure(api_key=api_key)
                st.session_state.chat_session = genai.GenerativeModel("gemma-3-12b-it").start_chat(history=[])
            
            tab_chat, tab_hist = st.tabs(["ğŸ’¬ ä¼šè©±", "ğŸ“ å±¥æ­´"])
            with tab_chat:
                if st.session_state.messages:
                    last = st.session_state.messages[-1]
                    if last["role"] == "assistant":
                        st.info("ğŸ¤– " + last["content"])
                
                prompt = st.text_area("è³ªå•:", height=70, key="chat_in")
                if st.button("é€ä¿¡", type="primary") and prompt:
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    if st.session_state.chat_session:
                        ci = _build_ci_context_for_chat(topology, target_dev_id)
                        full_p = f"Context: {json.dumps(ci)}\nQuestion: {prompt}"
                        with st.spinner("AI thinking..."):
                            resp = generate_content_with_retry(st.session_state.chat_session.model, full_p, stream=False)
                            if resp:
                                st.session_state.messages.append({"role": "assistant", "content": resp.text})
                    st.rerun()
