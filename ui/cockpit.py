import streamlit as st
import pandas as pd
import json
import time
import re
import hashlib
from typing import Optional, List, Dict, Any

try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False

from registry import get_paths, load_topology, get_display_name
from alarm_generator import generate_alarms_for_scenario, Alarm, get_alarm_summary
from inference_engine import LogicalRCA
from network_ops import (
    generate_analyst_report_streaming, 
    generate_remediation_commands_streaming, 
    run_remediation_parallel_v2, 
    RemediationEnvironment
)
from utils.helpers import get_status_from_alarms, get_status_icon, load_config_by_id
from utils.llm_helper import get_rate_limiter, generate_content_with_retry
from verifier import verify_log_content
from .graph import render_topology_graph

# =====================================================
# å¾©å…ƒã•ã‚ŒãŸãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =====================================================
def _pick_first(mapping: dict, keys: list, default: str = "") -> str:
    """ãƒãƒƒãƒ”ãƒ³ã‚°ã‹ã‚‰æœ€åˆã®éç©ºå€¤ã‚’å–å¾—"""
    for k in keys:
        try:
            v = mapping.get(k)
            if v: return str(v)
        except: pass
    return default

def _build_ci_context_for_chat(topology: dict, target_node_id: str) -> dict:
    """ãƒãƒ£ãƒƒãƒˆç”¨ã®CIã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
    node = topology.get(target_node_id)
    md = node.get('metadata', {}) if node and isinstance(node, dict) else (getattr(node, 'metadata', {}) if node else {})
    ci = {
        "device_id": target_node_id or "",
        "hostname": _pick_first(md, ["hostname", "host"], default=target_node_id or ""),
        "vendor": _pick_first(md, ["vendor"], default=""),
        "model": _pick_first(md, ["model"], default=""),
    }
    try:
        conf = load_config_by_id(target_node_id)
        if conf: ci["config_excerpt"] = conf[:1000]
    except: pass
    return ci

def run_diagnostic_simulation_no_llm(scenario: str, target_node) -> dict:
    """è¨ºæ–­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ç–‘ä¼¼å®Ÿè¡Œ"""
    dev_id = getattr(target_node, "id", "UNKNOWN") if target_node else "UNKNOWN"
    lines = [f"[PROBE] scenario={scenario}", f"target={dev_id}", ""]
    if "WAN" in scenario:
        lines += ["show ip int brief", "Gi0/0/0 UP", "BGP State: Established"]
    else:
        lines += ["show system alarms", "No active alarms"]
    return {"status": "SUCCESS", "sanitized_log": "\n".join(lines), "device_id": dev_id}

# =====================================================
# ãƒ¡ã‚¤ãƒ³æç”»é–¢æ•°
# =====================================================
def render_incident_cockpit(site_id: str, api_key: Optional[str]):
    display_name = get_display_name(site_id)
    scenario = st.session_state.site_scenarios.get(site_id, "æ­£å¸¸ç¨¼åƒ")
    
    # å¾©å…ƒã•ã‚ŒãŸãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã€Œæˆ»ã‚‹ã€ãƒœã‚¿ãƒ³
    col_header = st.columns([4, 1])
    with col_header[0]:
        st.markdown(f"### ğŸ›¡ï¸ AIOps ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ»ã‚³ãƒƒã‚¯ãƒ”ãƒƒãƒˆ")
    with col_header[1]:
        st.markdown('<span id="back-btn-marker"></span>', unsafe_allow_html=True)
        st.markdown("""
        <style>
        #back-btn-marker + div button {
            background-color: #d32f2f !important;
            color: white !important;
            font-weight: bold !important;
        }
        </style>
        """, unsafe_allow_html=True)
        if st.button("ğŸ”™ ä¸€è¦§ã«æˆ»ã‚‹", key="back_btn"):
            st.session_state.active_site = None
            st.rerun()

    # ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
    paths = get_paths(site_id)
    topology = load_topology(paths.topology_path)
    alarms = generate_alarms_for_scenario(topology, scenario)
    
    # äºˆå…†ã‚·ã‚°ãƒŠãƒ«æ³¨å…¥
    injected = st.session_state.get("injected_weak_signal")
    if injected and injected["device_id"] in topology:
        for m in injected.get("messages", []):
            alarms.append(Alarm(injected["device_id"], m, "INFO", False))

    engine_key = f"engine_{site_id}"
    if engine_key not in st.session_state.logic_engines:
        st.session_state.logic_engines[engine_key] = LogicalRCA(topology)
    engine = st.session_state.logic_engines[engine_key]
    
    results = engine.analyze(alarms) if alarms else []
    status = get_status_from_alarms(scenario, alarms)

    # å¾©å…ƒã•ã‚ŒãŸKPIãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    st.markdown("---")
    k1, k2, k3 = st.columns(3)
    k1.metric("ğŸš¨ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", f"{get_status_icon(status)} {status}")
    k2.metric("ğŸ“Š ã‚¢ãƒ©ãƒ¼ãƒ æ•°", f"{len(alarms)}ä»¶")
    k3.metric("ğŸ¯ è¢«ç–‘ç®‡æ‰€", f"{len([r for r in results if r.get('prob', 0) > 0.5])}ä»¶")
    st.markdown("---")

    # =====================================================
    # æ ¹æœ¬åŸå› ã¨å½±éŸ¿ç¯„å›²ã®å³å¯†ãªåˆ†é›¢
    # =====================================================
    root_ids = {a.device_id for a in alarms if a.is_root_cause}
    ds_ids = {a.device_id for a in alarms if not a.is_root_cause}
    
    # æ ¹æœ¬åŸå› å€™è£œ: ã‚¢ãƒ©ãƒ¼ãƒ ã§æ ¹æœ¬åŸå› åˆ¤å®šã•ã‚ŒãŸã‚‚ã®ã€ã¾ãŸã¯äºˆå…†
    rc_list = [r for r in results if r.get('is_prediction') or r['id'] in root_ids]
    # å½±éŸ¿ãƒ‡ãƒã‚¤ã‚¹: æ ¹æœ¬åŸå› ä»¥å¤–ã®ã‚¢ãƒ©ãƒ¼ãƒ ãŒå‡ºã¦ã„ã‚‹ã‚‚ã®
    ds_list = [r for r in results if r['id'] in ds_ids and r['id'] not in root_ids]

    # é’å¸¯ãƒãƒŠãƒ¼ã®å¾©å…ƒ
    if rc_list and ds_list:
        st.info(f"ğŸ“ **æ ¹æœ¬åŸå› **: {rc_list[0]['id']} â†’ å½±éŸ¿ç¯„å›²: é…ä¸‹ {len(ds_list)} æ©Ÿå™¨")

    # Future Radar (äºˆå…†ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç¤º)
    preds = [r for r in rc_list if r.get('is_prediction')]
    if preds:
        with st.container(border=True):
            st.markdown("##### ğŸ”® AIOps Future Radar (Precognition)")
            for p in preds:
                st.warning(f"âš ï¸ **{p['id']}**: æ·±åˆ»ãªéšœå®³ã¸é€²å±•ã™ã‚‹æã‚ŒãŒã‚ã‚Šã¾ã™ã€‚æ€¥æ€§æœŸã¾ã§æ®‹ã‚Šç´„{p.get('prediction_time_to_critical_min', 60)}åˆ†")
                # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å³æ™‚æç¤º
                rec_actions = p.get("recommended_actions", [])
                if rec_actions:
                    st.markdown(f"ğŸ‘‰ **ã¾ãšã‚„ã‚‹ã¹ãã“ã¨:** {rec_actions[0]['title']} ({rec_actions[0]['effect']})")

    # æ ¹æœ¬åŸå› å€™è£œãƒ†ãƒ¼ãƒ–ãƒ«ã®æç”»
    if rc_list:
        st.markdown("#### ğŸ¯ æ ¹æœ¬åŸå› å€™è£œ")
        df_rc = pd.DataFrame([{
            "é †ä½": i+1,
            "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "ğŸ”® äºˆå…†" if x.get('is_prediction') else "ğŸ”´ å±é™º (æ ¹æœ¬åŸå› )" if x['prob']>=0.9 else "ğŸŸ¡ è­¦å‘Š",
            "ãƒ‡ãƒã‚¤ã‚¹": x['id'],
            "åŸå› ": x.get('label'),
            "ç¢ºä¿¡åº¦": f"{x['prob']*100:.0f}%",
            "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³": "ğŸš€ è‡ªå‹•ä¿®å¾©ãŒå¯èƒ½" if x['prob']>=0.8 else "ğŸ” è©³ç´°èª¿æŸ»",
            "_obj": x
        } for i, x in enumerate(rc_list)])
        
        event = st.dataframe(
            df_rc.drop(columns=["_obj"]), 
            use_container_width=True, 
            hide_index=True, 
            selection_mode="single-row", 
            on_select="rerun"
        )
        
        if event.selection and len(event.selection.rows) > 0:
            st.session_state.selected_candidate = df_rc.iloc[event.selection.rows[0]]["_obj"]
        elif rc_list and not st.session_state.get("selected_candidate"):
            st.session_state.selected_candidate = rc_list[0]

    # å½±éŸ¿ã‚’å—ã‘ã¦ã„ã‚‹æ©Ÿå™¨ãƒªã‚¹ãƒˆã®å¾©å…ƒ
    if ds_list:
        with st.expander(f"â–¼ å½±éŸ¿ã‚’å—ã‘ã¦ã„ã‚‹æ©Ÿå™¨ ({len(ds_list)}å°) - ä¸Šæµå¾©æ—§å¾…ã¡", expanded=False):
            st.dataframe(
                pd.DataFrame([{"No": i+1, "ãƒ‡ãƒã‚¤ã‚¹": d['id'], "çŠ¶æ…‹": "âš« å¿œç­”ãªã—", "å‚™è€ƒ": "ä¸Šæµå¾©æ—§å¾…ã¡"} for i, d in enumerate(ds_list)]), 
                use_container_width=True, 
                hide_index=True
            )

    # ä»¥å‰ã®2ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®ç¶­æŒ
    col_l, col_r = st.columns([1.2, 1])
    
    # === å·¦ã‚«ãƒ©ãƒ : ãƒˆãƒãƒ­ã‚¸ãƒ¼ & è¨ºæ–­ ===
    with col_l:
        st.subheader("ğŸŒ Network Topology")
        st.graphviz_chart(render_topology_graph(topology, alarms, results), use_container_width=True)
        
        st.markdown("---")
        st.subheader("ğŸ› ï¸ Auto-Diagnostics")
        if st.button("ğŸš€ è¨ºæ–­å®Ÿè¡Œ (Run Diagnostics)", type="primary"):
            with st.status("ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¨ºæ–­ãƒ­ã‚°ã‚’åé›†ä¸­..."):
                res = run_diagnostic_simulation_no_llm(scenario, st.session_state.get("selected_candidate"))
                st.session_state.live_result = res
            st.rerun()
        
        if st.session_state.get("live_result"):
            res = st.session_state.live_result
            st.markdown("#### ğŸ“„ Diagnostic Results")
            st.code(res.get("sanitized_log"), language="text")

    # === å³ã‚«ãƒ©ãƒ : AIåˆ†æ & ãƒãƒ£ãƒƒãƒˆ ===
    with col_r:
        st.subheader("ğŸ“ AI Analyst & Chat")
        cand = st.session_state.get("selected_candidate")
        if cand:
            st.info(f"Target: **{cand['id']}**\n{cand.get('label')}")
            
            tab_rpt, tab_chat = st.tabs(["ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆ", "ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ"])
            with tab_rpt:
                if st.button("ğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"):
                    with st.spinner("AIãŒãƒˆãƒãƒ­ã‚¸ãƒ¼ã¨ãƒ­ã‚°ã‚’åˆ†æä¸­..."):
                        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—
                        time.sleep(1) # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                        st.session_state.generated_report = f"### åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {cand['id']}\nãƒ‡ã‚¸ã‚¿ãƒ«ãƒ„ã‚¤ãƒ³ã®æ¨è«–ã«åŸºã¥ãã€..."
                
                if st.session_state.generated_report:
                    st.markdown(st.session_state.generated_report)
            
            with tab_chat:
                if not st.session_state.get("chat_session") and api_key:
                    genai.configure(api_key=api_key)
                    st.session_state.chat_session = genai.GenerativeModel("gemma-3-12b-it").start_chat(history=[])
                
                chat_cont = st.container(height=300)
                with chat_cont:
                    for msg in st.session_state.get("messages", []):
                        st.markdown(f"**{'ğŸ¤–' if msg['role']=='assistant' else 'ğŸ‘¤'}**: {msg['content']}")
                
                prompt = st.chat_input("AIã«è³ªå•...")
                if prompt:
                    st.session_state.setdefault("messages", []).append({"role": "user", "content": prompt})
                    # LLMå‘¼ã³å‡ºã—å‡¦ç†
                    st.rerun()
