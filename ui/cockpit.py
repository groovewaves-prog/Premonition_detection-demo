import streamlit as st
import pandas as pd
import json
import time
import re
import hashlib
from typing import Optional, List, Dict, Any

try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False

from registry import get_paths, load_topology, get_display_name
from alarm_generator import generate_alarms_for_scenario, Alarm, get_alarm_summary
from inference_engine import LogicalRCA
from network_ops import (
    generate_analyst_report_streaming, 
    generate_remediation_commands_streaming, 
    run_remediation_parallel_v2, 
    RemediationEnvironment
)
from utils.helpers import get_status_from_alarms, get_status_icon, load_config_by_id
from utils.llm_helper import get_rate_limiter, generate_content_with_retry
from verifier import verify_log_content
from .graph import render_topology_graph

# =====================================================
# Helper Functions
# =====================================================
def _hash_text(text: str) -> str:
    return hashlib.sha256((text or "").encode("utf-8")).hexdigest()[:16]

def _pick_first(mapping: dict, keys: list, default: str = "") -> str:
    for k in keys:
        try:
            v = mapping.get(k)
            if v: return str(v)
        except: pass
    return default

def _build_ci_context_for_chat(topology: dict, target_node_id: str) -> dict:
    node = topology.get(target_node_id)
    md = node.get('metadata', {}) if node and isinstance(node, dict) else (node.metadata if node else {})
    ci = {
        "device_id": target_node_id or "",
        "hostname": _pick_first(md, ["hostname", "host"], default=target_node_id or ""),
        "vendor": _pick_first(md, ["vendor"], default=""),
        "model": _pick_first(md, ["model"], default=""),
        "os": _pick_first(md, ["os"], default=""),
        "site": _pick_first(md, ["site", "location"], default="")
    }
    try:
        conf = load_config_by_id(target_node_id)
        if conf: ci["config_excerpt"] = conf[:1000]
    except: pass
    return ci

def run_diagnostic_simulation_no_llm(scenario: str, target_node) -> dict:
    dev_id = getattr(target_node, "id", "UNKNOWN") if target_node else "UNKNOWN"
    lines = [f"[PROBE] scenario={scenario}", f"target={dev_id}", ""]
    if "WAN" in scenario:
        lines += ["show ip int brief", "Gi0/0/0 UP", "BGP State: Established"]
    else:
        lines += ["show system alarms", "No active alarms"]
    return {"status": "SUCCESS", "sanitized_log": "\n".join(lines), "device_id": dev_id}

# =====================================================
# Main Render Function
# =====================================================
def render_incident_cockpit(site_id: str, api_key: Optional[str]):
    display_name = get_display_name(site_id)
    scenario = st.session_state.site_scenarios.get(site_id, "æ­£å¸¸ç¨¼åƒ")
    
    # --- Header ---
    c1, c2 = st.columns([4, 1])
    c1.markdown(f"### ğŸ›¡ï¸ AIOps ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ»ã‚³ãƒƒã‚¯ãƒ”ãƒƒãƒˆ")
    
    st.markdown("""
    <style>
    div[data-testid="stButton"] button { border-radius: 6px; }
    .st-emotion-cache-1r6slb0 { border: 1px solid #ddd; border-radius: 8px; padding: 15px; }
    </style>
    """, unsafe_allow_html=True)
    
    if c2.button("ğŸ”™ ä¸€è¦§ã«æˆ»ã‚‹", key="back_btn"):
        st.session_state.active_site = None
        st.rerun()

    # --- Load Data ---
    paths = get_paths(site_id)
    topology = load_topology(paths.topology_path)
    if not topology:
        st.error("ãƒˆãƒãƒ­ã‚¸ãƒ¼èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼")
        return

    alarms = generate_alarms_for_scenario(topology, scenario)
    status = get_status_from_alarms(scenario, alarms)
    
    # Injection
    injected = st.session_state.get("injected_weak_signal")
    if injected and injected["device_id"] in topology:
        for m in injected.get("messages", []):
            alarms.append(Alarm(injected["device_id"], m, "INFO", False))

    # Analysis
    engine_key = f"engine_{site_id}"
    if engine_key not in st.session_state.logic_engines:
        st.session_state.logic_engines[engine_key] = LogicalRCA(topology)
    engine = st.session_state.logic_engines[engine_key]
    
    # â˜… ä¿®æ­£ç®‡æ‰€: å¤‰æ•°åã‚’ results ã«çµ±ä¸€
    results = engine.analyze(alarms) if alarms else []
    if not results: results = [{"id": "SYSTEM", "label": "æ­£å¸¸ç¨¼åƒ", "prob": 0.0, "type": "Normal"}]

    # --- KPI & Precognition ---
    preds = [r for r in results if r.get('is_prediction')]
    pred_count = len(preds)
    
    st.markdown("---")
    k1, k2, k3 = st.columns(3)
    k1.metric("ğŸš¨ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", f"{get_status_icon(status)} {status}")
    k2.metric("ğŸ“Š ã‚¢ãƒ©ãƒ¼ãƒ æ•°", len(alarms))
    
    delta_color = "inverse" if pred_count > 0 else "off"
    delta_msg = "âš¡ å°†æ¥ã®ãƒªã‚¹ã‚¯ã‚’æ¤œçŸ¥" if pred_count > 0 else "å•é¡Œãªã—"
    k3.metric("ğŸ”® äºˆå…†æ¤œçŸ¥ (Precognition)", f"{pred_count}ä»¶", delta=delta_msg, delta_color=delta_color)
    
    st.markdown("---")

    # --- Future Radar (Precognition) ---
    if preds:
        st.markdown("### ğŸ”® AIOps Future Radar")
        st.caption("AIãŒäºˆæ¸¬ã™ã‚‹æœªæ¥ã®éšœå®³ã‚¤ãƒ™ãƒ³ãƒˆã€‚ã‚¯ãƒªãƒƒã‚¯ã§è©³ç´°åˆ†æã¸ã‚¸ãƒ£ãƒ³ãƒ—ã—ã¾ã™ã€‚")
        
        st.markdown("""
        <style>
        .future-card {
            border-left: 6px solid #9C27B0;
            background-color: #F3E5F5;
            padding: 12px;
            border-radius: 6px;
            margin-bottom: 10px;
        }
        </style>
        """, unsafe_allow_html=True)

        for p in preds:
            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³è¨ˆç®—ï¼ˆæ•°å€¤ã¨ã—ã¦å–å¾—ï¼‰
            time_min = p.get('prediction_time_to_critical_min', 60)
            
            with st.container():
                st.markdown(f"""
                <div class="future-card">
                    <div style="display:flex; justify-content:space-between; align-items:center;">
                        <div>
                            <span style="font-size:1.1em; font-weight:bold; color:#4A148C;">ğŸ“ {p['id']}</span>
                            <br><span style="color:#6A1B9A;">{p.get('label', '').replace('ğŸ”® [äºˆå…†] ', '')}</span>
                        </div>
                        <div style="text-align:right;">
                            <span style="font-size:1.2em; font-weight:bold; color:#880E4F;">{p.get('prob', 0)*100:.0f}%</span>
                            <br><span style="font-size:0.8em; color:#666;">ç™ºç”Ÿç¢ºç‡</span>
                        </div>
                    </div>
                    <div style="margin-top:8px; font-size:0.9em; color:#555;">
                        å½±éŸ¿ç¯„å›²: é…ä¸‹ {p.get('prediction_affected_count', 0)} å° / æ—©æœŸæ¤œçŸ¥: {p.get('prediction_early_warning_hours', 0)}æ™‚é–“å‰
                    </div>
                </div>
                """, unsafe_allow_html=True)
                
                c_time, c_act = st.columns([3, 1])
                with c_time:
                    # æ™‚é–“ã®é€¼è¿«åº¦ã«å¿œã˜ã¦è‰²ã‚’å¤‰ãˆã‚‹
                    bar_val = max(0, min(100, 100 - time_min))
                    st.progress(bar_val, text=f"ğŸ”¥ éšœå®³ç™ºç”Ÿã¾ã§: ã‚ã¨ç´„ {time_min} åˆ†")
                with c_act:
                    if st.button("è©³ç´°å¯¾å¿œ", key=f"btn_future_{p['id']}", type="primary", use_container_width=True):
                        st.session_state.selected_candidate = p

        st.markdown("---")

    # --- Candidates & Selection ---
    rc_list = []
    ds_list = []
    root_cause_ids = {a.device_id for a in alarms if a.is_root_cause}
    
    for r in results:
        if r.get('is_prediction'): rc_list.append(r)
        elif r['id'] in root_cause_ids: rc_list.append(r)
        elif r.get('prob', 0) > 0.5: rc_list.append(r)
        elif r['id'] != 'SYSTEM': ds_list.append(r)
            
    if not rc_list and not alarms: rc_list = [{"id": "SYSTEM", "label": "æ­£å¸¸ç¨¼åƒ", "prob": 0.0}]

    sel_cand = st.session_state.get("selected_candidate")
    
    # === Main Layout ===
    col_l, col_r = st.columns([1.1, 1.2]) # å³å´ã‚’å°‘ã—åºƒãï¼ˆãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ç”¨ï¼‰
    
    # Left: Visualization
    with col_l:
        st.subheader("ğŸŒ Network Topology")
        st.graphviz_chart(render_topology_graph(topology, alarms, results), use_container_width=True)
        
        st.markdown("#### ğŸ¯ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå€™è£œ")
        df = pd.DataFrame([{
            "Type": "ğŸ”®" if x.get('is_prediction') else "ğŸ”´" if x.get('prob')>=0.9 else "ğŸŸ¡",
            "Device": x['id'],
            "Cause": x.get('label'),
            "Conf": f"{x.get('prob',0)*100:.0f}%",
            "_obj": x
        } for x in rc_list])
        
        event = st.dataframe(
            df.drop(columns=["_obj"]), 
            use_container_width=True, 
            hide_index=True, 
            selection_mode="single-row", 
            on_select="rerun"
        )
        
        if event.selection.rows:
            sel_cand = df.iloc[event.selection.rows[0]]["_obj"]
            st.session_state.selected_candidate = sel_cand

        if ds_list:
            with st.expander(f"â–¼ å½±éŸ¿ãƒ‡ãƒã‚¤ã‚¹ ({len(ds_list)}å°)", expanded=False):
                st.dataframe(pd.DataFrame([{"Device": d['id'], "Status": "Unreachable"} for d in ds_list]), use_container_width=True, hide_index=True)

    # Right: Incident Workspace (One-Stop Operation)
    with col_r:
        if sel_cand:
            # 1. Header Card
            bg = "#F3E5F5" if sel_cand.get('is_prediction') else "#FFEBEE"
            bd = "#9C27B0" if sel_cand.get('is_prediction') else "#D32F2F"
            title_icon = "ğŸ”®" if sel_cand.get('is_prediction') else "ğŸš¨"
            
            st.markdown(f"""
            <div style="background-color: {bg}; border-left: 6px solid {bd}; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <h3 style="margin:0; color:#333;">{title_icon} {sel_cand['id']}</h3>
                <p style="margin:5px 0 0 0; font-weight:bold; color:#555;">{sel_cand.get('label')}</p>
                <p style="font-size:0.9em; color:#666;">ä¿¡é ¼åº¦: {sel_cand.get('prob',0)*100:.0f}%</p>
            </div>
            """, unsafe_allow_html=True)
            
            # 2. æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (Primary Actions) - AIã«èãå‰ã«è¡¨ç¤º
            rec_actions = sel_cand.get("recommended_actions", [])
            if rec_actions and sel_cand.get('is_prediction'):
                st.markdown("#### âš¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (Primary Actions)")
                for act in rec_actions:
                    with st.container():
                        st.markdown(f"**ğŸ‘‰ {act['title']}**")
                        st.caption(f"åŠ¹æœ: {act['effect']}")
                st.divider()

            # 3. Workflow Tabs (Action Center)
            tab_act, tab_chat, tab_rpt = st.tabs(["ğŸ› ï¸ å¯¾å¿œã‚¢ã‚¯ã‚·ãƒ§ãƒ³", "ğŸ’¬ AIãƒãƒ£ãƒƒãƒˆ", "ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆ"])
            
            with tab_act:
                # Step 1: Analyze
                with st.expander("ğŸ” Step 1: åŸå› ãƒ»å½±éŸ¿ã®ç¢ºèª", expanded=True):
                    if sel_cand.get('reason'):
                        st.info(sel_cand.get('reason'))
                    
                    st.checkbox("ãƒ­ã‚°ã‚’ç¢ºèªã—ãŸ", key=f"chk_log_{sel_cand['id']}")
                    st.checkbox("å½±éŸ¿ç¯„å›²ã‚’ç¢ºèªã—ãŸ", key=f"chk_imp_{sel_cand['id']}")

                # Step 2: Remediate
                with st.expander("ğŸ› ï¸ Step 2: ä¿®å¾©å¯¾å¿œ", expanded=True):
                    if st.session_state.remediation_plan is None:
                        if st.button("âœ¨ æ‰‹é †æ›¸ã‚’ç”Ÿæˆ (AI)", use_container_width=True):
                            with st.spinner("Generating Plan..."):
                                time.sleep(1) # Mock
                                st.session_state.remediation_plan = """
                                **AIæ¨å¥¨æ‰‹é †:**
                                1. `show interface status` ã§ç‰©ç†å±¤ã‚’ç¢ºèª
                                2. ãƒãƒ¼ãƒˆãƒªã‚»ãƒƒãƒˆ (`shutdown` -> `no shutdown`)
                                3. ç–é€šç¢ºèª (`ping`)
                                """
                                st.rerun()
                    
                    if st.session_state.remediation_plan:
                        st.markdown(st.session_state.remediation_plan)
                        col_run, col_clr = st.columns([2, 1])
                        if col_run.button("ğŸš€ Playbookå®Ÿè¡Œ", type="primary", use_container_width=True):
                            st.toast("Playbookã‚’å®Ÿè¡Œä¸­...")
                            time.sleep(1)
                            st.session_state.recovered_devices[sel_cand['id']] = True
                            st.success("å®Ÿè¡Œå®Œäº†: æ­£å¸¸æ€§ã‚’ç¢ºèªã—ã¾ã—ãŸ")
                            st.balloons()
                        if col_clr.button("ã‚¯ãƒªã‚¢", use_container_width=True):
                            st.session_state.remediation_plan = None
                            st.rerun()

                # Step 3: Resolve (One-Click Feedback)
                st.markdown("#### âœ… Step 3: çµæœç™»éŒ² (Feedback)")
                c_ok, c_fp, c_mute = st.columns(3)
                if c_ok.button("è§£æ±º (Resolved)", type="primary", use_container_width=True):
                    st.toast(f"ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ {sel_cand['id']} ã‚’è§£æ±ºæ¸ˆã¨ã—ã¦è¨˜éŒ²ã—ã¾ã—ãŸã€‚")
                if c_fp.button("èª¤æ¤œçŸ¥ (FP)", use_container_width=True):
                    st.toast("èª¤æ¤œçŸ¥ã¨ã—ã¦å ±å‘Šã—ã¾ã—ãŸã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åæ˜ ã•ã‚Œã¾ã™ã€‚")
                if c_mute.button("é™è¦³ (Mute)", use_container_width=True):
                    st.toast("ã“ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’24æ™‚é–“ãƒŸãƒ¥ãƒ¼ãƒˆã—ã¾ã™ã€‚")

            with tab_chat:
                if not st.session_state.chat_session and api_key and GENAI_AVAILABLE:
                    genai.configure(api_key=api_key)
                    st.session_state.chat_session = genai.GenerativeModel("gemma-3-12b-it").start_chat(history=[])
                
                chat_cont = st.container(height=300)
                with chat_cont:
                    for msg in st.session_state.messages[-10:]:
                        icon = "ğŸ¤–" if msg["role"] == "assistant" else "ğŸ‘¤"
                        st.markdown(f"**{icon}** {msg['content']}")

                prompt = st.chat_input("AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«è³ªå•...")
                if prompt:
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    if st.session_state.chat_session:
                        ci = _build_ci_context_for_chat(topology, sel_cand['id'])
                        full_p = f"Context: {json.dumps(ci)}\nQuestion: {prompt}"
                        with st.spinner("Thinking..."):
                            resp = generate_content_with_retry(st.session_state.chat_session.model, full_p, stream=False)
                            if resp:
                                st.session_state.messages.append({"role": "assistant", "content": resp.text})
                    st.rerun()

            with tab_rpt:
                if st.button("ğŸ“ å ±å‘Šæ›¸ä½œæˆ (PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼)", use_container_width=True):
                    st.success("ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼ˆãƒ¢ãƒƒã‚¯ï¼‰")
                    st.markdown("""
                    **éšœå®³å ±å‘Šæ›¸ãƒ‰ãƒ©ãƒ•ãƒˆ**
                    * ç™ºç”Ÿæ™‚åˆ»: 2026-02-17 10:00
                    * å¯¾è±¡: WAN_ROUTER_01
                    * åŸå› : ãƒã‚¤ã‚¯ãƒ­ãƒãƒ¼ã‚¹ãƒˆã«ã‚ˆã‚‹ãƒ‘ã‚±ãƒƒãƒˆãƒ‰ãƒ­ãƒƒãƒ—
                    * å¯¾å¿œ: QoSãƒãƒªã‚·ãƒ¼èª¿æ•´ã«ã‚ˆã‚Šè§£æ¶ˆ
                    """)

        else:
            # å¾…æ©Ÿç”»é¢
            st.info("ğŸ‘ˆ å·¦å´ã®ãƒªã‚¹ãƒˆã‹ã‚‰ã€å¯¾å¿œã™ã‚‹ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã¾ãŸã¯äºˆå…†ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            st.markdown("""
            **ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚¬ã‚¤ãƒ‰:**
            1. **ğŸ”® äºˆå…†** ã¯å„ªå…ˆçš„ã«ç¢ºèªã—ã€äºˆé˜²æªç½®ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚
            2. **ğŸ”´ éšœå®³** ã¯å½±éŸ¿ç¯„å›²ã‚’ç¢ºèªã—ã€ç›´ã¡ã«è‡ªå‹•ä¿®å¾©ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
            3. **ğŸŸ¡ è­¦å‘Š** ã¯é™è¦³ã¾ãŸã¯ãƒã‚±ãƒƒãƒˆèµ·ç¥¨ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
            """)
