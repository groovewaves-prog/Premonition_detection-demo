import streamlit as st
import pandas as pd
import json
import time
import hashlib
from typing import Optional, List, Dict, Any

try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False

from registry import get_paths, load_topology, get_display_name
from alarm_generator import generate_alarms_for_scenario, Alarm, get_alarm_summary
from inference_engine import LogicalRCA
from network_ops import (
    generate_analyst_report_streaming,
    generate_remediation_commands_streaming,
    run_remediation_parallel_v2,
    RemediationEnvironment
)
from utils.helpers import get_status_from_alarms, get_status_icon, load_config_by_id
from utils.llm_helper import get_rate_limiter, generate_content_with_retry
from verifier import verify_log_content
from .graph import render_topology_graph

# =====================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =====================================================
def _hash_text(text: str) -> str:
    return hashlib.sha256((text or "").encode("utf-8")).hexdigest()[:16]


def _pick_first(mapping: dict, keys: list, default: str = "") -> str:
    for k in keys:
        try:
            v = mapping.get(k)
            if v:
                return str(v)
        except:
            pass
    return default


def _build_ci_context_for_chat(topology: dict, target_node_id: str) -> dict:
    node = topology.get(target_node_id)
    if node and hasattr(node, 'metadata'):
        md = node.metadata or {}
    elif isinstance(node, dict):
        md = node.get('metadata', {})
    else:
        md = {}
    ci = {
        "device_id": target_node_id or "",
        "hostname": _pick_first(md, ["hostname", "host", "name"], default=(target_node_id or "")),
        "vendor": _pick_first(md, ["vendor", "manufacturer", "maker", "brand"], default=""),
        "os": _pick_first(md, ["os", "platform", "os_name"], default=""),
        "model": _pick_first(md, ["model", "hw_model", "product"], default=""),
        "role": _pick_first(md, ["role", "type", "device_role"], default=""),
    }
    try:
        conf = load_config_by_id(target_node_id) if target_node_id else ""
        if conf:
            ci["config_excerpt"] = conf[:1500]
    except Exception:
        pass
    return ci


def run_diagnostic_simulation_no_llm(scenario: str, target_node_obj) -> dict:
    """LLMã‚’å‘¼ã°ãªã„ç–‘ä¼¼è¨ºæ–­"""
    device_id = getattr(target_node_obj, "id", "UNKNOWN") if target_node_obj else "UNKNOWN"
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    lines = [
        f"[PROBE] ts={ts}",
        f"[PROBE] scenario={scenario}",
        f"[PROBE] target_device={device_id}",
        "",
    ]

    recovered_devices = st.session_state.get("recovered_devices") or {}
    recovered_map = st.session_state.get("recovered_scenario_map") or {}

    if recovered_devices.get(device_id) and recovered_map.get(device_id) == scenario:
        if "FW" in scenario:
            lines += ["show chassis cluster status", "Redundancy group 0: healthy", "control link: up", "fabric link: up"]
        elif "WAN" in scenario:
            lines += ["show ip interface brief", "GigabitEthernet0/0 up up", "Neighbor 203.0.113.2 Established",
                      "ping 203.0.113.2 repeat 5", "Success rate is 100 percent (5/5)"]
        elif "L2SW" in scenario:
            lines += ["show environment", "Fan: OK", "Temperature: OK", "show interface status", "Uplink: up"]
        else:
            lines += ["show system alarms", "No active alarms", "ping 8.8.8.8 repeat 5", "Success rate is 100 percent (5/5)"]
        return {"status": "SUCCESS", "sanitized_log": "\n".join(lines), "device_id": device_id}

    if "WANå…¨å›ç·šæ–­" in scenario or "[WAN]" in scenario:
        lines += ["show ip interface brief", "GigabitEthernet0/0 down down", "show ip bgp summary",
                  "Neighbor 203.0.113.2 Idle", "ping 203.0.113.2 repeat 5", "Success rate is 0 percent (0/5)"]
    elif "FWç‰‡ç³»éšœå®³" in scenario or "[FW]" in scenario:
        lines += ["show chassis cluster status", "Redundancy group 0: degraded", "control link: down", "fabric link: up"]
    elif "L2SW" in scenario:
        lines += ["show environment", "Fan: FAIL", "Temperature: HIGH", "show interface status", "Uplink: flapping"]
    else:
        lines += ["show system alarms", "No active alarms"]

    return {"status": "SUCCESS", "sanitized_log": "\n".join(lines), "device_id": device_id}


# =====================================================
# ãƒ¡ã‚¤ãƒ³æç”»é–¢æ•°
# =====================================================
def render_incident_cockpit(site_id: str, api_key: Optional[str]):
    display_name = get_display_name(site_id)
    scenario = st.session_state.site_scenarios.get(site_id, "æ­£å¸¸ç¨¼åƒ")

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    col_header = st.columns([4, 1])
    with col_header[0]:
        st.markdown(f"### ğŸ›¡ï¸ AIOps ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ»ã‚³ãƒƒã‚¯ãƒ”ãƒƒãƒˆ")
    with col_header[1]:
        if st.button("ğŸ”™ ä¸€è¦§ã«æˆ»ã‚‹", key="back_to_list"):
            st.session_state.active_site = None
            st.rerun()

    # ãƒˆãƒãƒ­ã‚¸ãƒ¼èª­ã¿è¾¼ã¿
    paths = get_paths(site_id)
    topology = load_topology(paths.topology_path)

    if not topology:
        st.error("ãƒˆãƒãƒ­ã‚¸ãƒ¼ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # ã‚¢ãƒ©ãƒ¼ãƒ ç”Ÿæˆ
    alarms = generate_alarms_for_scenario(topology, scenario)
    status = get_status_from_alarms(scenario, alarms)

    # äºˆå…†ã‚·ã‚°ãƒŠãƒ«æ³¨å…¥
    injected = st.session_state.get("injected_weak_signal")
    if injected and injected["device_id"] in topology:
        messages = injected.get("messages", [injected.get("message", "")])
        for msg in messages:
            if msg:
                alarms.append(Alarm(
                    device_id=injected["device_id"],
                    message=msg,
                    severity="INFO",
                    is_root_cause=False
                ))

    # LogicalRCA ã‚¨ãƒ³ã‚¸ãƒ³
    engine_key = f"engine_{site_id}"
    if engine_key not in st.session_state.logic_engines:
        st.session_state.logic_engines[engine_key] = LogicalRCA(topology)
    engine = st.session_state.logic_engines[engine_key]

    if alarms:
        analysis_results = engine.analyze(alarms)
    else:
        analysis_results = [{
            "id": "SYSTEM",
            "label": "æ­£å¸¸ç¨¼åƒ",
            "prob": 0.0,
            "type": "Normal",
            "tier": 3,
            "reason": "ã‚¢ãƒ©ãƒ¼ãƒ ãªã—"
        }]

    # =====================================================
    # KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹
    # =====================================================
    root_cause_alarms = [a for a in alarms if a.is_root_cause]
    total_alarms = len(alarms)
    noise_reduction = ((total_alarms - len(root_cause_alarms)) / total_alarms * 100) if total_alarms > 0 else 0.0
    action_required = len(set(a.device_id for a in root_cause_alarms))
    prediction_results = [r for r in analysis_results if r.get('is_prediction')]
    prediction_count = len(prediction_results)

    st.markdown("---")
    cols = st.columns(3)
    cols[0].metric("ğŸš¨ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", f"{get_status_icon(status)} {status}")
    cols[1].metric("ğŸ“Š ã‚¢ãƒ©ãƒ¼ãƒ æ•°", f"{total_alarms}ä»¶")
    suspect_count = len([r for r in analysis_results if r.get('prob', 0) > 0.5])
    if prediction_count > 0:
        cols[2].metric("ğŸ¯ è¢«ç–‘ç®‡æ‰€", f"{suspect_count}ä»¶",
                       delta=f"ã†ã¡ğŸ”®äºˆå…† {prediction_count}ä»¶", delta_color="off")
    else:
        cols[2].metric("ğŸ¯ è¢«ç–‘ç®‡æ‰€", f"{suspect_count}ä»¶")

    kpi_cols = st.columns(3)
    with kpi_cols[0]:
        delta_text = "â†‘ é«˜åŠ¹ç‡ç¨¼åƒä¸­" if noise_reduction > 90 else ("â†’ é€šå¸¸ç¨¼åƒ" if noise_reduction > 50 else "â†“ è¦ç¢ºèª")
        delta_color = "normal" if noise_reduction > 90 else ("off" if noise_reduction > 50 else "inverse")
        kpi_cols[0].metric("ğŸ“‰ ãƒã‚¤ã‚ºå‰Šæ¸›ç‡", f"{noise_reduction:.1f}%", delta=delta_text, delta_color=delta_color)
    with kpi_cols[1]:
        kpi_cols[1].metric("ğŸ”® äºˆå…†æ¤œçŸ¥", f"{prediction_count}ä»¶",
                           delta="âš¡ è¦æ³¨æ„" if prediction_count > 0 else "å•é¡Œãªã—",
                           delta_color="inverse" if prediction_count > 0 else "normal")
    with kpi_cols[2]:
        kpi_cols[2].metric("ğŸš¨ è¦å¯¾å¿œã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ", f"{action_required}ä»¶",
                           delta="â†‘ å¯¾å‡¦ãŒå¿…è¦" if action_required > 0 else "å•é¡Œãªã—",
                           delta_color="inverse" if action_required > 0 else "normal")

    st.markdown("---")

    # =====================================================
    # æ ¹æœ¬åŸå› å€™è£œã¨ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®åˆ†é›¢
    # =====================================================
    root_cause_device_ids = set(a.device_id for a in alarms if a.is_root_cause)
    downstream_device_ids = set(a.device_id for a in alarms if not a.is_root_cause)

    root_cause_candidates = []
    downstream_devices = []

    for cand in analysis_results:
        device_id = cand.get('id', '')
        if cand.get('is_prediction'):
            root_cause_candidates.append(cand)
        elif device_id in root_cause_device_ids:
            root_cause_candidates.append(cand)
        elif device_id in downstream_device_ids:
            downstream_devices.append(cand)
        elif cand.get('prob', 0) > 0.5:
            root_cause_candidates.append(cand)

    if not root_cause_candidates and not alarms:
        root_cause_candidates = [{
            "id": "SYSTEM", "label": "æ­£å¸¸ç¨¼åƒ", "prob": 0.0,
            "type": "Normal", "tier": 3, "reason": "ã‚¢ãƒ©ãƒ¼ãƒ ãªã—"
        }]

    if root_cause_candidates and downstream_devices:
        st.info(f"ğŸ“ **æ ¹æœ¬åŸå› **: {root_cause_candidates[0]['id']} â†’ å½±éŸ¿ç¯„å›²: é…ä¸‹ {len(downstream_devices)} æ©Ÿå™¨")

    # =====================================================
    # ğŸ”® AIOps Future Radarï¼ˆäºˆå…†å°‚ç”¨è¡¨ç¤ºã‚¨ãƒªã‚¢ï¼‰
    # =====================================================
    prediction_candidates = [c for c in root_cause_candidates if c.get('is_prediction')]

    if prediction_candidates:
        st.markdown("### ğŸ”® AIOps Future Radar")
        with st.container(border=True):
            injected_info = st.session_state.get("injected_weak_signal")
            if injected_info:
                level = injected_info.get("level", 0)
                scenario_name = injected_info.get("scenario", "")
                st.info(
                    f"âš ï¸ **äºˆå…†æ¤œçŸ¥**: ç¾åœ¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çŠ¶æ…‹ã¯ã€Œæ­£å¸¸ã€ã§ã™ãŒã€"
                    f"AIãŒå¾®ç´°ãªã‚·ã‚°ãƒŠãƒ«ã‹ã‚‰å°†æ¥ã®éšœå®³ãƒªã‚¹ã‚¯ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚"
                    f"ï¼ˆåŠ£åŒ–ã‚·ãƒŠãƒªã‚ª: {scenario_name} / ãƒ¬ãƒ™ãƒ«: {level}/5ï¼‰"
                )
            else:
                st.info("âš ï¸ **äºˆå…†æ¤œçŸ¥**: AIãŒå°†æ¥ã®éšœå®³ãƒªã‚¹ã‚¯ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")

            radar_cols = st.columns(min(len(prediction_candidates), 3))
            for idx, pred_item in enumerate(prediction_candidates[:3]):
                with radar_cols[idx]:
                    prob_pct = f"{pred_item.get('prob', 0)*100:.0f}%"
                    pred_timeline = pred_item.get('prediction_timeline', 'ä¸æ˜')
                    pred_affected = pred_item.get('prediction_affected_count', 0)
                    pred_label = pred_item.get('label', '').replace('ğŸ”® [äºˆå…†] ', '')
                    pred_early_hours = pred_item.get('prediction_early_warning_hours', 0)

                    st.error(f"**ğŸ“ {pred_item['id']}**")
                    st.markdown(
                        f"<div style='text-align:center;'>"
                        f"<span style='font-size:36px; font-weight:bold; color:#d32f2f;'>{prob_pct}</span>"
                        f"<br><span style='color:#666;'>ç™ºç”Ÿç¢ºç‡ï¼ˆæ€¥æ€§æœŸ: {pred_timeline}ï¼‰</span>"
                        f"</div>", unsafe_allow_html=True
                    )
                    st.divider()
                    st.markdown(f"**äºˆæ¸¬éšœå®³:** {pred_label}")
                    if pred_early_hours >= 24:
                        early_display = f"æœ€å¤§ **{pred_early_hours // 24}æ—¥å‰** ã‹ã‚‰æ¤œçŸ¥å¯èƒ½"
                    elif pred_early_hours > 0:
                        early_display = f"æœ€å¤§ **{pred_early_hours}æ™‚é–“å‰** ã‹ã‚‰æ¤œçŸ¥å¯èƒ½"
                    else:
                        early_display = "ä¸æ˜"
                    st.markdown(f"**æ—©æœŸäºˆå…†:** {early_display}")
                    st.markdown(f"**æ€¥æ€§æœŸ:** ç™ºç—‡å¾Œ **{pred_timeline}** ã«æ·±åˆ»åŒ–")
                    st.markdown(f"**å½±éŸ¿ç¯„å›²:** é…ä¸‹ **{pred_affected}å°** ãŒé€šä¿¡æ–­ã®æã‚Œ")

                    with st.expander("ğŸ” æ¤œçŸ¥ã•ã‚ŒãŸäºˆå…† (Weak Signal)"):
                        st.text(pred_item.get('reason', ''))
                        factors = pred_item.get('prediction_confidence_factors', {})
                        if factors:
                            st.caption(
                                f"ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦: {factors.get('base', 0):.2f} / "
                                f"ãƒãƒƒãƒå“è³ª: {factors.get('match_quality', 0):.2f} / "
                                f"SPOF: {'Yes' if factors.get('is_spof') else 'No'} / "
                                f"å†—é•·æ€§: {'Yes' if factors.get('has_redundancy') else 'No'}"
                            )
        st.markdown("---")

    # =====================================================
    # ğŸ¯ æ ¹æœ¬åŸå› å€™è£œãƒ†ãƒ¼ãƒ–ãƒ«
    # â˜…â˜…â˜… ä¿®æ­£â‘ : alarm_info_mapã‚’ä½¿ã£ãŸseverityåŸºæº–ã®åˆ¤å®šã«æˆ»ã™ â˜…â˜…â˜…
    # =====================================================
    selected_incident_candidate = None
    target_device_id = None

    if root_cause_candidates:
        # ã‚¢ãƒ©ãƒ¼ãƒ ã®severityã¨silentãƒ•ãƒ©ã‚°ã‚’ãƒ‡ãƒã‚¤ã‚¹IDã§ãƒãƒƒãƒ”ãƒ³ã‚°
        alarm_info_map = {}
        for a in alarms:
            if a.device_id not in alarm_info_map:
                alarm_info_map[a.device_id] = {'severity': 'INFO', 'is_silent': False}
            if a.severity == 'CRITICAL':
                alarm_info_map[a.device_id]['severity'] = 'CRITICAL'
            elif a.severity == 'WARNING' and alarm_info_map[a.device_id]['severity'] != 'CRITICAL':
                alarm_info_map[a.device_id]['severity'] = 'WARNING'
            if hasattr(a, 'is_silent_suspect') and a.is_silent_suspect:
                alarm_info_map[a.device_id]['is_silent'] = True

        df_data = []
        for rank, cand in enumerate(root_cause_candidates, 1):
            prob = cand.get('prob', 0)
            cand_type = cand.get('type', 'UNKNOWN')
            device_id = cand['id']
            alarm_info = alarm_info_map.get(device_id, {'severity': 'INFO', 'is_silent': False})

            # â˜… æ—§app.pyã¨åŒã˜åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆseverityåŸºæº–ï¼‰
            if cand.get('is_prediction'):
                status_text = "ğŸ”® äºˆå…†æ¤œçŸ¥"
                timeline = cand.get('prediction_timeline', '')
                affected = cand.get('prediction_affected_count', 0)
                early_hours = cand.get('prediction_early_warning_hours', 0)
                early_str = (f"(äºˆå…†: {early_hours // 24}æ—¥å‰ã€œ)" if early_hours >= 24
                             else (f"(äºˆå…†: {early_hours}æ™‚é–“å‰ã€œ)" if early_hours > 0 else ""))
                if timeline and affected:
                    action = f"âš¡ æ€¥æ€§æœŸ{timeline}ä»¥å†… {early_str} ({affected}å°å½±éŸ¿)"
                else:
                    action = f"âš¡ äºˆé˜²çš„å¯¾å‡¦ã‚’æ¨å¥¨ {early_str}"
            elif alarm_info['is_silent'] or "Silent" in cand_type:
                status_text = "ğŸŸ£ ã‚µã‚¤ãƒ¬ãƒ³ãƒˆç–‘ã„"
                action = "ğŸ” ä¸Šä½ç¢ºèª"
            elif alarm_info['severity'] == 'CRITICAL':
                # â˜… ã“ã“ãŒä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ: probé–¾å€¤ã§ã¯ãªãCRITICAL severity ã§åˆ¤å®š
                status_text = "ğŸ”´ å±é™º (æ ¹æœ¬åŸå› )"
                action = "ğŸš€ è‡ªå‹•ä¿®å¾©ãŒå¯èƒ½"
            elif alarm_info['severity'] == 'WARNING':
                status_text = "ğŸŸ¡ è­¦å‘Š"
                action = "ğŸ” è©³ç´°èª¿æŸ»"
            elif prob > 0.6:
                status_text = "ğŸŸ¡ è¢«ç–‘ç®‡æ‰€"
                action = "ğŸ” è©³ç´°èª¿æŸ»"
            else:
                status_text = "âšª ç›£è¦–ä¸­"
                action = "ğŸ‘ï¸ é™è¦³"

            df_data.append({
                "é †ä½": rank,
                "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": status_text,
                "ãƒ‡ãƒã‚¤ã‚¹": device_id,
                "åŸå› ": cand.get('label', ''),
                "ç¢ºä¿¡åº¦": f"{prob*100:.0f}%",
                "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³": action,
                "_id": device_id,
                "_prob": prob
            })

        df = pd.DataFrame(df_data)

        st.markdown("#### ğŸ¯ æ ¹æœ¬åŸå› å€™è£œ")
        event = st.dataframe(
            df[["é †ä½", "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", "ãƒ‡ãƒã‚¤ã‚¹", "åŸå› ", "ç¢ºä¿¡åº¦", "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"]],
            use_container_width=True,
            hide_index=True,
            selection_mode="single-row",
            on_select="rerun"
        )

        if event.selection and len(event.selection.rows) > 0:
            sel_row = df.iloc[event.selection.rows[0]]
            for cand in root_cause_candidates:
                if cand['id'] == sel_row['_id']:
                    selected_incident_candidate = cand
                    target_device_id = cand['id']
                    break
        elif root_cause_candidates:
            selected_incident_candidate = root_cause_candidates[0]
            target_device_id = root_cause_candidates[0]['id']

        # å½±éŸ¿ãƒ‡ãƒã‚¤ã‚¹ï¼ˆä¸‹æµï¼‰ä¸€è¦§
        if downstream_devices:
            with st.expander(f"â–¼ å½±éŸ¿ã‚’å—ã‘ã¦ã„ã‚‹æ©Ÿå™¨ ({len(downstream_devices)}å°) - ä¸Šæµå¾©æ—§å¾…ã¡", expanded=False):
                dd_df = pd.DataFrame([
                    {"No": i+1, "ãƒ‡ãƒã‚¤ã‚¹": d['id'], "çŠ¶æ…‹": "âš« å¿œç­”ãªã—", "å‚™è€ƒ": "ä¸Šæµå¾©æ—§å¾…ã¡"}
                    for i, d in enumerate(downstream_devices)
                ])
                if len(downstream_devices) >= 10:
                    with st.container(height=300):
                        st.dataframe(dd_df, use_container_width=True, hide_index=True)
                else:
                    st.dataframe(dd_df, use_container_width=True, hide_index=True)

    # =====================================================
    # 2ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    # =====================================================
    col_map, col_chat = st.columns([1.2, 1])

    # === å·¦ã‚«ãƒ©ãƒ : ãƒˆãƒãƒ­ã‚¸ãƒ¼ & Auto-Diagnostics ===
    with col_map:
        st.subheader("ğŸŒ Network Topology")
        graph = render_topology_graph(topology, alarms, analysis_results)
        st.graphviz_chart(graph, use_container_width=True)

        st.markdown("---")
        st.subheader("ğŸ› ï¸ Auto-Diagnostics")

        if st.button("ğŸš€ è¨ºæ–­å®Ÿè¡Œ (Run Diagnostics)", type="primary"):
            if not api_key:
                st.error("API Key Required")
            else:
                with st.status("Agent Operating...", expanded=True) as status_widget:
                    st.write("ğŸ”Œ Connecting to device...")
                    target_node_obj = topology.get(target_device_id) if target_device_id else None
                    res = run_diagnostic_simulation_no_llm(scenario, target_node_obj)
                    st.session_state.live_result = res
                    if res["status"] == "SUCCESS":
                        st.write("âœ… Log Acquired & Sanitized.")
                        status_widget.update(label="Diagnostics Complete!", state="complete", expanded=False)
                        log_content = res.get('sanitized_log', "")
                        st.session_state.verification_result = verify_log_content(log_content)
                        st.session_state.trigger_analysis = True
                    else:
                        st.write("âŒ Connection Failed.")
                        status_widget.update(label="Diagnostics Failed", state="error")
                st.rerun()

        if st.session_state.live_result:
            res = st.session_state.live_result
            if res["status"] == "SUCCESS":
                st.markdown("#### ğŸ“„ Diagnostic Results")
                with st.container(border=True):
                    if st.session_state.verification_result:
                        v = st.session_state.verification_result
                        c1, c2, c3 = st.columns(3)
                        c1.metric("Ping Status", v.get('ping_status'))
                        c2.metric("Interface", v.get('interface_status'))
                        c3.metric("Hardware", v.get('hardware_status'))
                    st.divider()
                    st.caption("ğŸ”’ Raw Logs (Sanitized)")
                    st.code(res["sanitized_log"], language="text")

    # =====================================================
    # === å³ã‚«ãƒ©ãƒ : AI Analyst Report & Remediation & Chat ===
    # =====================================================
    with col_chat:
        # ============================================
        # A. AI Analyst Report
        # ============================================
        st.subheader("ğŸ“ AI Analyst Report")

        if selected_incident_candidate:
            cand = selected_incident_candidate

            if st.session_state.generated_report is None:
                st.info(f"ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé¸æŠä¸­: **{cand['id']}** ({cand.get('label', '')})")

                if api_key and (scenario != "æ­£å¸¸ç¨¼åƒ" or cand.get('is_prediction')):
                    is_pred = cand.get('is_prediction')
                    btn_label = ("ğŸ”® äºˆå…†ã®ç¢ºèªæ‰‹é †ã‚’ç”Ÿæˆ (Predictive Analysis)"
                                 if is_pred else "ğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ (Generate Report)")

                    if st.button(btn_label):
                        report_container = st.empty()
                        target_conf = load_config_by_id(cand['id'])
                        verification_context = cand.get("verification_log", "ç‰¹ã«ãªã—")

                        t_node = topology.get(cand["id"])
                        t_node_dict = {
                            "id": getattr(t_node, "id", None) if t_node else None,
                            "type": getattr(t_node, "type", None) if t_node else None,
                            "layer": getattr(t_node, "layer", None) if t_node else None,
                            "metadata": (getattr(t_node, "metadata", {}) or {}) if t_node else {},
                        }
                        parent_id = t_node.parent_id if t_node and hasattr(t_node, 'parent_id') else None
                        children_ids = [
                            nid for nid, n in topology.items()
                            if (getattr(n, "parent_id", None) if hasattr(n, 'parent_id')
                                else n.get('parent_id')) == cand["id"]
                        ]
                        topology_context = {
                            "node": t_node_dict,
                            "parent_id": parent_id,
                            "children_ids": children_ids
                        }

                        cache_key_analyst = "|".join([
                            "analyst", site_id, scenario,
                            str(cand.get("id")),
                            _hash_text(verification_context),
                        ])

                        if cache_key_analyst in st.session_state.report_cache:
                            full_text = st.session_state.report_cache[cache_key_analyst]
                            report_container.markdown(full_text)
                        else:
                            try:
                                report_container.write("ğŸ¤– AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
                                placeholder = report_container.empty()
                                full_text = ""

                                for chunk in generate_analyst_report_streaming(
                                    scenario=scenario,
                                    target_node=t_node,
                                    analysis_result={"node": t_node_dict, "topology": topology_context},
                                    target_conf=target_conf,
                                    verification_context=verification_context,
                                    api_key=api_key,
                                    max_retries=2,
                                    backoff=3
                                ):
                                    full_text += chunk
                                    placeholder.markdown(full_text)

                                if not full_text or full_text.startswith("Error"):
                                    full_text = f"âš ï¸ åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {full_text}"
                                    placeholder.markdown(full_text)

                                st.session_state.report_cache[cache_key_analyst] = full_text
                            except Exception as e:
                                full_text = f"âš ï¸ åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {type(e).__name__}: {e}"
                                report_container.markdown(full_text)

                        st.session_state.generated_report = full_text
            else:
                # ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤ºï¼ˆheight=400ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚³ãƒ³ãƒ†ãƒŠï¼‰
                with st.container(height=400, border=True):
                    st.markdown(st.session_state.generated_report)
                if st.button("ğŸ”„ ãƒ¬ãƒãƒ¼ãƒˆå†ä½œæˆ"):
                    st.session_state.generated_report = None
                    st.rerun()

        # ============================================
        # â˜…â˜…â˜… B. å¾©æ—§æ‰‹é † + ä¿®å¾©å®Ÿè¡Œ(Execute)ãƒœã‚¿ãƒ³
        #         ã€ŒAI Analyst Reportã€ã¨ã€ŒRemediation & Chatã€ã®é–“ã«é…ç½®
        # ============================================
        if selected_incident_candidate and selected_incident_candidate["prob"] > 0.6:
            is_pred_rem = selected_incident_candidate.get('is_prediction')

            if st.session_state.remediation_plan is None:
                # Generate Fix ãƒœã‚¿ãƒ³ï¼ˆãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ç›´ä¸‹ã€Remediation & Chat ã®å‰ï¼‰
                fix_label = ("ğŸ”® äºˆé˜²æªç½®ãƒ—ãƒ©ãƒ³ã‚’ç”Ÿæˆ" if is_pred_rem
                             else "âœ¨ ä¿®å¾©ãƒ—ãƒ©ãƒ³ã‚’ä½œæˆ (Generate Fix)")
                report_prereq = ("ã€ŒğŸ”® äºˆå…†ã®ç¢ºèªæ‰‹é †ã‚’ç”Ÿæˆã€" if is_pred_rem
                                 else "ã€ŒğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ (Generate Report)ã€")

                if st.button(fix_label):
                    if st.session_state.generated_report is None:
                        st.warning(f"å…ˆã«{report_prereq}ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                    else:
                        remediation_container = st.empty()
                        t_node = topology.get(selected_incident_candidate["id"])

                        rem_scenario = scenario
                        if is_pred_rem:
                            pred_timeline = selected_incident_candidate.get('prediction_timeline', 'ä¸æ˜')
                            pred_affected = selected_incident_candidate.get('prediction_affected_count', 0)
                            pred_early_hours = selected_incident_candidate.get('prediction_early_warning_hours', 0)
                            pred_time_critical = selected_incident_candidate.get('prediction_time_to_critical_min', 0)
                            early_ctx = (f"æœ€å¤§{pred_early_hours // 24}æ—¥å‰ã‹ã‚‰æ¤œçŸ¥å¯èƒ½" if pred_early_hours >= 24
                                         else (f"æœ€å¤§{pred_early_hours}æ™‚é–“å‰ã‹ã‚‰æ¤œçŸ¥å¯èƒ½" if pred_early_hours > 0
                                               else "æ—©æœŸæ¤œçŸ¥ãƒ‘ã‚¿ãƒ¼ãƒ³"))
                            rem_scenario = (
                                f"[äºˆå…†å¯¾å¿œ - Predictive Maintenance] {selected_incident_candidate['id']}ã§éšœå®³ã®å‰å…†ã‚’æ¤œå‡ºã€‚\n"
                                f"ãƒ»æ—©æœŸäºˆå…†: {early_ctx}\n"
                                f"ãƒ»æ€¥æ€§æœŸ: ç™ºç—‡å¾Œ{pred_time_critical}åˆ†ã«æ·±åˆ»åŒ–ã®æã‚Œï¼ˆå½±éŸ¿{pred_affected}å°ï¼‰\n\n"
                                f"ã€Œå¾©æ—§ã€ã§ã¯ãªãã€Œäºˆé˜²æªç½®ã€ã¨ã—ã¦æ‰‹é †ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚"
                            )

                        cache_key_remediation = "|".join([
                            "remediation", site_id, scenario,
                            str(selected_incident_candidate.get("id")),
                            _hash_text(st.session_state.generated_report or ""),
                        ])

                        if cache_key_remediation in st.session_state.report_cache:
                            remediation_text = st.session_state.report_cache[cache_key_remediation]
                            remediation_container.markdown(remediation_text)
                        else:
                            try:
                                loading_msg = ("ğŸ”® äºˆé˜²æªç½®ãƒ—ãƒ©ãƒ³ç”Ÿæˆä¸­..." if is_pred_rem
                                               else "ğŸ¤– å¾©æ—§ãƒ—ãƒ©ãƒ³ç”Ÿæˆä¸­...")
                                remediation_container.write(loading_msg)
                                placeholder = remediation_container.empty()
                                remediation_text = ""

                                for chunk in generate_remediation_commands_streaming(
                                    scenario=rem_scenario,
                                    analysis_result=st.session_state.generated_report or "",
                                    target_node=t_node,
                                    api_key=api_key,
                                    max_retries=2,
                                    backoff=3
                                ):
                                    remediation_text += chunk
                                    placeholder.markdown(remediation_text)

                                if not remediation_text or remediation_text.startswith("Error"):
                                    remediation_text = f"âš ï¸ å¾©æ—§ãƒ—ãƒ©ãƒ³ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {remediation_text}"
                                    placeholder.markdown(remediation_text)

                                st.session_state.report_cache[cache_key_remediation] = remediation_text
                            except Exception as e:
                                remediation_text = f"âš ï¸ å¾©æ—§ãƒ—ãƒ©ãƒ³ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {type(e).__name__}: {e}"
                                remediation_container.markdown(remediation_text)

                        st.session_state.remediation_plan = remediation_text
                        st.rerun()

            if st.session_state.remediation_plan is not None:
                # å¾©æ—§æ‰‹é †ã‚³ãƒ³ãƒ†ãƒŠ
                with st.container(height=400, border=True):
                    st.info("AI Generated Recovery Procedureï¼ˆå¾©æ—§æ‰‹é †ï¼‰")
                    st.markdown(st.session_state.remediation_plan)

                # â˜… ä¿®å¾©å®Ÿè¡Œ(Execute) / ã‚­ãƒ£ãƒ³ã‚»ãƒ« ãƒœã‚¿ãƒ³
                #   â†’ AI Analyst Report ã¨ Remediation & Chat ã®ã€Œé–“ã€ã«è¡¨ç¤ºã•ã‚Œã‚‹
                col_exec1, col_exec2 = st.columns(2)
                with col_exec1:
                    exec_clicked = st.button("ğŸš€ ä¿®å¾©å®Ÿè¡Œ (Execute)", type="primary")
                with col_exec2:
                    cancel_clicked = st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«")

                if cancel_clicked:
                    st.session_state.remediation_plan = None
                    st.session_state.verification_log = None
                    st.rerun()

                if exec_clicked:
                    if not api_key:
                        st.error("API Key Required")
                    else:
                        with st.status("ğŸ”§ ä¿®å¾©å‡¦ç†å®Ÿè¡Œä¸­...", expanded=True) as status_widget:
                            target_node_obj = topology.get(selected_incident_candidate["id"])
                            device_info = (target_node_obj.metadata
                                           if target_node_obj and hasattr(target_node_obj, 'metadata')
                                           else {})

                            st.write("ğŸ”„ Executing remediation steps in parallel...")

                            results_rem = run_remediation_parallel_v2(
                                device_id=selected_incident_candidate["id"],
                                device_info=device_info,
                                scenario=scenario,
                                environment=RemediationEnvironment.DEMO,
                                timeout_per_step=30
                            )

                            st.write("ğŸ“‹ Remediation steps result:")
                            all_success = True
                            remediation_summary = []

                            for step_name in ["Backup", "Apply", "Verify"]:
                                result = results_rem.get(step_name)
                                if result:
                                    st.write(str(result))
                                    remediation_summary.append(str(result))
                                    if result.status != "success":
                                        all_success = False

                            verification_log = "\n".join(remediation_summary)
                            st.session_state.verification_log = verification_log

                            if all_success:
                                st.write("âœ… All remediation steps completed successfully.")
                                status_widget.update(label="Process Finished", state="complete", expanded=False)
                                st.session_state.recovered_devices[selected_incident_candidate["id"]] = True
                                st.session_state.recovered_scenario_map[selected_incident_candidate["id"]] = scenario
                                if not st.session_state.balloons_shown:
                                    st.balloons()
                                    st.session_state.balloons_shown = True
                                st.success("âœ… System Recovered Successfully!")
                            else:
                                st.write("âš ï¸ Some remediation steps failed. Please review.")
                                status_widget.update(label="Process Finished - With Errors", state="error", expanded=True)

                if st.session_state.get("verification_log"):
                    st.markdown("#### ğŸ” Post-Fix Verification Logs")
                    st.code(st.session_state.verification_log, language="text")

        # ============================================
        # C. Remediation & Chatï¼ˆAIã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º + Chatï¼‰
        # ============================================
        st.markdown("---")
        st.subheader("ğŸ¤– Remediation & Chat")

        if selected_incident_candidate and selected_incident_candidate["prob"] > 0.6:
            is_pred_rem = selected_incident_candidate.get('is_prediction')
            if is_pred_rem:
                timeline = selected_incident_candidate.get('prediction_timeline', 'ä¸æ˜')
                affected = selected_incident_candidate.get('prediction_affected_count', 0)
                early_hours = selected_incident_candidate.get('prediction_early_warning_hours', 0)
                early_display = (f"æœ€å¤§ <b>{early_hours // 24}æ—¥å‰</b> ã‹ã‚‰æ¤œçŸ¥å¯èƒ½" if early_hours >= 24
                                 else (f"æœ€å¤§ <b>{early_hours}æ™‚é–“å‰</b> ã‹ã‚‰æ¤œçŸ¥å¯èƒ½" if early_hours > 0
                                       else "ä¸æ˜"))
                st.markdown(f"""
                <div style="background-color:#fff3e0;padding:10px;border-radius:5px;border:1px solid #ff9800;color:#e65100;margin-bottom:10px;">
                    <strong>ğŸ”® Digital Twin æœªæ¥äºˆæ¸¬ (Predictive Maintenance)</strong><br>
                    <b>{selected_incident_candidate['id']}</b> ã§éšœå®³ã®å…†å€™ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚<br>
                    ãƒ»æ—©æœŸäºˆå…†: {early_display}<br>
                    ãƒ»æ€¥æ€§æœŸé€²è¡Œ: ç™ºç—‡å¾Œ <b>{timeline}</b> ã«æ·±åˆ»åŒ–ã®æã‚Œ<br>
                    ãƒ»å½±éŸ¿ç¯„å›²: <b>{affected}å°</b> ã®ãƒ‡ãƒã‚¤ã‚¹ã«å½±éŸ¿ã®å¯èƒ½æ€§<br>
                    ãƒ»æ¨å¥¨: ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã®äºˆé˜²äº¤æ›/å¯¾å¿œ<br>
                    (ä¿¡é ¼åº¦: <span style="font-size:1.2em;font-weight:bold;">{selected_incident_candidate['prob']*100:.0f}%</span>)
                </div>
                """, unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div style="background-color:#e8f5e9;padding:10px;border-radius:5px;border:1px solid #4caf50;color:#2e7d32;margin-bottom:10px;">
                    <strong>âœ… AI Analysis Completed</strong><br>
                    ç‰¹å®šã•ã‚ŒãŸåŸå›  <b>{selected_incident_candidate['id']}</b> ã«å¯¾ã™ã‚‹å¾©æ—§æ‰‹é †ãŒåˆ©ç”¨å¯èƒ½ã§ã™ã€‚<br>
                    (ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: <span style="font-size:1.2em;font-weight:bold;">{selected_incident_candidate['prob']*100:.0f}</span>)
                </div>
                """, unsafe_allow_html=True)

        else:
            if selected_incident_candidate:
                device_id = selected_incident_candidate.get('id', '')
                score = selected_incident_candidate['prob'] * 100
                if device_id == "SYSTEM" and score == 0:
                    st.markdown("""
                    <div style="background-color:#e8f5e9;padding:10px;border-radius:5px;border:1px solid #4caf50;color:#2e7d32;margin-bottom:10px;">
                        <strong>âœ… æ­£å¸¸ç¨¼åƒä¸­</strong><br>
                        ç¾åœ¨ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯æ­£å¸¸ã«ç¨¼åƒã—ã¦ã„ã¾ã™ã€‚å¯¾å¿œãŒå¿…è¦ãªã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    st.markdown(f"""
                    <div style="background-color:#fff3e0;padding:10px;border-radius:5px;border:1px solid #ff9800;color:#e65100;margin-bottom:10px;">
                        <strong>âš ï¸ ç›£è¦–ä¸­</strong><br>
                        å¯¾è±¡: <b>{device_id}</b><br>
                        (ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: {score:.0f} - 60ä»¥ä¸Šã§è‡ªå‹•ä¿®å¾©ã‚’æ¨å¥¨)
                    </div>
                    """, unsafe_allow_html=True)

        # â˜…â˜…â˜… ä¿®æ­£â‘¢: Chat with AI Agent (æ—§UIã®Expanderå½¢å¼ã‚’å®Œå…¨å¾©å…ƒ) â˜…â˜…â˜…
        with st.expander("ğŸ’¬ Chat with AI Agent", expanded=False):
            _chat_target_id = ""
            if selected_incident_candidate:
                _chat_target_id = selected_incident_candidate.get("id", "") or ""
            if not _chat_target_id and target_device_id:
                _chat_target_id = target_device_id

            _chat_ci = _build_ci_context_for_chat(topology, _chat_target_id) if _chat_target_id else {}
            if _chat_ci:
                _vendor = _chat_ci.get("vendor", "") or "Unknown"
                _os = _chat_ci.get("os", "") or "Unknown"
                _model_name = _chat_ci.get("model", "") or "Unknown"
                st.caption(f"å¯¾è±¡æ©Ÿå™¨: {_chat_target_id}   Vendor: {_vendor}   OS: {_os}   Model: {_model_name}")

            # ã‚¯ã‚¤ãƒƒã‚¯è³ªå•ãƒœã‚¿ãƒ³
            q1, q2, q3 = st.columns(3)
            with q1:
                if st.button("è¨­å®šãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", use_container_width=True):
                    st.session_state.chat_quick_text = "ã“ã®æ©Ÿå™¨ã§ã€ç¾åœ¨ã®è¨­å®šã‚’å®‰å…¨ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã™ã‚‹æ‰‹é †ã¨ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
            with q2:
                if st.button("ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯", use_container_width=True):
                    st.session_state.chat_quick_text = "ã“ã®æ©Ÿå™¨ã§ã€å¤‰æ›´ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹ä»£è¡¨çš„ãªæ‰‹é †ï¼ˆå€™è£œï¼‰ã¨æ³¨æ„ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
            with q3:
                if st.button("ç¢ºèªã‚³ãƒãƒ³ãƒ‰", use_container_width=True):
                    st.session_state.chat_quick_text = "ä»Šå›ã®ç—‡çŠ¶ã‚’åˆ‡ã‚Šåˆ†ã‘ã‚‹ãŸã‚ã«ã€ã¾ãšå®Ÿè¡Œã™ã¹ãç¢ºèªã‚³ãƒãƒ³ãƒ‰ï¼ˆshow/diagnosticï¼‰ã‚’å„ªå…ˆåº¦é †ã«æ•™ãˆã¦ãã ã•ã„ã€‚"

            if st.session_state.chat_quick_text:
                st.info("ã‚¯ã‚¤ãƒƒã‚¯è³ªå•ï¼ˆã‚³ãƒ”ãƒ¼ã—ã¦è²¼ã‚Šä»˜ã‘ï¼‰")
                st.code(st.session_state.chat_quick_text)

            if st.session_state.chat_session is None and api_key and GENAI_AVAILABLE:
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel("gemma-3-12b-it")
                st.session_state.chat_session = model.start_chat(history=[])

            # ã‚¿ãƒ–ã§ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ: ä¼šè©± / å±¥æ­´
            tab1, tab2 = st.tabs(["ğŸ’¬ ä¼šè©±", "ğŸ“ å±¥æ­´"])

            with tab1:
                if st.session_state.messages:
                    last_msg = st.session_state.messages[-1]
                    if last_msg["role"] == "assistant":
                        st.info("ğŸ¤– æœ€æ–°ã®å›ç­”")
                        with st.container(height=300):
                            st.markdown(last_msg["content"])

                st.markdown("---")
                prompt = st.text_area(
                    "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                    height=70,
                    placeholder="Ctrl+Enter ã¾ãŸã¯ é€ä¿¡ãƒœã‚¿ãƒ³ã§é€ä¿¡",
                    key="chat_textarea"
                )

                col1, col2, col3 = st.columns([3, 1, 1])
                with col2:
                    send_button = st.button("é€ä¿¡", type="primary", use_container_width=True)
                with col3:
                    if st.button("ã‚¯ãƒªã‚¢"):
                        st.session_state.messages = []
                        st.rerun()

                if send_button and prompt:
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    if st.session_state.chat_session:
                        ci = _build_ci_context_for_chat(topology, _chat_target_id) if _chat_target_id else {}
                        ci_prompt = f"""ã‚ãªãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é‹ç”¨ï¼ˆNOC/SREï¼‰ã®å®Ÿå‹™è€…ã§ã™ã€‚
æ¬¡ã® CI æƒ…å ±ã¨ Config æŠœç²‹ã‚’å¿…ãšå‚ç…§ã—ã¦ã€å…·ä½“çš„ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€CI (JSON)ã€‘
{json.dumps(ci, ensure_ascii=False, indent=2)}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘
{prompt}

å›ç­”ãƒ«ãƒ¼ãƒ«:
- CI/Config ã«åŸºã¥ãå…·ä½“æ‰‹é †ãƒ»ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’æç¤ºã™ã‚‹
- è¿½åŠ ç¢ºèªãŒå¿…è¦ãªã‚‰ã€è³ªå•ã¯æœ€å°é™ï¼ˆ1ã€œ2ç‚¹ï¼‰ã«çµã‚‹
- ä¸æ˜ãªå‰æã¯æ¨æ¸¬ã›ãšã€ŒCIã«ç„¡ã„ã®ã§ç¢ºèªãŒå¿…è¦ã€ã¨æ˜è¨˜ã™ã‚‹
"""
                        with st.spinner("AI ãŒå›ç­”ã‚’ç”Ÿæˆä¸­..."):
                            try:
                                response = generate_content_with_retry(
                                    st.session_state.chat_session.model, ci_prompt, stream=False
                                )
                                if response:
                                    full_response = response.text if hasattr(response, "text") else str(response)
                                    if not full_response.strip():
                                        full_response = "AIå¿œç­”ãŒç©ºã§ã—ãŸã€‚"
                                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                                else:
                                    st.error("AIã‹ã‚‰ã®å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                            except Exception as e:
                                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.rerun()

            with tab2:
                if st.session_state.messages:
                    history_container = st.container(height=400)
                    with history_container:
                        for i, msg in enumerate(st.session_state.messages):
                            icon = "ğŸ¤–" if msg["role"] == "assistant" else "ğŸ‘¤"
                            with st.container(border=True):
                                st.markdown(f"{icon} **{msg['role'].upper()}** (ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {i+1})")
                                st.markdown(msg["content"])
                else:
                    st.info("ä¼šè©±å±¥æ­´ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚")
