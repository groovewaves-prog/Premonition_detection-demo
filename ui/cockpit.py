# ui/cockpit.py (Full UX Restoration + Enhanced Precognition)
import streamlit as st
import pandas as pd
import json
import time
import re
import hashlib
from typing import Optional, List, Dict, Any

try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False

from registry import get_paths, load_topology, get_display_name
from alarm_generator import generate_alarms_for_scenario, Alarm, get_alarm_summary
from inference_engine import LogicalRCA
from network_ops import (
    generate_analyst_report_streaming, 
    generate_remediation_commands_streaming, 
    run_remediation_parallel_v2, 
    RemediationEnvironment
)
from utils.helpers import get_status_from_alarms, get_status_icon, load_config_by_id
from utils.llm_helper import get_rate_limiter, generate_content_with_retry
from verifier import verify_log_content
from .graph import render_topology_graph

# --- å¾©å…ƒã•ã‚ŒãŸãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def _hash_text(text: str) -> str:
    return hashlib.sha256((text or "").encode("utf-8")).hexdigest()[:16]

def _pick_first(mapping: dict, keys: list, default: str = "") -> str:
    for k in keys:
        try:
            v = mapping.get(k, None)
        except Exception: v = None
        if v is None: continue
        if isinstance(v, (int, float, bool)):
            s = str(v)
            if s: return s
        elif isinstance(v, str):
            if v.strip(): return v.strip()
    return default

def _build_ci_context_for_chat(topology: dict, target_node_id: str) -> dict:
    node = topology.get(target_node_id)
    if node:
        md = node.metadata if hasattr(node, 'metadata') else node.get('metadata', {})
    else: md = {}
    ci = {
        "device_id": target_node_id or "",
        "hostname": _pick_first(md, ["hostname", "host", "name"], default=(target_node_id or "")),
        "vendor": _pick_first(md, ["vendor", "manufacturer"], default=""),
        "model": _pick_first(md, ["model", "hw_model"], default=""),
        "role": _pick_first(md, ["role", "type"], default=""),
        "site": _pick_first(md, ["site", "location"], default=""),
    }
    try:
        conf = load_config_by_id(target_node_id) if target_node_id else ""
        if conf: ci["config_excerpt"] = conf[:1500]
    except Exception: pass
    return ci

def run_diagnostic_simulation_no_llm(selected_scenario: str, target_node_obj) -> dict:
    device_id = getattr(target_node_obj, "id", "UNKNOWN") if target_node_obj else "UNKNOWN"
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    lines = [f"[PROBE] ts={ts}", f"[PROBE] scenario={selected_scenario}", f"[PROBE] target_device={device_id}", ""]
    recovered_devices = st.session_state.get("recovered_devices") or {}
    if recovered_devices.get(device_id):
        lines += ["show chassis cluster status", "Redundancy group 0: healthy", "control link: up"]
    else:
        if "WAN" in selected_scenario: lines += ["show ip interface brief", "GigabitEthernet0/0 down down", "Neighbor 203.0.113.2 Idle"]
        elif "FW" in selected_scenario: lines += ["show chassis cluster status", "Redundancy group 0: degraded", "control link: down"]
        else: lines += ["show system alarms", "No active alarms"]
    return {"status": "SUCCESS", "sanitized_log": "\n".join(lines), "device_id": device_id}

# --- ãƒ¡ã‚¤ãƒ³æç”» ---
def render_incident_cockpit(site_id: str, api_key: Optional[str]):
    display_name = get_display_name(site_id)
    scenario = st.session_state.site_scenarios.get(site_id, "æ­£å¸¸ç¨¼åƒ")
    
    # å¾©å…ƒã•ã‚ŒãŸãƒ˜ãƒƒãƒ€ãƒ¼
    col_header = st.columns([4, 1])
    with col_header[0]: st.markdown(f"### ğŸ›¡ï¸ AIOps ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ»ã‚³ãƒƒã‚¯ãƒ”ãƒƒãƒˆ")
    with col_header[1]:
        st.markdown('<span id="back-btn-marker"></span>', unsafe_allow_html=True)
        st.markdown("<style>#back-btn-marker + div button { background-color: #d32f2f !important; color: white !important; border: 2px solid #b71c1c !important; font-weight: bold !important; }</style>", unsafe_allow_html=True)
        if st.button("ğŸ”™ ä¸€è¦§ã«æˆ»ã‚‹", key="back_to_list"):
            st.session_state.active_site = None
            st.rerun()
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
    paths = get_paths(site_id)
    topology = load_topology(paths.topology_path)
    if not topology:
        st.error("ãƒˆãƒãƒ­ã‚¸ãƒ¼èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼")
        return

    alarms = generate_alarms_for_scenario(topology, scenario)
    status = get_status_from_alarms(scenario, alarms)
    
    injected = st.session_state.get("injected_weak_signal")
    if injected and injected["device_id"] in topology:
        for m in injected.get("messages", []):
            alarms.append(Alarm(injected["device_id"], m, "INFO", False))

    engine_key = f"engine_{site_id}"
    if engine_key not in st.session_state.logic_engines:
        st.session_state.logic_engines[engine_key] = LogicalRCA(topology)
    engine = st.session_state.logic_engines[engine_key]
    
    results = engine.analyze(alarms) if alarms else []
    if not results: results = [{"id": "SYSTEM", "label": "æ­£å¸¸ç¨¼åƒ", "prob": 0.0, "type": "Normal"}]

    # KPIè¡¨ç¤º (å®Œå…¨å¾©å…ƒ)
    preds = [r for r in results if r.get('is_prediction')]
    st.markdown("---")
    k1, k2, k3 = st.columns(3)
    k1.metric("ğŸš¨ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", f"{get_status_icon(status)} {status}")
    k2.metric("ğŸ“Š ã‚¢ãƒ©ãƒ¼ãƒ æ•°", f"{len(alarms)}ä»¶")
    k3.metric("ğŸ¯ è¢«ç–‘ç®‡æ‰€", f"{len([r for r in results if r.get('prob', 0) > 0.5])}ä»¶", delta=f"ã†ã¡ğŸ”®äºˆå…† {len(preds)}ä»¶" if preds else None, delta_color="off")
    
    st.markdown("---")

    # =====================================================
    # ğŸ”® æ–°è¦: äºˆå…†æƒ…å ±ç‰¹åŒ–UX (Future Radar & Actions)
    # =====================================================
    if preds:
        st.markdown("### ğŸ”® AIOps Future Radar")
        st.caption("ç¾åœ¨ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‹ã‚‰ã€ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ„ã‚¤ãƒ³ãŒå°†æ¥ã®éšœå®³ã‚’äºˆæ¸¬ã—ã¾ã—ãŸã€‚")
        for p in preds:
            with st.container():
                # ã‚«ãƒ¼ãƒ‰å½¢å¼ã§äºˆå…†ã‚’è¡¨ç¤º
                st.markdown(f"""
                <div style="border: 2px solid #E1BEE7; border-left: 10px solid #9C27B0; background-color: #F3E5F5; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                    <div style="display:flex; justify-content:space-between; align-items:center;">
                        <h4 style="margin:0; color:#4A148C;">ğŸ“ {p['id']} : {p.get('label', '').replace('ğŸ”® [äºˆå…†] ', '')}</h4>
                        <span style="background-color:#9C27B0; color:white; padding:5px 15px; border-radius:20px; font-weight:bold;">ç¢ºä¿¡åº¦ {p.get('prob', 0)*100:.0f}%</span>
                    </div>
                    <div style="margin-top:15px; display:grid; grid-template-columns: 1fr 1fr 1fr; gap:20px;">
                        <div style="background:white; padding:10px; border-radius:5px;">
                            <span style="font-size:0.8em; color:#666;">æ—©æœŸæ¤œçŸ¥</span><br>
                            <b>{p.get('prediction_early_warning_hours', 0)}æ™‚é–“å‰</b> ã«å…†å€™ã‚’æ•æ‰
                        </div>
                        <div style="background:white; padding:10px; border-radius:5px;">
                            <span style="font-size:0.8em; color:#666;">æ€¥æ€§æœŸ(Critical)ã¾ã§</span><br>
                            <b>ã‚ã¨ç´„ {p.get('prediction_time_to_critical_min', 0)} åˆ†</b> ã§æ·±åˆ»åŒ–
                        </div>
                        <div style="background:white; padding:10px; border-radius:5px;">
                            <span style="font-size:0.8em; color:#666;">å½±éŸ¿ã®åºƒãŒã‚Š</span><br>
                            é…ä¸‹ <b>{p.get('prediction_affected_count', 0)} å°</b> ãŒé€šä¿¡æ–­ã®æã‚Œ
                        </div>
                    </div>
                </div>
                """, unsafe_allow_html=True)
                
                # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (ã ã‹ã‚‰ã©ã†ã™ã‚‹) ã‚’æ˜ç¤º
                rec_actions = p.get("recommended_actions", [])
                if rec_actions:
                    st.markdown("##### âš¡ äºˆé˜²çš„æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (Primary Actions)")
                    cols_act = st.columns(len(rec_actions))
                    for idx, act in enumerate(rec_actions):
                        with cols_act[idx]:
                            with st.container(border=True):
                                st.markdown(f"**ğŸ‘‰ {act['title']}**")
                                st.caption(f"åŠ¹æœ: {act['effect']}")

                if st.button(f"ğŸ” {p['id']} ã®äºˆå…†ã‚’è©³ç´°åˆ†æã™ã‚‹", key=f"btn_future_{p['id']}"):
                    st.session_state.selected_candidate = p
        st.markdown("---")

    # =====================================================
    # å¾©å…ƒã•ã‚ŒãŸã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç®¡ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    # =====================================================
    root_cause_ids = {a.device_id for a in alarms if a.is_root_cause}
    downstream_ids = {a.device_id for a in alarms if not a.is_root_cause}
    rc_list = [r for r in results if r.get('is_prediction') or r['id'] in root_cause_ids or r.get('prob', 0) > 0.5]
    ds_list = [r for r in results if r['id'] in downstream_ids]

    # é’ã„ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ¼ã®å¾©å…ƒ
    if rc_list and ds_list:
        st.info(f"ğŸ“ **æ ¹æœ¬åŸå› **: {rc_list[0]['id']} â†’ å½±éŸ¿ç¯„å›²: é…ä¸‹ {len(ds_list)} æ©Ÿå™¨")

    st.markdown("#### ğŸ¯ æ ¹æœ¬åŸå› å€™è£œ")
    df_data = []
    for i, c in enumerate(rc_list, 1):
        p = c.get('prob', 0)
        status_txt = "ğŸ”® äºˆå…†" if c.get('is_prediction') else "ğŸ”´ å±é™º" if p > 0.9 else "ğŸŸ¡ è­¦å‘Š"
        df_data.append({"é †ä½": i, "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": status_txt, "ãƒ‡ãƒã‚¤ã‚¹": c['id'], "åŸå› ": c.get('label'), "ç¢ºä¿¡åº¦": f"{p*100:.0f}%", "_obj": c})
    
    df = pd.DataFrame(df_data)
    sel = st.dataframe(df.drop(columns=["_obj"]), use_container_width=True, hide_index=True, selection_mode="single-row", on_select="rerun")
    
    if sel.selection.rows:
        st.session_state.selected_candidate = df.iloc[sel.selection.rows[0]]["_obj"]
    elif rc_list and not st.session_state.get("selected_candidate"):
        st.session_state.selected_candidate = rc_list[0]

    if ds_list:
        with st.expander(f"â–¼ å½±éŸ¿ã‚’å—ã‘ã¦ã„ã‚‹æ©Ÿå™¨ ({len(ds_list)}å°) - ä¸Šæµå¾©æ—§å¾…ã¡", expanded=False):
            st.dataframe(pd.DataFrame([{"Device": d['id'], "Status": "Unreachable"} for d in ds_list]), use_container_width=True, hide_index=True)

    # ä»¥å‰ã®2ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®å¾©å…ƒ
    col_l, col_r = st.columns([1.2, 1])
    
    with col_l:
        st.subheader("ğŸŒ Network Topology")
        st.graphviz_chart(render_topology_graph(topology, alarms, results), use_container_width=True)
        st.markdown("---")
        st.subheader("ğŸ› ï¸ Auto-Diagnostics")
        if st.button("ğŸš€ è¨ºæ–­å®Ÿè¡Œ (Run Diagnostics)", type="primary"):
            with st.status("Agent Operating...", expanded=True) as status:
                res = run_diagnostic_simulation_no_llm(scenario, st.session_state.selected_candidate)
                st.session_state.live_result = res
                st.session_state.verification_result = verify_log_content(res.get('sanitized_log', ""))
                status.update(label="Diagnostics Complete!", state="complete", expanded=False)
            st.rerun()
        if st.session_state.get("live_result"):
            res = st.session_state.live_result
            with st.container(border=True):
                if st.session_state.get("verification_result"):
                    v = st.session_state.verification_result
                    c1, c2, c3 = st.columns(3)
                    c1.metric("Ping", v.get('ping_status'))
                    c2.metric("Intf", v.get('interface_status'))
                    c3.metric("HW", v.get('hardware_status'))
                st.divider()
                st.code(res.get("sanitized_log"), language="text")

    with col_r:
        st.subheader("ğŸ“ AI Analyst Report")
        cand = st.session_state.get("selected_candidate")
        if cand:
            if st.session_state.generated_report is None:
                st.info(f"Target: **{cand['id']}**")
                btn_lbl = "ğŸ”® äºˆå…†åˆ†æãƒ¬ãƒãƒ¼ãƒˆä½œæˆ" if cand.get('is_prediction') else "ğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ"
                if st.button(btn_lbl):
                    full_text = ""
                    placeholder = st.empty()
                    for chunk in generate_analyst_report_streaming(scenario, topology.get(cand['id']), {"id": cand['id']}, load_config_by_id(cand['id']), "", api_key):
                        full_text += chunk
                        placeholder.markdown(full_text)
                    st.session_state.generated_report = full_text
            else:
                with st.container(height=400, border=True): st.markdown(st.session_state.generated_report)
                if st.button("ğŸ”„ å†ä½œæˆ"): st.session_state.generated_report = None; st.rerun()
        
        st.markdown("---")
        st.subheader("ğŸ¤– Remediation & Chat")
        with st.expander("ğŸ’¬ Chat with AI Agent", expanded=False):
            if not st.session_state.get("chat_session") and api_key:
                genai.configure(api_key=api_key)
                st.session_state.chat_session = genai.GenerativeModel("gemma-3-12b-it").start_chat(history=[])
            chat_cont = st.container(height=300)
            with chat_cont:
                for msg in st.session_state.get("messages", []):
                    st.markdown(f"**{'ğŸ¤–' if msg['role']=='assistant' else 'ğŸ‘¤'}**: {msg['content']}")
            prompt = st.chat_input("AIã«è³ªå•...")
            if prompt:
                st.session_state.setdefault("messages", []).append({"role": "user", "content": prompt})
                ci = _build_ci_context_for_chat(topology, cand['id'] if cand else None)
                resp = generate_content_with_retry(st.session_state.chat_session.model, f"Context: {json.dumps(ci)}\nQuestion: {prompt}", stream=False)
                st.session_state.messages.append({"role": "assistant", "content": resp.text})
                st.rerun()
